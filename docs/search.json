[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Data Analysis (STÆ529M)",
    "section": "",
    "text": "Bayesian Data Analysis (STÆ529M)\nThis page contains Quarto templates for the assignments as well as solutions when they have been made available.\nOn each assignment/solution’s page there is a button that can be clicked in order to download the template for that assignment/solution. The student can then write their solution into the premade Quarto template and render it to HTML for turnin if they wish.\n\n\nQuarto\nAll of the templates are written usen Quarto Markdown Documents (.qmd). Quarto is basically the same as RMarkdown so you should not have a hard time switching over to using Quarto. The good thing about Quarto is that you can easily use it to create website. This website was created using Quarto, for example.\nI would recommend you look at the following materials to acquaint yourselves with the use of Quarto:\n\nThe Quarto Webpage\n\nQuarto for Academics\nBeautiful reports and presentations with Quarto\n\n\n\n\nGitHub\nThis whole page is built from a GitHub repository at github.com/bgautijonsson/stae529m. Instead of manually downloading each file you can clone the repository and pull the contents regularly to your computer to receive the newest versions of all documents.\nGit, GitHub and version control in general are essential skills so I would recommend you practice their use.\nI recommend the following material to get acquainted with Git and GitHub.\n\nGit and Github with R\nGit in general"
  },
  {
    "objectID": "assignments/hw7/solution/index.html",
    "href": "assignments/hw7/solution/index.html",
    "title": "Homework #7 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n```{r setup}\n#| output: hide\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(rjags)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(gt)\nlibrary(purrr)\nlibrary(patchwork)\nlibrary(psych, include.only = c(\"kurtosi\", \"skew\"))\ntheme_set(theme_classic())\n```\n\n\n2\nDownload the airquality dataset in R. Fit the following model to the data\n\\[\n\\mathrm{ozone_i} \\sim \\mathrm{Normal}(\\beta_1 + \\beta_2\\mathrm{solar.R_i} + \\beta_3\\mathrm{temp_i} + \\beta_4\\mathrm{wind_i}, \\sigma^2)\n\\]\nUse posterior predictive checks to verify that the model fits well. If you find model misspecification, suggest (but do not fit) alternatives.\n\nSolution:\n\nd &lt;- na.omit(airquality)\nozone &lt;- d$Ozone\nX &lt;- d[ , c(\"Solar.R\", \"Wind\", \"Temp\")] |&gt; as.matrix() |&gt; scale()\n\ndata &lt;- list(\n    ozone = ozone,\n    X = X,\n    n = nrow(X),\n    p = ncol(X)\n)\n\nmodel_string &lt;- textConnection(\"model{\n    #Likelihood\n    for (i in 1:n) {\n        ozone[i] ~ dnorm(mu[i], tau)\n        mu[i] = alpha + inprod(X[i, ], beta[])\n        Y_rep[i] ~ dnorm(mu[i], tau)\n    }\n    \n    for (i in 1:p) {\n        beta[i] ~ dnorm(0, 0.001)\n    }\n    \n    alpha ~ dnorm(0, 0.001)\n    tau ~ dgamma(0.01, 0.01)\n    sigma2 = 1 / tau;\n    \n}\")\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    n.chains = 4,\n    quiet = TRUE\n)\n\nupdate(model, 5000)\n\nparams &lt;- c(\"alpha\", \"beta\", \"sigma2\", \"Y_rep\")\nsamples &lt;- coda.samples(\n    model,\n    variable.names = params,\n    n.iter = 5000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\nWe first look at summary statistics and trace plots and see that our chains seem to have converged well.\n\nposterior_jags |&gt; \n    subset_draws(variable = c(\"alpha\", \"beta\", \"sigma2\")) |&gt; \n    summarise_draws() |&gt; \n    gt() |&gt; \n    fmt_number()\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n41.94\n41.93\n2.02\n2.01\n38.60\n45.25\n1.00\n20,159.44\n19,338.89\n\n\nbeta[1]\n5.44\n5.44\n2.12\n2.11\n1.96\n8.88\n1.00\n16,722.06\n18,731.63\n\n\nbeta[2]\n−11.85\n−11.84\n2.37\n2.34\n−15.73\n−8.00\n1.00\n12,208.76\n16,756.11\n\n\nbeta[3]\n15.68\n15.68\n2.43\n2.40\n11.68\n19.66\n1.00\n10,986.35\n15,293.66\n\n\nsigma2\n457.39\n451.49\n63.67\n61.61\n363.05\n572.01\n1.00\n18,232.60\n18,525.99\n\n\n\n\n\n\n\n\nsamples |&gt; \n    mcmc_trace(\n        regex_pars = c(\"alpha\", \"beta\", \"sigma\")\n    )\n\n\n\n\nThen we can use the bayesplot package for easy posterior predictive checks. To do this we first create a matrix y_rep containing some of the draws from our posterior predictive distribution (PPD) we sampled using JAGS above. We choose to use only a subset of the posterior draws in order to make our plotting go faster.\nFirst we look at the whole distribution of our predictions and compare it to the observed distribution in our data. We notice the observed distribution is right-skewed. This does not necessarily mean we should use a non-normal likelihood. It could also mean we simply didn’t capture enough of the variance in the data with the predictor variable available to us.\n\ny_rep &lt;- posterior_jags |&gt; \n    subset_draws(variable = \"Y_rep\") |&gt; \n    resample_draws(ndraws = 200) |&gt; \n    as_draws_matrix()\n\nppc_dens_overlay(ozone, y_rep) +\n    labs(\n        title = \"Our PPD is less skewed than the observed data\"\n    )\n\n\n\n\nThe plots below tell us that the range and standard deviation of predictions are adequate but our model performs very badly on max, min, skew and kurtosis. Kurtosis is a measure of heavy the tails of a distribution is, a higher kurtosis means that the distribution has longer tails (think student-t versus normal). A large positive value of skew means that a distribution slopes longer to the right, a large negative value means it slopes longer to the right and a value of 0 means symmetry.\n\nrange &lt;- function(x) max(x) - min(x)\nmy_stats &lt;- list(\"max\", \"min\", \"range\", \"sd\", \"skew\", \"kurtosi\")\nplot_gof_check &lt;- function(check) {\n    ppc_stat(ozone, y_rep, stat = check) +\n        labs(\n            title = check\n        )\n}\n\nmy_stats |&gt; \n    map(plot_gof_check) |&gt; \n    wrap_plots()\n\n\n\n\nThese goodness of fit checks imply that we might need to use a likelihood that has longer tails and is positively skewed. One way to model the data in that way is to model it on the log scale (since all the observed values are &gt; 0) and maybe using a student-t instead of a normal distribution.\nSuch a model might look something like this in JAGS code\n\nmodel_string &lt;- textConnection(\"model{\n    #Likelihood\n    for (i in 1:n) {\n        ##### New likelihood #####\n        ozone[i] ~ dt(mu[i], tau, k)\n        ##### Log link function ##\n        log(mu[i]) = alpha + inprod(X[i, ], beta[])\n        Y_rep[i] ~ dnorm(mu[i], tau)\n    }\n    \n    for (i in 1:p) {\n        beta[i] ~ dnorm(0, 0.001)\n    }\n    \n    alpha ~ dnorm(0, 0.001)\n    tau ~ dgamma(0.01, 0.01)\n    sigma2 = 1 / tau;\n    ##### Prior for k, the degrees of freedom ######\n    k ~ dgamma(0.01, 0.01);\n    \n}\")\n\n\n\n\n4\nUse the “Mr. October” data. Compare the two models:\n\nusing Bayes factors, DIC and WAIC. Assume the Uniform(0, c) prior for all \\(\\lambda_j\\) and compare the results for \\(c = 1\\) and \\(c = 10\\).\n\nSolution:\nThere are two ways of calculating the Bayes factors in this assignment:\n\nSince this is a comparison of relatively simple models we can write out the equations for \\(P(\\mathcal M_1 \\vert Y)\\) and \\(P(\\mathcal M_2 \\vert Y)\\)\n\n\\[\n\\begin{aligned}\nP(\\mathcal M_2 \\vert Y) &= \\frac{P(Y\\vert\\mathcal M_2) P(\\mathcal M_2)}{P(Y)} \\\\\n&\\propto P(Y\\vert \\mathcal M_2)P(\\mathcal M_2) \\\\\n&= P(\\mathcal M_2)\\int_0^c \\prod_{i=1}^2f(Y_i \\vert \\lambda_0)\\pi(\\lambda_0)d\\lambda_0 \\\\\n&= P(\\mathcal M_2)\\int_0^c \\prod_{i=1}^2f(Y_i \\vert \\lambda_0)\\frac{1}{c} d\\lambda_0 \\\\\n&= \\frac{1}{c}P(\\mathcal M_2)\\int_0^c \\prod_{i=1}^2 \\frac{(N_i\\lambda_0)^{Y_i}e^{-N_i\\lambda_0}}{Y_i!} d\\lambda_0 \\\\\n&= \\frac1c P(\\mathcal M_2) \\int_0^c \\frac{N_1^{Y_1}N_2^{Y_2}}{Y_1!Y_2!}\\lambda_0^{Y_1+Y_2}e^{-(N_1+N_2)\\lambda_0}d\\lambda_0 \\\\\n&= \\frac1c P(\\mathcal M_2) \\frac{N_1^{Y_1}N_2^{Y_2}}{Y_1!Y_2!}\\int_0^c \\lambda_0^{Y_1+Y_2}e^{-(N_1+N_2)\\lambda_0}d\\lambda_0\n\\end{aligned}\n\\]\nThis looks like the kernel of a Gamma(A, B) distribution with \\(A = Y_1 + Y_2 + 1\\) and \\(B = N_1 + N_2\\).\n\\[\n\\begin{aligned}\nP(\\mathcal M_2 \\vert Y) &= \\frac1c P(\\mathcal M_2) \\frac{N_1^{Y_1}N_2^{Y_2}}{Y_1!Y_2!} \\frac{\\Gamma(A)}{B^A} \\int_0^c \\frac{B^A}{\\Gamma(A)} \\lambda_0^{A - 1}e^{-B\\lambda_0}d\\lambda_0 \\\\\n&= \\frac1c P(\\mathcal M_2) \\frac{N_1^{Y_1}N_2^{Y_2}}{Y_1!Y_2!} \\frac{(Y_1 + Y_2)!}{(N_1 + N_2)^{Y_1 + Y_2 + 1}} P(\\lambda_0 &lt; c \\vert Y),\n\\end{aligned}\n\\]\nwhere \\(\\lambda_0 \\sim \\mathrm{Gamma}(Y_1 + Y_2 + 1, N_1 + N_2)\\).\nIn a similar way we end up getting\n\\[\n\\begin{aligned}\nP(\\mathcal M_1 \\vert Y) &= \\frac{1}{c^2} P(\\mathcal M_1) \\frac{1}{N_1N_2} \\int_0^c \\frac{N_1^{Y_1+1}}{\\Gamma(Y_1+1)} \\lambda_1^{Y_1 + 1 - 1}e^{-N_1\\lambda_1}d\\lambda_1 \\int_0^c \\frac{N_2^{Y_2+1}}{\\Gamma(Y_2+1)} \\lambda_2^{Y_2 + 1 - 1}e^{-N_2\\lambda_2}d\\lambda_2 \\\\\n&= \\frac{1}{c^2} P(\\mathcal M_1)\\frac{1}{N_1N_2} P(\\lambda_1 &lt; c \\vert Y) P(\\lambda_2 &lt; c \\vert Y),\n\\end{aligned}\n\\]\nwhere \\(\\lambda_1 \\sim \\mathrm{Gamma}(Y_1 + 1, N_1)\\) and \\(\\lambda_2 \\sim \\mathrm{Gamma}(Y_2 + 1, N_2)\\). Writing up the ratio of these two, we get\n\\[\n\\begin{aligned}\nBF &= \\frac{P(Y\\vert \\mathcal M_1)}{P(Y \\vert \\mathcal M_2)} \\\\\n&= \\frac{P(\\mathcal M_1 \\vert Y) / P(\\mathcal M_1)}{P(\\mathcal M_2 \\vert Y) / P(\\mathcal M_2)} \\\\\n&= \\frac{\\frac{1}{c^2} \\frac{1}{N_1N_2} P(\\lambda_1 &lt; c \\vert Y) P(\\lambda_2 &lt; c \\vert Y)}{\\frac1c \\frac{N_1^{Y_1}N_2^{Y_2}}{Y_1!Y_2!} \\frac{\\Gamma(A)}{B^A} P(\\lambda_0 &lt; c \\vert Y)} \\\\\n&= \\frac1c \\frac{Y_1!Y_2!}{N_1^{Y_1+1}N_2^{Y_2+1}} \\frac{B^A}{\\Gamma(A)} \\frac{P(\\lambda_1 &lt; c \\vert Y) P(\\lambda_2 &lt; c \\vert Y)}{P(\\lambda_0 &lt; c \\vert Y)} \\\\\n&= \\frac1c \\frac{Y_1!Y_2!}{(Y_1+Y_2)!} \\frac{(N_1+N_2)^{Y_1+Y_2+1}}{N_1^{Y_1+1}N_2^{Y_2+1}} \\frac{P(\\lambda_1 &lt; c \\vert Y) P(\\lambda_2 &lt; c \\vert Y)}{P(\\lambda_0 &lt; c \\vert Y)},\n\\end{aligned}\n\\]\nwhere \\(\\lambda_0 \\sim \\mathrm{Gamma}(Y_1 + Y_2 + 1, N_1 + N_2)\\), \\(\\lambda_1 \\sim \\mathrm{Gamma}(Y_1 + 1, N_1)\\) and \\(\\lambda_2 \\sim \\mathrm{Gamma}(Y_2 + 1, N_2)\\)\nWriting this as an R function, we get\n\n```{r}\nbf &lt;- function(C, Y = c(563, 10), N = c(2820, 27)) {\n    \n    out &lt;- lfactorial(Y[1]) + lfactorial(Y[2]) - lfactorial(sum(Y))\n    out &lt;- out + (sum(Y) + 1) * log(sum(N)) - sum((Y + 1) * log(N))\n    out &lt;- out + pgamma(C, shape = Y[1] + 1, rate = N[1], log.p = TRUE)\n    out &lt;- out + pgamma(C, shape = Y[2] + 1, rate = N[2], log.p = TRUE)\n    out &lt;- out - pgamma(C, shape = sum(Y) + 1, rate = sum(N), log.p = TRUE)\n    out &lt;- out - log(C)\n    \n    exp(out)\n}\n\nbf(C = 1)\nbf(C = 10)\n```\n\n[1] 1.397494\n[1] 0.1397724\n\n\n\nThe other possibility is to use JAGS to calculate the posterior probability of each model for us and then calculate the Bayes factor based on the posterior distribution. To do this, we add an auxiliary variable, \\(z\\), to our sampler and write the two models into the same sampler as below. For each sample in our posterior where \\(z=1\\), our sampler will sample from \\(\\mathcal M_2\\) and otherwise sample from \\(\\mathcal M_2\\). If our sampler converges, we can then calculate the posterior mean of \\(z\\) as a close approximation to the Bayes factor.\n\n\n```{r}\n#| eval: false\nmodel_string_bf &lt;- textConnection(\"model{\n    #Likelihood\n    for (i in 1:2) {\n        Y[i] ~ dpois(N[i] * ifelse(z == 1, lambda_0, lambda[i]))\n        lambda[i] ~ dunif(0, C)\n        like[i] &lt;- dpois(Y[i], lambda[i] * N[i])\n    }\n    \n    z ~ dbern(0.5)\n    lambda_0 ~ dunif(0, C)\n}\")\n```\n\n\n\nCode\nbf_jags &lt;- function(C, Y = c(563, 10), N = c(2820, 27)) {\n    Y &lt;- c(563, 10)\n    N &lt;- c(2820, 27)\n    \n    data &lt;- list(\n        Y = Y,\n        N = N,\n        C = C\n    )\n    \n    model_string_bf &lt;- textConnection(\"model{\n    #Likelihood\n    for (i in 1:2) {\n        Y[i] ~ dpois(N[i] * ifelse(z == 1, lambda_0, lambda[i]))\n        lambda[i] ~ dunif(0, C)\n        like[i] &lt;- dpois(Y[i], lambda[i] * N[i])\n    }\n    \n    z ~ dbern(0.5)\n    lambda_0 ~ dunif(0, C)\n}\")\n    \n    \n    model_bf &lt;- jags.model(\n        model_string_bf,\n        data = data,\n        n.chains = 4,\n        quiet = TRUE\n    )\n    \n    update(model_bf, 100000, progress.bar = \"none\")\n    \n    samples_bf &lt;- coda.samples(\n        model_bf,\n        variable.names = c(\"z\"),\n        n.iter = 100000,\n        progress.bar = \"none\"\n    )\n    \n    p_m2 &lt;- samples_bf |&gt; \n        as_draws_df() |&gt; \n        subset_draws(variable = \"z\") |&gt; \n        summarise_draws(mean) |&gt; \n        pull(mean)\n    \n    p_m1 &lt;- 1 - p_m2\n    \n    p_m1 / (1 - p_m1)\n}\n\n\n\n\nCode\nfit_model &lt;- function(C, Y = c(563, 10), N = c(2820, 27)) {\n    data &lt;- list(\n        Y = Y,\n        N = N,\n        C = C\n    )\n    \n    model_string1 &lt;- textConnection(\"model{\n    #Likelihood\n    for (i in 1:2) {\n        Y[i] ~ dpois(N[i] * lambda[i])\n        lambda[i] ~ dunif(0, C)\n        like[i] &lt;- dpois(Y[i], lambda[i] * N[i])\n    }\n}\")\n    \n    model_string2 &lt;- textConnection(\"model{\n    #Likelihood\n    for (i in 1:2) {\n        Y[i] ~ dpois(N[i] * lambda)\n        like[i] &lt;- dpois(Y[i], lambda * N[i])\n    }\n    lambda ~ dunif(0, C)\n}\")\n    \n    model1 &lt;- jags.model(\n        model_string1,\n        data = data,\n        n.chains = 4,\n        quiet = TRUE\n    )\n    \n    model2 &lt;- jags.model(\n        model_string2,\n        data = data,\n        n.chains = 4,\n        quiet = TRUE\n    )\n    \n    update(model1, 10000, progress.bar = \"none\")\n    update(model2, 10000, progress.bar = \"none\")\n    \n    params &lt;- c( \"like\")\n    samples &lt;- coda.samples(\n        model1,\n        variable.names = params,\n        n.iter = 10000,\n        progress.bar = \"none\"\n    )\n    \n    DIC1 &lt;- dic.samples(model1, n.iter = 50000, progress.bar = \"none\")\n    DIC1 &lt;- sum(DIC1$deviance + DIC1$penalty)\n    \n    like &lt;- rbind(samples[[1]], samples[[2]], samples[[3]], samples[[4]])\n    fbar &lt;- colMeans(like)\n    Pw &lt;- sum(apply(log(like), 2, var))\n    WAIC1 &lt;- -2 * sum(log(fbar)) + 2 * Pw\n    \n    \n    samples &lt;- coda.samples(\n        model2,\n        variable.names = params,\n        n.iter = 10000,\n        progress.bar = \"none\"\n    )\n    \n    DIC2 &lt;- dic.samples(model2, n.iter = 50000, progress.bar = \"none\")\n    DIC2 &lt;- sum(DIC2$deviance + DIC2$penalty)\n    \n    like &lt;- rbind(samples[[1]], samples[[2]], samples[[3]], samples[[4]])\n    fbar &lt;- colMeans(like)\n    Pw &lt;- sum(apply(log(like), 2, var))\n    WAIC2 &lt;- -2 * sum(log(fbar)) + 2 * Pw\n    \n    bf &lt;- bf_jags(C, Y, N)\n    \n    tibble(\n        model = c(\"Two lambdas\", \"One lambda\"),\n        BF = c(bf, 1 / bf),\n        DIC = c(DIC1, DIC2),\n        WAIC = c(WAIC1, WAIC2)\n    )\n}\n\n\n\nresults &lt;- tibble(\n    C = c(1, 10)\n) |&gt; \n    mutate(\n        results = map(C, fit_model)\n    )\n\nresults |&gt; \n    unnest(results) |&gt; \n    mutate(\n        C = str_c(\"c = \", C)\n    ) |&gt; \n    group_by(C) |&gt; \n    gt() |&gt; \n    fmt_number(decimals = 3) |&gt; \n    cols_label(\n        BF = \"Bayes Factor\",\n        model = \"Model\"\n    )\n\n\n\n\n\n\n\nModel\nBayes Factor\nDIC\nWAIC\n\n\n\n\nc = 1\n\n\nTwo lambdas\n1.490\n16.323\n15.742\n\n\nOne lambda\n0.671\n17.437\n17.134\n\n\nc = 10\n\n\nTwo lambdas\n0.159\n16.345\n15.845\n\n\nOne lambda\n6.270\n17.426\n17.221\n\n\n\n\n\n\n\n\n# sampling_distribution &lt;- crossing(\n#     iter = 1:500,\n#     C = c(1, 10),\n#     N = c(2820, 27),\n# ) |&gt; \n#     mutate(\n#         Y = rpois(2, 0.2 * N),\n#         .by = c(iter, C)\n#     ) |&gt; \n#     group_by(iter, C) |&gt; \n#     reframe(\n#         results = fit_model(unique(C), Y, N)\n#     ) \n# \n# sampling_distribution |&gt; write_rds(\"sampling_distribution.rds\")\n\nread_rds(\"sampling_distribution.rds\") |&gt; \n    unnest(results) |&gt; \n    filter(model == \"Two lambdas\") |&gt; \n    select(iter, C, simul = BF) |&gt; \n    inner_join(\n        results |&gt; \n            unnest(results) |&gt; \n            filter(model == \"Two lambdas\") |&gt; \n            select(C, obs = BF),\n        by = join_by(C)\n    ) |&gt; \n    mutate(\n        C = str_c(\"c = \", C)\n    ) |&gt; \n    ggplot() +\n    geom_histogram(aes(x = simul), alpha = 0.5) +\n    geom_vline(\n        aes(xintercept = obs),\n        lty = 2,\n        linewidth = 0.5,\n        alpha = 0.5\n    ) +\n    facet_grid(cols = vars(C), scales = \"free\") +\n    labs(\n        title = \"Sampling distribution of the Bayes Factor for Model 1\",\n        subtitle = \"Calculated by sampling data from Model 2 and rerunning our analysis many times.\",\n        x = \"Distribution of simulated Bayes Factors (histogram)\\nand observed Bayes Factors (broken lines)\",\n        y = NULL\n    )\n\n\n\n\n\n\n\n10\nDownload the WWWusage dataset in R. Using data from times \\(t = 5, \\dots, 100\\) as outcomes, fit the autoregressive model\n\\[\nY_t \\vert Y_{t-1}, \\dots, Y_1 \\sim \\mathrm{Normal}(\\beta_0 + \\beta_1 Y_{t-1} + \\dots + \\beta_L Y_{t-L}, \\sigma^2),\n\\]\nwhere \\(Y_t\\) is the WWW usage at time \\(t\\). Compare the models with \\(L = 1, 2, 3, 4\\) and select the best time lag L.\n\nSolution:\n\nfit_ar_model &lt;- function(L) {\n    Y &lt;- WWWusage\n    \n    data &lt;- list(\n        Y = Y,\n        L = L\n    )\n    \n    model_string &lt;- textConnection(\"model{\n    #Likelihood\n    for (t in 5:100) {\n        Y[t] ~ dnorm(mu[t], tau)\n        mu[t] &lt;- alpha + inprod(Y[(t-L):(t-1)], beta[])\n        like[t] &lt;- dnorm(Y[t], mu[t], tau)\n    }\n    \n    for (i in 1:L) {\n        beta[i] ~ dnorm(0, 0.001)\n    }\n    \n    alpha ~ dnorm(0, 0.001)\n    tau ~ dgamma(0.01, 0.01)\n    sigma2 &lt;- 1 / tau\n}\")\n    \n    model &lt;- jags.model(\n        model_string,\n        data = data,\n        n.chains = 4,\n        quiet = TRUE\n    )\n    update(model, 10000, progress.bar = \"none\")\n    \n    \n    params &lt;- c(\"like\")\n    \n    samples &lt;- coda.samples(\n        model,\n        variable.names = params,\n        n.iter = 10000,\n        progress.bar = \"none\"\n    )\n    \n    DIC &lt;- dic.samples(model, n.iter = 50000, progress.bar = \"none\")\n    DIC &lt;- sum(DIC$deviance + DIC$penalty)\n    \n    like &lt;- rbind(samples[[1]], samples[[2]], samples[[3]], samples[[4]])\n    fbar &lt;- colMeans(like)\n    Pw &lt;- sum(apply(log(like), 2, var))\n    WAIC &lt;- -2 * sum(log(fbar)) + 2 * Pw\n    \n    tibble(\n        L = L,\n        DIC = DIC,\n        WAIC = WAIC\n    )\n}\n\nresults &lt;- map_dfr(1:4, fit_ar_model)\n\nresults |&gt;\n    gt() |&gt; \n    fmt_number()\n\n\n\n\n\n\n\nL\nDIC\nWAIC\n\n\n\n\n1.00\n612.83\n612.40\n\n\n2.00\n512.15\n512.02\n\n\n3.00\n507.32\n507.45\n\n\n4.00\n493.48\n494.46"
  },
  {
    "objectID": "assignments/hw1/solution/index.html",
    "href": "assignments/hw1/solution/index.html",
    "title": "Homework #1 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n1. Sample survey\nSuppose we are going to sample 100 individuals from a county (with population size much larger than 100) and ask each sampled person whether they support policy Z or not. Let \\(Y_i = 1\\) if person \\(i\\) in the sample supports the policy, and \\(Y_i = 0\\) otherwise.\n\nAssume \\(Y_1, \\dots, Y_{100}\\) are, conditoinal on \\(\\theta\\), i.i.d. binary random variables with expectation \\(\\theta\\). Write down the joint distribution of \\(\\mathrm{Pr}(Y_1 = y_1, \\dots, Y_{100} = y_{100})\\) in a compact form. Also write down the form of \\(\\mathrm{Pr}(\\sum_{i_1}^{100}Y_i=y)\\).\n\n\nSolution: First, we write down the distribution of a single observation, \\(Y_i\\):\n\\[\nP(Y_i = 1\\vert \\theta) = \\theta \\quad \\mathrm{and} \\quad P(Y_i = 0|\\theta) = (1 - \\theta).\n\\]\nWe can write these two cases together as follows\n\\[\nPr(Y_i = y_i) = \\theta^{y_i}(1 - \\theta)^{1 - y_i}.\n\\]\nWe recognize this as the Bernoulli distribution. The joint distribution is obtained by multiplying together all the observations\n\\[\nPr(Y_1 = y_1, \\dots, Y_{100} = y_{100}) = \\prod_{i=1}^{100}\\theta^{y_i}(1 - \\theta)^{1 - y_i} = \\theta^{\\sum_i y_i}(1 - \\theta)^{100 - \\sum_i y_i}.\n\\]\nThis is the distribution of any specific sequence of \\(y_i\\)’s. Let \\(X = \\sum_{i=1}^{100}y_i\\). There are \\(\\binom{100}{X} = \\frac{100!}{X!(100-X)!}\\) ways to choose a group of size \\(X\\) from a group of size \\(100\\). Then we can write the distribution of \\(X\\) as\n\\[\nPr(X = x) = \\binom{100}{x} \\theta^x(1 - \\theta)^{100 - x}\n\\]\n\n\nFor the moment, suppose you believed that \\(\\theta \\in \\{0, 0.1, \\dots, 0.9, 1.0\\}\\). Given that the results of the survey were \\(\\sum_{i=1}^{100}Y_i=73\\), compute \\(\\mathrm{Pr}(\\sum_{i=1}^{100}Y_i=73\\vert\\theta)\\) for each of these \\(11\\) values of \\(\\theta\\) and plot these probabilities as a function of \\(\\theta\\) (point mass at each value of \\(\\theta\\)).\n\n\nSolution: The equation above now becomes\n\\[\nPr(X = 73|\\theta) = \\binom{100}{73} \\theta^{73}(1 - \\theta)^{27}\n\\]\nWhen evaluating this expression, we do our calculations on the log scale to avoid numerical underflow. This gives us\n\\[\nPr(X = 73|\\theta) = \\binom{100}{73} e^{73 \\ln(\\theta) + 27 \\ln(1 - \\theta)}.\n\\]\nWe could easily calculate the binomial coefficient on the log scale, but I trust that our programming language of choice has an efficient and safe implementation.\n```{r}\n#| layout: [[2, 4]]\n# Include your code here\n\nmy_likelihood &lt;- function(theta) {\n    choose(100, 73) * exp(73 * log(theta) + 27 * log(1 - theta))\n}\n\nthetas &lt;- seq(0, 1, by = 0.1)\nresult &lt;- my_likelihood(thetas)\n\ndata.frame(theta = thetas, likelihood = result)\n\nplot(\n    thetas, \n    result, \n    xlab = expression(theta), \n    ylab = expression(Pr(X==73*\"|\"*theta))\n)\n```\n\n\n\n   theta   likelihood\n1    0.0 0.000000e+00\n2    0.1 1.114936e-50\n3    0.2 4.378461e-30\n4    0.3 8.515317e-19\n5    0.4 1.750513e-11\n6    0.5 1.512525e-06\n7    0.6 2.204769e-03\n8    0.7 7.196692e-02\n9    0.8 2.168109e-02\n10   0.9 8.758007e-07\n11   1.0 0.000000e+00\n\n\n\n\n\n\n\n\nNow suppose you originally had no prior information to believe one of these \\(\\theta\\)-values over another, and thus \\(\\mathrm{Pr}(\\theta=0.0) = \\mathrm{Pr}(\\theta=0.1) = \\dots = \\mathrm{Pr}(\\theta=0.9) = \\mathrm{Pr}(\\theta = 1.0) = \\frac{1}{11}\\). Use Bayes’ rule to compute \\(p(\\theta\\vert \\sum_{i=1}^{100}Y_i=73)\\) for each \\(\\theta\\)-value. Make a plot of this posterior distribution as a function of \\(\\theta\\) (point mass at each value of \\(\\theta\\)).\n\n\nSolution: First we write out all the equations that we know, before piecing them together:\n\\[\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{Pr(X=73|\\theta)\\pi(\\theta)}{m(X=73)} \\\\\nPr(X=73|\\theta) &= \\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\\\\n\\pi(\\theta) &= \\frac{1}{11} \\\\\nm(X=73) &= \\sum_\\theta\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}.\n\\end{aligned}\n\\]\nThis gives us\n\\[\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}\n}{\\sum_\\theta\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}} \\\\\n&= \\frac{\\theta^{73}(1-\\theta)^{27}}{\\sum_\\theta \\theta^{73}(1-\\theta)^{27}}\n\\end{aligned}\n\\]\nOnce again doing our calculations on the log scale, we get\n\\[\np(\\theta \\vert X = 73) = \\frac{e^{73\\ln(\\theta) + 27\\ln(1 - \\theta)}}{\\sum_\\theta e^{73\\ln(\\theta) + 27\\ln(1 - \\theta)}}\n\\]\n```{r}\n#| layout: [[2, 4]]\n# Include your code here\n\nmy_posterior &lt;- function(theta) {\n    theta_seq &lt;- seq(0, 1, by = 0.1)\n    C &lt;- sum(exp(73 * log(theta_seq) + 27 * log(1 - theta_seq)))\n    exp(73 * log(theta) + 27 * log(1 - theta)) / C\n}\n\nthetas &lt;- seq(0, 1, by = 0.1)\nposterior &lt;- my_posterior(thetas)\n\ndata.frame(theta = thetas, posterior = posterior)\n\nplot(\n    thetas, \n    posterior, \n    xlab = expression(theta), \n    ylab = expression(p(theta*\"|\"*Y))\n)\n```\n\n\n\n   theta    posterior\n1    0.0 0.000000e+00\n2    0.1 1.163146e-49\n3    0.2 4.567788e-29\n4    0.3 8.883524e-18\n5    0.4 1.826206e-10\n6    0.5 1.577927e-05\n7    0.6 2.300105e-02\n8    0.7 7.507881e-01\n9    0.8 2.261859e-01\n10   0.9 9.136709e-06\n11   1.0 0.000000e+00\n\n\n\n\n\n\n\n\nNow suppose you allow \\(\\theta\\) to be any value in the interval \\([0, 1]\\). Using the uniform prior density for \\(\\theta\\), namely, \\(p(\\theta) = 1\\), derive and plot the posterior density of \\(\\theta\\) as a function of \\(\\theta\\). According to the posterior density, what is the probability of \\(\\theta &gt; 0.8\\)?\n\n\nSolution: This time, our posterior is seen to be\n\\[\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot 1\n}{\\int_0^1\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot 1 d\\theta} \\\\\n&= \\frac{\\theta^{73}(1-\\theta)^{27}}{\\int_0^1 \\theta^{73}(1-\\theta)^{27}d\\theta} \\\\\n&= C \\cdot \\theta^{73}(1-\\theta)^{27}\n\\end{aligned}\n\\]\nWe can use the fact that the equation above should integrate to one to find out what \\(C\\) is.\n\\[\n\\int_0^1 C\\cdot \\theta^{73}(1 - \\theta)^{27} = 1 \\\\\n\\int_0^1 \\theta^{73}(1 - \\theta)^{27} = \\frac1C.\n\\]\nFrom working with the Beta distribution, we know that\n\\[\n\\int_0^1\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1 - x)^{\\beta-1} = 1.\n\\]\nUsing this, we see that\n\\[\nC = \\frac{\\Gamma(73 + 27 + 2)}{\\Gamma(73 + 1)\\Gamma(27 + 1)},\n\\]\ngiving us\n\\[\np(\\theta \\vert X = 73) = \\frac{\\Gamma(73 + 27 + 2)}{\\Gamma(73 + 1)\\Gamma(27 + 1)} \\theta^{73}(1 - \\theta)^{27}.\n\\]\nThis means that after observing the data, our uncertainty about \\(\\theta\\) is described by the Beta distribution with \\(\\alpha = 73+1\\) and \\(\\beta = 27+1\\).\n\\[\n\\theta\\vert Y \\sim \\mathrm{Beta}(73 + 1, 27 + 1)\n\\]\nWe’ll use R’s built in dbeta (d for density) function to evaluate this.\n```{r}\n#| layout: [[2, 4]]\n# Include your code here\nthetas &lt;- seq(0, 1, by = 0.1)\nposterior &lt;- dbeta(thetas, 73 + 1, 27 + 1)\n\ndata.frame(theta = thetas, posterior = posterior)\n\nplot(\n    thetas, \n    posterior, \n    xlab = expression(theta), \n    ylab = expression(p(theta*\"|\"*Y))\n)\n```\n\n\n\n   theta    posterior\n1    0.0 0.000000e+00\n2    0.1 1.126085e-48\n3    0.2 4.422245e-28\n4    0.3 8.600470e-17\n5    0.4 1.768018e-09\n6    0.5 1.527650e-04\n7    0.6 2.226817e-01\n8    0.7 7.268659e+00\n9    0.8 2.189790e+00\n10   0.9 8.845588e-05\n11   1.0 0.000000e+00\n\n\n\n\n\n\n\n1 - pbeta(0.8, 73 + 1, 27 + 1)\n\n[1] 0.03848785\n\n\n\n\nWhy are the heights of posterior densities in c. and d. not the same?\n\n\nSolution: The density in c. belongs to a discrete variable and the density in d. belongs to a continuous variable. This means the two functions have different meanings. Discrete random variables have densities (often called mass functions) that can be interpreted as probabilities, but that is not the case for continuous random variables. Thus the density of a continuous random variable can be greater than one as long as it integrates to one over it’s domain.\n\n\n\n2. Random numbers, probability density functions (pdf) and cumulative density functions (cdf)\nThe goal of this exercise is to generate random numbers, plot the histogram, the empirical pdf and cdf for these numbers, and see how they compare to the theoretical pdf and cdf. The goal is also to compare the sample mean and standard deviation to the theoretical mean and standard deviation.\n\nGenerate \\(B = 3000\\) numbers from the gamma distribution with parameters \\(\\alpha = 2\\) and \\(\\beta = 0.1\\). Compute the sample mean and the sample standard deviation and compare to the theoretical mean and standard deviation.\n\n\nSolution:\n\n```{r}\n# Include your code here\nB &lt;- 3000\nalpha &lt;- 2\nbeta &lt;- 0.1\nX &lt;- rgamma(n = B, shape = alpha, rate = beta)\n\ntheoretical_mean &lt;- alpha / beta\ntheoretical_sd &lt;- sqrt(alpha / beta^2)\n\nobs_mean &lt;- mean(X)\nobs_sd &lt;- sd(X)\n\ndata.frame(\n    theoretical_mean,\n    obs_mean,\n    theoretical_sd,\n    obs_sd\n)\n```\n\n  theoretical_mean obs_mean theoretical_sd   obs_sd\n1               20 19.53301       14.14214 13.66634\n\n\n\n\nPlot the theoretical density (pdf) of the gamma distribution. Plot the empirical density based on the data on the same graph. Plot the histogram of the data on another graph.\n\n\nSolution:\nUsing base R:\n\n```{r}\n# Include your code here\n\nx_dens &lt;- density(X, from = 0)\n\nplot(\n    x_dens, \n    main = \"Comparing the empirical and theoretical densities\", \n    col = \"blue\"\n)\ncurve(\n    dgamma(x, shape = alpha, rate = beta), \n    from = 0, to = 120, \n    add = TRUE, \n    col = \"red\"\n)\nlegend(\n    \"top\", \n    c(\"Empirical\", \"Theoretical\"), \n    col = c(\"blue\", \"red\"), \n    lty = 1\n)\n```\n\n\n\n\nUsing the tidyverse:\n\n```{r}\n#| output: false\nlibrary(dplyr)\nlibrary(ggplot2)\ntheme_set(theme_classic())\n```\n\n\n```{r}\ntibble(\n    X = X\n) |&gt; \n    ggplot(aes(X)) +\n    geom_density(\n        bounds = c(0, Inf),\n        aes(color = \"Empirical\", lty = \"Empirical\")\n        ) +\n    stat_function(\n        fun = function(x) dgamma(x, shape =alpha, rate = beta),\n        geom = \"line\",\n        aes(color = \"Theoretical\", lty = \"Theoretical\")\n    ) +\n    labs(\n        color = \"Type\",\n        linetype = \"Type\",\n        x = \"X\",\n        y = \"Density\",\n        title = \"Comparing the empirical and theoretical densities\"\n    )\n```\n\n\n\n\n\n\nPlot the theoretical cumulative density function (cdf) of the gamma distribution. Plot the empirical cumulative density based on the data on the same graph.\n\n\nSolution:\nUsing base R:\n\n```{r}\n# Include your code here\nX_sorted &lt;- sort(X)\nquantile &lt;- seq_along(X_sorted) / (length(X_sorted) + 1)\n\nplot(\n    X_sorted, quantile, \n    type = \"l\", \n    col = \"blue\", \n    main = \"Comparing the empirical and theoretical CDF\",\n    xlab = \"x\",\n    ylab = expression(P(X&lt;x))\n)\ncurve(\n    pgamma(x, shape = alpha, rate = beta), \n    from = 0, to = 120, \n    add = TRUE, \n    col = \"red\"\n)\nlegend(\n    \"right\", \n    c(\"Empirical\", \"Theoretical\"),\n    col = c(\"blue\", \"red\"), \n    lty = 1\n)\n```\n\n\n\n\nUsing the tidyverse:\n\n```{r}\nlibrary(dplyr)\nlibrary(ggplot2)\ntibble(\n    X = X\n) |&gt; \n    arrange(X) |&gt; \n    mutate(\n        empirical = row_number() / (n() + 1)\n    ) |&gt; \n    ggplot(aes(X)) +\n    geom_line(\n        aes(y = empirical, color = \"Empirical\", lty = \"Empirical\")\n        ) +\n    stat_function(\n        fun = function(x) pgamma(x, shape =alpha, rate = beta),\n        geom = \"line\",\n        aes(color = \"Theoretical\", lty = \"Theoretical\")\n    ) +\n    labs(\n        color = \"Type\",\n        linetype = \"Type\",\n        x = \"X\",\n        y = \"Density\",\n        title = \"Comparing the empirical and theoretical CDF\"\n    )\n```",
    "crumbs": [
      "Solutions",
      "Homework #1 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw6/solution/index.html",
    "href": "assignments/hw6/solution/index.html",
    "title": "Homework #6 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n```{r setup}\n#| output: hide\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(rjags)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(gt)\ntheme_set(theme_classic())\n```\n\n\n4\nDownload the US gun control data from the book’s website. For state \\(i\\), let \\(Y_i\\) be the number of homicides and \\(N_i\\) be the population.\n\nFit the model \\(Y_i \\vert \\beta \\sim \\mathrm{Poisson}(N_i\\lambda_i)\\) where \\(\\log(\\lambda_i) = \\mathbf X_i\\beta\\). Use uninformative priors and \\(p = 7\\) covariates in \\(\\mathbf X_i\\): the intercept, the five confounders \\(\\mathbf Z_i\\), and the total number of gun laws in state \\(i\\). Provide justification that the MCMC sampler has converged and sufficiently explored the posterior distribution and summarize the posterior of \\(\\beta\\)\n\n\nSolution:\n\n\nload(url(\"https://www4.stat.ncsu.edu/~bjreich/BSMdata/guns.RData\",))\n\n\nGunLaws &lt;- rowSums(X)\nmy_X &lt;- cbind(Z, GunLaws) |&gt; scale()\n\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:N_obs) {\n        Y[i] ~ dpois(N[i] * lambda[i])\n        log(lambda[i]) &lt;- alpha + (X[i, ] %*% beta)\n    }\n    for (j in 1:6) {\n        beta[j] ~ dnorm(0, 0.1)\n    }\n    \n    alpha ~ dnorm(0, 0.1)\n}\")\n\ndata = list(\n  X = my_X,\n  N = N,\n  Y = Y,\n  N_obs = length(Y)\n)\n\nmodel &lt;- jags.model(\n  model_string,\n  data = data,\n  n.chains = 4,\n  quiet = TRUE\n)\n\nupdate(model, 10000, progress.bar = \"none\")\n\n\nsamples &lt;- coda.samples(\n  model,\n  variable.names = c(\"alpha\", \"beta\"),\n  n.iter = 10000,\n  progress.bar = \"none\"\n)\n\nposterior_jags &lt;- samples |&gt; \n  as_draws_df()\n\nThe table below shows us that the \\(\\hat R\\) and ESS of our variables is sufficiently low and the trace plot shows no signs of a lack of convergence. The table below also shows a summary of the posterior distrubution of each model parameter.\n\nposterior_jags |&gt; \n  summarise_draws() |&gt; \n  gt() |&gt; \n  fmt_number(\n    decimals = 2\n  ) \n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n−9.16\n−9.16\n0.01\n0.01\n−9.18\n−9.15\n1.00\n13,414.48\n18,958.25\n\n\nbeta[1]\n0.32\n0.32\n0.01\n0.01\n0.30\n0.35\n1.00\n3,778.12\n7,256.56\n\n\nbeta[2]\n−0.05\n−0.05\n0.01\n0.01\n−0.06\n−0.03\n1.00\n5,682.28\n10,585.90\n\n\nbeta[3]\n0.02\n0.02\n0.01\n0.01\n0.01\n0.03\n1.00\n12,427.16\n16,946.11\n\n\nbeta[4]\n−0.01\n−0.01\n0.01\n0.01\n−0.02\n0.01\n1.00\n6,439.86\n11,669.08\n\n\nbeta[5]\n0.00\n0.00\n0.01\n0.01\n−0.01\n0.01\n1.00\n8,774.36\n14,705.81\n\n\nbeta[6]\n−0.08\n−0.08\n0.01\n0.01\n−0.09\n−0.06\n1.00\n6,892.82\n12,702.51\n\n\n\n\n\n\n\n\nmcmc_trace(\n  posterior_jags\n)\n\n\n\n\n\nFit a Negative binomial regression model and compare with the results from Poisson regression.\n\n\nSolution:\n\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:N_obs) {\n        Y[i] ~ dnegbin(q[i], m)\n        q[i] &lt;- m / (m + N[i] * lambda[i])\n        log(lambda[i]) &lt;- alpha + inprod(X[i, ], beta[])\n    }\n    for (j in 1:6) {\n        beta[j] ~ dnorm(0, 0.0001)\n    }\n    \n    alpha ~ dnorm(0, 0.0001)\n    m ~ dgamma(0.1, 0.1)\n}\")\n\ndata = list(\n  X = my_X,\n  N = N,\n  Y = Y,\n  N_obs = length(Y)\n)\n\nmodel &lt;- jags.model(\n  model_string,\n  data = data,\n  n.chains = 4,\n  quiet = TRUE\n)\n\nupdate(model, 10000, progress.bar = \"none\")\n\n\nsamples &lt;- coda.samples(\n  model,\n  variable.names = c(\"alpha\", \"beta\", \"m\"),\n  n.iter = 10000,\n  progress.bar = \"none\"\n)\n\nposterior_jags &lt;- samples |&gt; \n  as_draws_df()\n\nThe table below shows us that the \\(\\hat R\\) and ESS of our variables is sufficiently low and the trace plot shows no signs of a lack of convergence. The table below also shows a summary of the posterior distrubution of each model parameter.\n\nposterior_jags |&gt; \n  summarise_draws() |&gt; \n  mutate(\n    plot_col = 1\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(\n    decimals = 2\n  ) \n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\nplot_col\n\n\n\n\nalpha\n−9.16\n−9.17\n0.02\n0.02\n−9.19\n−9.14\n1.00\n24,306.41\n23,025.54\n1.00\n\n\nbeta[1]\n0.31\n0.31\n0.04\n0.04\n0.24\n0.37\n1.00\n4,224.12\n7,457.87\n1.00\n\n\nbeta[2]\n−0.04\n−0.04\n0.03\n0.03\n−0.09\n0.00\n1.00\n7,980.91\n13,916.39\n1.00\n\n\nbeta[3]\n0.01\n0.01\n0.02\n0.02\n−0.03\n0.04\n1.00\n13,979.67\n19,509.81\n1.00\n\n\nbeta[4]\n0.01\n0.01\n0.03\n0.03\n−0.03\n0.05\n1.00\n7,132.44\n13,483.43\n1.00\n\n\nbeta[5]\n0.01\n0.01\n0.03\n0.03\n−0.03\n0.05\n1.00\n9,036.48\n14,989.34\n1.00\n\n\nbeta[6]\n−0.10\n−0.10\n0.03\n0.03\n−0.14\n−0.06\n1.00\n7,125.51\n11,216.85\n1.00\n\n\nm\n88.50\n86.20\n22.80\n21.92\n55.31\n129.51\n1.00\n15,875.78\n17,765.77\n1.00\n\n\n\n\n\n\n\n\nmcmc_trace(\n  posterior_jags\n)\n\n\n\n\n\nFor the Poisson model in (a), compute the posterior predictive distribution for each state with the number of gun laws set to zero. Repeat this with the number of gun laws set to 25 (the maximum number). According to these calculations, how would the number of deaths nationwide be affected by these policy changes? Do you trust these projections?\n\n\nSolution:\nHere it is important to keep in mind that if we scale the original X predictors, we need to apply the same scaling to the new values we use for 0 or 25 gun laws, i.e.\n\\[\n0_{\\mathrm{scaled}} = \\frac{0 - \\mathrm{mean}(X)}{ \\mathrm{SD}(X)}\n\\]\nand\n\\[\n25_{\\mathrm{scaled}} = \\frac{25 - \\mathrm{mean}(X)}{ \\mathrm{SD}(X)}\n\\]\nwhere X is a vector containing the number of gun laws in each state.\n\nmean_gunlaws &lt;- mean(GunLaws)\nsd_gunlaws &lt;- sd(GunLaws)\n\nscaled_zero &lt;- (0 - mean_gunlaws) / sd_gunlaws\nscaled_25 &lt;- (25 - mean_gunlaws) / sd_gunlaws\n\ndata = list(\n  X = my_X,\n  scaled_zero = scaled_zero,\n  scaled_25 = scaled_25,\n  N = N,\n  Y = Y,\n  N_obs = length(Y)\n)\n\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:N_obs) {\n        Y[i] ~ dnegbin(q[i], m)\n        q[i] &lt;- m / (m + N[i] * lambda[i])\n        log(lambda[i]) &lt;- alpha + inprod(X[i, ], beta[])\n    \n        Y0[i] ~ dnegbin(q0[i], m)\n        q0[i] &lt;- m / (m + N[i] * lambda0[i])\n        log(lambda0[i]) &lt;- alpha + inprod(X[i, 1:5], beta[1:5]) + scaled_zero * beta[6]\n    \n        Y25[i] ~ dnegbin(q25[i], m)\n        q25[i] &lt;- m / (m + N[i] * lambda25[i])\n        log(lambda25[i]) &lt;- alpha + inprod(X[i, 1:5], beta[1:5]) + scaled_25 * beta[6]\n        \n    }\n    for (j in 1:6) {\n        beta[j] ~ dnorm(0, 0.0001)\n    }\n    \n    national_difference &lt;- sum(Y25 - Y0)\n    alpha ~ dnorm(0, 0.0001)\n    m ~ dgamma(0.1, 0.1)\n}\")\n\nmodel &lt;- jags.model(\n  model_string,\n  data = data,\n  n.chains = 4,\n  quiet = TRUE\n)\n\nupdate(model, 10000, progress.bar = \"none\")\n\n\nsamples &lt;- coda.samples(\n  model,\n  variable.names = c(\"national_difference\"),\n  n.iter = 10000,\n  progress.bar = \"none\"\n)\n\nThe plot below shows the posterior distribution of the nation-wide reduction in homicides when all counties implement all 25 gun laws, versus if all counties implement zero gun laws. We see that 98% of the posterior mass lies beneath 0, so the model still estimates a 2% chance that implementing all gun laws will not reduce homicides by a measurable amount.\n\nsamples |&gt; \n  as_draws_df() |&gt; \n  subset_draws(variable = \"national_difference\") |&gt; \n  summarise_draws(\"P(x &lt; 0)\" = function(x) mean(x &lt; 0))\n\n# A tibble: 1 × 2\n  variable            `P(x &lt; 0)`\n  &lt;chr&gt;                    &lt;num&gt;\n1 national_difference       1.00\n\n\nThe plot below shows the posterior distribution of the reduction in homicides.\n\nsamples |&gt; \n  mcmc_areas(\n    pars = \"national_difference\",\n    prob = 0.95,\n    prob_outer = 1\n  ) +\n  labs(\n    title = \"Posterior distribution of reduction in homicides\",\n    subtitle = \"The blue area is a 95% credible interval for the reduction in total deaths\\nif all counties implemented all 25 gun laws versus 0 gun laws\"\n  )\n\n\n\n\n\n\n\n\n7\nConsider the one-way random effects model\n\\[\nY_{ij} \\vert \\alpha_i, \\sigma^2 \\sim \\mathrm{Normal}(\\alpha_i, \\sigma^2)\n\\]\nand\n\\[\n\\alpha_i \\sim \\mathrm{Normal}(0, \\tau^2)\n\\]\nfor \\(i = 1, \\dots, n\\) and \\(j = 1, \\dots, m\\). Assuming conjugate priors \\(\\sigma^2, \\tau^2 \\sim \\mathrm{InvGamma}(a, b)\\) derive the full conditional distributions of \\(\\alpha_1\\), \\(\\sigma^2\\) and \\(\\tau^2\\) and outline (but do not code) an MCMC algorithm to sample from the posterior.\n\nSolution:\n\\[\n\\begin{aligned}\n\\alpha_j \\vert rest &\\sim \\mathrm{Normal}\\left( \\frac{\\sum_{i=1}^n Y_{ij}}{n + \\sigma^2/\\tau^2}, \\frac{\\sigma^2}{n + \\sigma^2/\\tau^2}\\right) \\\\\n\\sigma^2 \\vert rest &\\sim \\mathrm{InvGamma}\\left(nm/2 + a, \\sum_{i=1}^{n} \\sum_{j=1}^{m}(Y_{ij} - a)^2/2 + b\\right) \\\\\n\\tau^2\\vert rest &\\sim \\mathrm{InvGamma}\\left(m/2+a, \\sum_{j=1}^{m}\\alpha^2_j/2 + b\\right)\n\\end{aligned}\n\\]\nTo implement the Gibbs sampler we first choose initial values for the parameters, then we go through the parameters sequentially and sample from their full conditional distributions outlined above.\n\n\n\n8\nLoad the gambia data from the geoR package in R. The response variable \\(Y_i\\) is the binary indicator that child \\(i\\) tested positive for malaria (pos) and the remaining seven variables are covariates.\n\nFit the logistic regression model\n\n\\[\n\\mathrm{logit[Prob(Y_i=1)]} = \\sum_{j=1}^p X_{ij}\\beta_j\n\\]\nwith uninformative priors for the \\(\\beta_j\\). Verify that the MCMC sampler has converged and summarize the effects of the covariates.\n\nSolution:\n\ndata(gambia, package = \"geoR\")\nY &lt;- gambia$pos\n\nX &lt;- gambia |&gt; select(-pos) |&gt; as.matrix() |&gt; scale()\n\ndata &lt;- list(\n  Y = Y, \n  N_obs = length(Y),\n  P = ncol(X), \n  X = X\n)\n\nmodel_string &lt;- textConnection(\"model{\n  #likelihood\n  for(i in 1:N_obs){\n    Y[i] ~ dbern(p[i])\n    logit(p[i]) &lt;- alpha + inprod(X[i,], beta[])\n  }\n  #priors\n  for(j in 1:P) {\n    beta[j] ~ dnorm(0, 0.01)\n  }\n  \n  alpha ~ dnorm(0, 0.0001)\n}\")\n\n\n# model &lt;- jags.model(model_string, data = data, n.chains = 2, )\n# update(model, 5000)\n# samples &lt;- coda.samples(model, variable.names = c(\"alpha\", \"beta\"), thin = 1, n.iter = 5000)\n# \n# write_rds(samples, \"8a_samples.rds\")\n\nsamples &lt;- read_rds(\"8a_samples.rds\")\n\nThe table below shows us that the \\(\\hat R\\) of our variables is sufficiently low and the trace plot shows no signs of a lack of convergence.\n\nsamples |&gt; \n  summarise_draws() |&gt; \n  gt() |&gt; \n  fmt_number()\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[1]\n0.38\n0.38\n0.09\n0.09\n0.23\n0.53\n1.00\n892.38\n1,605.03\n\n\nbeta[2]\n−0.34\n−0.34\n0.05\n0.05\n−0.42\n−0.25\n1.00\n4,283.74\n5,481.87\n\n\nbeta[3]\n0.27\n0.27\n0.05\n0.05\n0.19\n0.35\n1.00\n6,049.84\n5,459.74\n\n\nbeta[4]\n−0.32\n−0.32\n0.05\n0.05\n−0.40\n−0.23\n1.00\n3,965.28\n4,987.25\n\n\nbeta[5]\n−0.02\n−0.02\n0.06\n0.06\n−0.12\n0.09\n1.00\n2,897.16\n4,790.87\n\n\nbeta[6]\n−0.02\n−0.02\n0.09\n0.09\n−0.18\n0.13\n1.00\n879.93\n1,753.36\n\n\nbeta[7]\n−0.20\n−0.20\n0.05\n0.06\n−0.29\n−0.11\n1.00\n3,137.29\n4,538.17\n\n\n\n\n\n\n\n\nsamples |&gt; \n  mcmc_trace()\n\n\n\n\n\n\nIn this dataset, the 2035 children reside in \\(L = 65\\) unique locations (defined by the x and y coordinates in the dataset). Let \\(s_i \\in \\{1, \\dots, L\\}\\) be the label of the location for observation \\(i\\). Fit the random effects logistic regression model\n\n\\[\n\\begin{gathered}\n\\mathrm{logit[Prob(Y_i=1)]} = \\sum_{j=1}^p X_{ij}\\beta_j + \\alpha_{s_i} \\\\\n\\alpha_l \\sim \\mathrm{Normal}(0, \\tau^2)\n\\end{gathered}\n\\]\nand the \\(\\beta_j\\) and \\(\\tau^2\\) have uninformative priors. Verify that the MCMC sampler has converged; explain why random effects might be needed here; discuss and explain any differences in the posteriors of the regression coefficients that occur when random effects are added to the model; plot the posterior means of the \\(\\alpha_l\\) by their spatial locations and suggest how this map might be useful to malaria researchers.\n\nSolution:\n\n\nloc &lt;- gambia |&gt; group_by(x, y) |&gt; mutate(id = cur_group_id()) |&gt; pull(id)\n# \n# data &lt;- list(\n#   Y = Y, \n#   N_obs = length(Y),\n#   P = ncol(X), \n#   X = X,\n#   loc = loc,\n#   N_locations = max(loc)\n# )\n# \n# model_string &lt;- textConnection(\"model{\n#   #likelihood\n#   for(i in 1:N_obs){\n#     Y[i] ~ dbern(p[i])\n#     logit(p[i]) &lt;- inprod(X[i,], beta[]) + alpha + alpha_location[loc[i]]\n#   }\n#   #priors\n#   for (j in 1:P) {\n#     beta[j] ~ dnorm(0, 0.01)\n#   }\n#   \n#   for (l in 1:N_locations) {\n#     alpha_location[l] ~ dnorm(0, tau)\n#   }\n#   \n#   alpha ~ dnorm(0, 0.0001)\n#   tau ~ dgamma(0.01, 0.01)\n# }\")\n# \n# model &lt;- jags.model(model_string, data = data, n.chains = 2, )\n# update(model, 5000)\n# samples &lt;- coda.samples(model, variable.names = c(\"alpha\", \"alpha_location\", \"beta\", \"tau\"), thin = 1, n.iter = 5000)\n# write_rds(samples, \"8b_samples.rds\")\n\nsamples &lt;- read_rds(\"8b_samples.rds\")\n\n\nsamples |&gt; \n  summarise_draws() |&gt; \n  arrange(desc(rhat)) |&gt; \n  slice_head(n = 15) |&gt; \n  gt() |&gt; \n  fmt_number()\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[6]\n−0.07\n−0.08\n0.22\n0.22\n−0.44\n0.30\n1.02\n172.23\n454.23\n\n\nbeta[1]\n0.48\n0.49\n0.22\n0.21\n0.12\n0.85\n1.02\n172.84\n417.75\n\n\nalpha_location[2]\n−0.01\n0.00\n0.37\n0.37\n−0.62\n0.59\n1.01\n989.72\n1,776.91\n\n\nalpha_location[42]\n0.41\n0.41\n0.42\n0.41\n−0.28\n1.11\n1.01\n722.80\n2,525.51\n\n\nalpha_location[55]\n0.36\n0.35\n0.38\n0.38\n−0.27\n0.98\n1.01\n344.78\n854.75\n\n\nalpha_location[50]\n0.29\n0.29\n0.42\n0.41\n−0.40\n0.96\n1.01\n1,242.19\n3,149.55\n\n\nalpha_location[1]\n0.66\n0.66\n0.42\n0.42\n−0.02\n1.35\n1.01\n975.87\n2,127.75\n\n\nalpha_location[8]\n−1.03\n−1.02\n0.41\n0.41\n−1.73\n−0.37\n1.00\n1,401.36\n2,207.14\n\n\nbeta[7]\n−0.21\n−0.21\n0.12\n0.12\n−0.41\n−0.02\n1.00\n604.25\n1,093.46\n\n\nalpha_location[13]\n1.24\n1.23\n0.48\n0.48\n0.47\n2.05\n1.00\n521.08\n1,463.92\n\n\nalpha_location[56]\n0.82\n0.81\n0.42\n0.42\n0.13\n1.53\n1.00\n1,598.93\n3,065.11\n\n\nalpha\n−0.64\n−0.64\n0.12\n0.12\n−0.83\n−0.45\n1.00\n608.20\n1,089.73\n\n\nalpha_location[39]\n−0.43\n−0.42\n0.39\n0.38\n−1.07\n0.21\n1.00\n1,090.25\n2,400.16\n\n\nalpha_location[17]\n0.73\n0.73\n0.44\n0.45\n0.02\n1.46\n1.00\n1,900.27\n3,431.01\n\n\nalpha_location[53]\n−0.22\n−0.22\n0.42\n0.42\n−0.92\n0.48\n1.00\n818.92\n2,555.47\n\n\n\n\n\n\n\n\nsamples |&gt; \n  mcmc_trace(\n    regex_pars = c(\"beta\", \"alpha$\")\n  )\n\n\n\n\n\nalpha_means &lt;- samples |&gt; \n  as_draws_df() |&gt; \n  subset_draws(variable = \"alpha_location\", regex = TRUE) |&gt; \n  summarise_draws(mean)\n\n\nplot_dat &lt;- gambia |&gt; \n  mutate(loc = loc) |&gt; \n  distinct(loc, x, y) |&gt; \n  arrange(loc) |&gt; \n  mutate(\n    alpha = alpha_means$mean\n  )\n\n\nlibrary(sf)\nlibrary(leaflet)\n\npal &lt;- colorNumeric(\n  \"RdBu\",\n  domain = range(alpha_means$mean),\n  reverse = FALSE\n)\n\nplot_dat |&gt; \n  st_as_sf(\n    coords = c(\"x\", \"y\"),\n    crs = \"+proj=utm +zone=28\"\n  ) |&gt; \n  st_make_valid() |&gt; \n  st_transform(\"WGS84\") |&gt; \n  leaflet() |&gt; \n  addProviderTiles(providers$OpenStreetMap) |&gt; \n  addCircleMarkers(\n    fillColor = ~ pal(alpha),\n    weight = 0,\n    fillOpacity = 0.8,\n    radius = ~ 4 * (alpha + 2),\n    label = ~ paste(\"alpha: \", round(alpha, 3))\n  ) |&gt; \n  addLegend(\n    \"topright\",\n    pal = pal,\n    values = ~ alpha,\n    labFormat = labelFormat(transform = function(x) sort(x, decreasing = TRUE)),\n  )",
    "crumbs": [
      "Solutions",
      "Homework #6 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw3/solution/index.html",
    "href": "assignments/hw3/solution/index.html",
    "title": "Homework #3 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file\n```{r}\n#| output: hide\n\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(invgamma)\ntheme_set(theme_classic())\n```",
    "crumbs": [
      "Solutions",
      "Homework #3 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw3/solution/index.html#exercise-6",
    "href": "assignments/hw3/solution/index.html#exercise-6",
    "title": "Homework #3 (Solution)",
    "section": "Exercise 6",
    "text": "Exercise 6\nAn assembly line relies on accurate measurements from an image-recognition algorithm at the first stage of the process. It is known that the algorithm is unbiased, so assume that measurements follow a normal distribution with mean zero,\n\\[\nY_i\\vert \\sigma^2\\overset{\\mathrm{iid}}{\\sim}\\mathrm{Normal}(0, \\sigma^2).\n\\]\nSome errors are permissible, but if \\(\\sigma\\) exceeds the threshold \\(c\\) then the algorithm must be replaced.\nYou make \\(n = 20\\) measurements and observe\n\\[\n\\sum_{i=1}^n Y_i = -2 \\quad \\mathrm{and} \\quad \\sum_{i=1}^n Y_i^2 = 15,\n\\]\nand conduct a Bayesian analysis with \\(\\mathrm{InvGamma}(a,b)\\) prior. compute the posterior probability that \\(\\sigma &gt; c\\) for:\n\n\\(c=1\\) and \\(a=b=0.1\\)\n\n\nSolution:\nOur likelihood is\n\\[\n\\begin{aligned}\nf(\\mathbf Y\\vert \\sigma^2) &\\propto \\prod_{i=1}^n \\frac1\\sigma \\exp\\left[-\\frac{(Y_i - \\mu)^2}{2\\sigma^2}\\right] \\\\\n&\\propto (\\sigma^2)^{-n/2}\\exp\\left(-\\frac{\\mathrm{SSE}}{2\\sigma^2}\n\\right).\n\\end{aligned}\n\\]\nIn our case, we can write out SSE as\n\\[\n\\begin{aligned}\n\\mathrm{SSE} &= \\sum_{i=1}^n (Y_i - \\mu)^2 \\\\\n&= \\sum_{i=1}^n Y_i^2 - 2 Y_i\\mu + \\mu^2 \\\\\n&= \\sum_{i=1}^n Y_i^2, \\quad\\quad \\mu = 0.\n\\end{aligned}\n\\]\nOur inverse gamma prior is\n\\[\n\\pi(\\sigma^2) \\propto (\\sigma^2)^{-(a+1)}\\exp\\left(- \\frac{b}{\\sigma^2}\\right).\n\\]\nCombining our likelihood and prior we get the posterior\n\\[\n\\begin{aligned}\np(\\sigma^2\\vert\\mathbf Y) &\\propto f(\\mathbf Y\\vert \\sigma^2)\\pi(\\sigma^2) \\\\\n&\\propto (\\sigma^2)^{-(A+1)}\\exp\\left(- \\frac{B}{\\sigma^2}\\right),\n\\end{aligned}\n\\]\nwhere \\(A = n/2 + a\\) and \\(B = \\mathrm{SSE}/2 + b\\), and we therefore see that\n\\[\n\\sigma^2 \\vert \\mathbf Y \\sim \\mathrm{InvGamma}(A, B).\n\\]\nBy using the right values for \\(n\\), SSE, \\(a\\), \\(b\\), and \\(c\\) we can easily use R’s inbuilt pgamma function to answer the question. To use the pgamma function, we just need to remember that if\n\\[\nX \\sim \\mathrm{Gamma}(\\alpha, \\beta),\n\\]\nthen\n\\[\n\\frac1X \\sim \\mathrm{InvGamma}(\\alpha, \\beta).\n\\]\nThus, since \\(\\sigma^2\\) is assumed to follow an inverse gamma distribution we can answer the question by calculating\n\\[\nP\\left(\\frac1{\\sigma^2} &lt; \\frac 1{c^2}\\right).\n\\]\n\n```{r}\nn &lt;- 20\nSSE &lt;- 15\nc &lt;- 1\na &lt;- 0.1\nb &lt;- 0.1\n\nA &lt;- n/2 + a\nB &lt;- SSE/2 + b\n\npgamma(1/c^2, shape = A, rate = B)\n```\n\n[1] 0.2249838\n\n\nWe could also use the pinvgamma function from the invgamma package\n\n1 - pinvgamma(c^2, shape = A, rate = B)\n\n[1] 0.2249838\n\n\n\n\nCode\nplot_solution &lt;- function(A, B, c) {\n    p1 &lt;- ggplot() +\n        stat_function(\n            geom = \"area\",\n            fun = dinvgamma,\n            xlim = c(0, c^2),\n            args = list(shape = A, rate = B),\n            alpha = 0.4\n        ) +\n        stat_function(\n            geom = \"area\",\n            fun = dinvgamma,\n            xlim = c(c^2, qinvgamma(0.99999, shape = A, rate = B)),\n            args = list(shape = A, rate = B)\n        ) +\n        annotate(\n            geom = \"text\",\n            x = 1.1 * c^2 + 0.1,\n            y = dgamma(1/c^2, shape = A, rate = B) + 0.05,\n            label = expression(sigma^2&gt;c^2)\n        ) +\n        coord_cartesian(expand = FALSE) +\n        labs(\n            subtitle = \"Inverse gamma\",\n            x = expression(sigma^2)\n        )\n    \n    p2 &lt;- ggplot() +\n        stat_function(\n            geom = \"area\",\n            fun = dgamma,\n            xlim = c(0, 1/c^2),\n            args = list(shape = A, rate = B)\n        ) +\n        stat_function(\n            geom = \"area\",\n            fun = dgamma,\n            xlim = c(1/c^2, qgamma(0.99999, shape = A , rate = B)),\n            args = list(shape = A, rate = B),\n            alpha = 0.4\n        ) +\n        annotate(\n            geom = \"text\",\n            x = 1/ (c^2 * 1.3) - 0.1,\n            y = dgamma(1/c^2, shape = A, rate = B) + 0.05,\n            label = expression(frac(1,sigma^2)&lt;frac(1,c^2))\n        ) +\n        coord_cartesian(expand = FALSE, clip = \"off\") +\n        labs(\n            subtitle = \"Gamma\",\n            x = expression(frac(1,sigma^2))\n        )\n    \n    p1 + p2 +\n        plot_annotation(\n            title = \"Comparing solutions using the Gamma or Inverse Gamma\"\n        ) &\n        theme(\n            axis.text.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.title.y = element_blank(),\n            axis.line.y = element_blank()\n        )\n    \n}\n\nplot_solution(A, B, c)\n\n\n\n\n\n\n\n\\(c=1\\) and \\(a=b=1.0\\)\n\n\nSolution:\n\n```{r}\nn &lt;- 20\nSSE &lt;- 15\nc &lt;- 1\na &lt;- 1\nb &lt;- 1\n\nA &lt;- n/2 + a\nB &lt;- SSE/2 + b\n\npgamma(1/c^2, shape = A, rate = B)\n```\n\n[1] 0.236638\n\n\n\n1 - pinvgamma(c^2, shape = A, rate = B)\n\n[1] 0.236638\n\n\n\n\nCode\nplot_solution(A, B, c)\n\n\n\n\n\n\n\n\\(c=2\\) and \\(a=b=0.1\\)\n\n\nSolution:\n\n```{r}\nn &lt;- 20\nSSE &lt;- 15\nc &lt;- 2\na &lt;- 0.1\nb &lt;- 0.1\n\nA &lt;- n/2 + a\nB &lt;- SSE/2 + b\n\npgamma(1/c^2, shape = A, rate = B)\n```\n\n[1] 2.560058e-05\n\n\n\n1 - pinvgamma(c^2, shape = A, rate = B)\n\n[1] 2.560058e-05\n\n\n\n\nCode\nplot_solution(A, B, c)\n\n\n\n\n\n\n\n\\(c=2\\) and \\(a=b=1.0\\)\n\n\nSolution:\n\n```{r}\nn &lt;- 20\nSSE &lt;- 15\nc &lt;- 2\na &lt;- 1\nb &lt;- 1\n\nA &lt;- n/2 + a\nB &lt;- SSE/2 + b\n\npgamma(1/c^2, shape = A, rate = B)\n```\n\n[1] 1.44581e-05\n\n\n\n1 - pinvgamma(c^2, shape = A, rate = B)\n\n[1] 1.44581e-05\n\n\n\n\nCode\nplot_solution(A, B, c)\n\n\n\n\n\n\nFor each \\(c\\), compute the ratio of probabilities for the two priors. Which, if any of the results are sensitive to the prior?\n\nSolution: In the table below, we compare the two choices of priors for each value of \\(c\\). We see that the ration between the two probabilities is close to \\(1\\) for \\(c=1\\), so it is relatively insensitive to the priors. On the other hand the ratio is \\(0.56\\) for \\(c=2\\) which tells us that our inference is sensitive to the choice of prior distribution when \\(c=2\\).\n\n\nCode\ncrossing(\n    c = c(1, 2),\n    ab = c(0.1, 1)\n) |&gt; \n    mutate(\n        A = n/2 + ab,\n        B = SSE/2 + ab,\n        prob = 1 - pinvgamma(c^2, shape = A, rate = B)\n    ) |&gt; \n    select(c, ab, prob) |&gt; \n    pivot_wider(names_from = ab, values_from = prob) |&gt; \n    mutate(\n        ratio = `1` / `0.1`\n    ) |&gt; \n    gt() |&gt; \n    tab_spanner(\n        label = \"Probability for each prior\",\n        columns = c(`0.1`, `1`)\n    ) |&gt; \n    cols_label(\n        `0.1` = \"a,b=0.1\",\n        `1` = \"a,b=1\"\n    ) |&gt; \n    fmt_number(\n        decimals = 5, rows = 2\n    ) |&gt; \n    fmt_number(\n        decimals = 3, rows = 1\n    ) |&gt; \n    fmt_number(decimals = 0, columns = 1)\n\n\n\n\n\n\n\n\nc\nProbability for each prior\nratio\n\n\na,b=0.1\na,b=1\n\n\n\n\n1\n0.225\n0.237\n1.052\n\n\n2\n0.00003\n0.00001\n0.56476",
    "crumbs": [
      "Solutions",
      "Homework #3 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw3/solution/index.html#exercise-16",
    "href": "assignments/hw3/solution/index.html#exercise-16",
    "title": "Homework #3 (Solution)",
    "section": "Exercise 16",
    "text": "Exercise 16\nSay \\(Y\\vert\\lambda \\sim \\mathrm{Gamma}(1, \\lambda).\\)\n\nDerive and plot the Jeffreys’ prior for \\(\\lambda\\)\n\n\nSolution: Jeffreys’ prior is\n\\[\n\\pi(\\theta) \\propto \\sqrt{I(\\theta)},\n\\]\nwhere \\(I(\\theta)\\) is the expected Fisher information, defined as\n\\[\nI(\\theta) = -E\\left( \\frac{d^2\\log f(Y\\vert \\theta)}{d\\theta^2}\\right).\n\\]\nThe PDF of the Gamma distribution can be written\n\\[\nf(x \\vert a, b) = \\frac{b^a}{\\Gamma(a)}x^{a-1}\\exp\\left(-xb\\right).\n\\]\nIn our case, \\(a=1\\) and \\(b=\\lambda\\)\n\\[\n\\begin{aligned}\nf(Y\\vert \\lambda) &= \\frac{\\lambda^1}{\\Gamma(1)}Y^0\\exp(-\\lambda Y) \\\\\n&= \\lambda \\exp(-\\lambda Y).\n\\end{aligned}\n\\]\nTaking the log we get\n\\[\n\\begin{aligned}\nl(Y\\vert\\theta) &=\\log f(Y\\vert\\lambda) \\\\\n&= \\log\\lambda - \\lambda Y \\\\\n\\frac{dl}{d\\lambda} &= \\frac1\\lambda - Y \\\\\n\\frac{dl^2}{d\\lambda^2} &= -\\frac{1}{\\lambda^2} \\\\\n&= -I(\\lambda).\n\\end{aligned}\n\\]\nWe thus see that\n\\[\n\\begin{aligned}\n\\pi(\\theta) &= \\sqrt{I(\\theta)} \\\\\n&= \\sqrt{\\frac{1}{\\lambda^2}} \\\\\n&= \\frac1\\lambda\n\\end{aligned}\n\\]\n\nggplot() +\n    stat_function(\n        geom = \"area\",\n        fun = function(x) 1 / x,\n        xlim = c(0, 20),\n        col = \"black\",\n        alpha = 0.5\n    ) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = expression(lambda),\n        y = expression(paste(pi, \"(\", lambda, \")\"))\n    )\n\n\n\n\n\n\nIs this prior proper?\n\n\nSolution: This is not a proper prior since it does not integrate to one.\n\n\nDerive the posterior and give conditions on \\(Y\\) to ensure it is proper.\n\n\nSolution:\nOur posterior is\n\\[\n\\begin{aligned}\np(\\lambda\\vert Y) &= \\frac{f(\\mathbf y\\vert \\lambda)\\pi(\\lambda)}{\\int_0^\\infty f(\\mathbf y\\vert \\lambda)\\pi(\\lambda)d\\lambda} \\\\\n&= \\frac{\\lambda \\exp(-\\lambda Y) \\cdot \\frac1\\lambda}{\\int_0^\\infty \\lambda \\exp(-\\lambda Y) \\cdot \\frac1\\lambda d\\lambda} \\\\\n&= \\frac{\\exp(-\\lambda Y)}{\\int_0^\\infty \\exp(-\\lambda Y) d\\lambda}\n\\end{aligned}\n\\]\nThe integral of \\(\\exp(-\\lambda Y)\\) from \\(0\\) to \\(\\infty\\) is\n\\[\n\\int_0^\\infty \\exp(-\\lambda Y)d\\lambda =  -\\frac1Y \\left[\\exp(-\\lambda Y) \\right]_0^\\infty.\n\\]\nIf \\(Y&gt;0\\), then this becomes \\(\\frac1Y\\), giving us the posterior distribution\n\\[\np(\\lambda\\vert Y) = Y \\exp(-Y\\lambda),\n\\]\nwhich is proper, since we assumed that \\(Y&gt;0\\).\nIf \\(Y = 0\\), then our posterior distribution will not integrate to one and thus will not be proper. We therefore require that \\(Y &gt; 0\\). Luckily \\(Y=0\\) with probability 0 as Y is a continuous variable.",
    "crumbs": [
      "Solutions",
      "Homework #3 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw3/solution/index.html#exercise-18",
    "href": "assignments/hw3/solution/index.html#exercise-18",
    "title": "Homework #3 (Solution)",
    "section": "Exercise 18",
    "text": "Exercise 18\nThe data in the table below are the result of a survey of commuters in 10 counties likely to be affected by a proposed addition of a high occupancy vehicle (HOV) lane.\n\nd &lt;- tribble(\n    ~County, ~Approve, ~Disapprove,\n    1, 12, 50,\n    2, 90, 150,\n    3, 80, 63,\n    4, 5, 10,\n    5, 63, 63,\n    6, 15, 8,\n    7, 67, 56,\n    8, 22, 19,\n    9, 56, 63,\n    10, 33, 19\n)\n\nd |&gt; \n    gt()\n\n\n\n\n\n\n\nCounty\nApprove\nDisapprove\n\n\n\n\n1\n12\n50\n\n\n2\n90\n150\n\n\n3\n80\n63\n\n\n4\n5\n10\n\n\n5\n63\n63\n\n\n6\n15\n8\n\n\n7\n67\n56\n\n\n8\n22\n19\n\n\n9\n56\n63\n\n\n10\n33\n19\n\n\n\n\n\n\n\n\nAnalyze the data in each county separately using the Jeffreys’ prior distribution and report the posterior 95% credible set for each county.\n\n\nSolution: We assume the data follow a binomial distribution\n\\[\nf(Y\\vert p) = \\binom{N}{Y}p^Y(1-p)^{N-Y}.\n\\]\nTo derive Jeffreys’ prior we first find the expected Fisher information\n\\[\n\\begin{aligned}\nl(Y\\vert p) &= \\log f(Y\\vert p) \\\\\n&= \\log\\binom{N}{Y} + Y\\log p + (N - Y)\\log(1 - p) \\\\\n\\frac{dl}{dp} &= \\frac Yp + \\frac{N-Y}{1-p} \\\\\n\\frac{dl^2}{dp^2} &= -\\frac{Y}{p^2} - \\frac{N - Y}{(1 - p)^2}.\n\\end{aligned}\n\\]\nThe expected value of \\(Y\\) is \\(Np\\) and the expected value of \\(N - Y\\) is \\(N - Np\\).\n\\[\n\\begin{aligned}\nI(p) &= \\frac{Np}{p^2} + \\frac{N - Np}{(1 - p)^2} \\\\\n&= \\frac{N}{p} + \\frac{N}{1 - p} \\\\\n&= \\frac{N}{p(1 - p)}.\n\\end{aligned}\n\\]\nJeffreys’ prior is therefore\n\\[\n\\pi(p) \\propto \\sqrt{\\frac{n}{p(1 - p)}} \\propto p^{1/2-1}(1-p)^{1/2-1},\n\\]\nwhich is the kernel of a \\(\\mathrm{Beta}(0.5, 0.5)\\) PDF. We can thus see that Jeffreys’ prior for a binomial proportion is a \\(\\mathrm{Beta}(0.5, 0.5)\\) distribution, and the posterior will be a \\(\\mathrm{Beta}(0.5 + Y, 0.5 + (N - Y))\\) distribution.\n\n```{r}\njeffreys_results &lt;- d |&gt; \n    mutate(\n        A = Approve + 0.5,\n        B = Disapprove + 0.5,\n        Lower = qbeta(0.025, A, B),\n        Upper = qbeta(0.975, A, B)\n    ) \n\njeffreys_results |&gt; \n    gt() |&gt; \n    cols_hide(\n        columns = c(Approve, Disapprove, A, B)\n    ) |&gt; \n    tab_spanner(\n        label = \"95% Credible Interval\", \n        columns = c(Lower, Upper)\n    ) |&gt; \n    fmt_percent(\n        columns = c(Lower, Upper)\n    )\n```\n\n\n\n\n\n\n\nCounty\n95% Credible Interval\n\n\nLower\nUpper\n\n\n\n\n1\n11.04%\n30.45%\n\n\n2\n31.55%\n43.74%\n\n\n3\n47.76%\n63.89%\n\n\n4\n14.03%\n58.42%\n\n\n5\n41.35%\n58.65%\n\n\n6\n44.89%\n81.98%\n\n\n7\n45.65%\n63.08%\n\n\n8\n38.58%\n68.24%\n\n\n9\n38.25%\n56.01%\n\n\n10\n49.93%\n75.54%\n\n\n\n\n\n\n\n\n\nLet \\(\\hat p_i\\) be the sample proportion of commuters in county \\(i\\) that approve of the HIV lane (e.g. \\(\\hat p_1 = 12/(12+50)=0.194\\)). Select \\(a\\) abd \\(b\\) so that the mean and variance of the \\(\\mathrm{Beta}(a,b)\\) distribution match the mean and variance of the sample proportions \\(\\hat p_1, \\dots, \\hat p_{10}\\).\n\n\nSolution:\n\nd |&gt; \n    mutate(\n        p = Approve / (Approve + Disapprove)\n    ) |&gt; \n    summarise(\n        mean = mean(p),\n        var = var(p)\n    ) |&gt; \n    gt() |&gt; \n    cols_label(\n        mean = \"Mean\",\n        var = \"Variance\"\n    )\n\n\n\n\n\n\n\nMean\nVariance\n\n\n\n\n0.4800001\n0.02025887\n\n\n\n\n\n\n\nSo we want the following to be approximately true\n\\[\n\\frac{\\alpha}{\\alpha + \\beta} = 0.48 \\qquad \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} =0.02\n\\]\nWe can solve this simply by creating a target function and using R’s built-in optim function\n\n```{r}\nmy_fun &lt;- function(pars) {\n    \n    alpha &lt;- pars[1]\n    beta &lt;- pars[2]\n    \n    # Calculate difference between current mean and desired mean\n    diff_mean &lt;- alpha / (alpha + beta) - 0.48\n    \n    # Calculate difference between current variance and desired variance\n    diff_var &lt;- (alpha * beta) / ((alpha + beta)^2 * (alpha + beta + 1)) - 0.02\n    \n    # Return the squared differences of both\n    diff_mean^2 + diff_var^2\n}\n\nresults &lt;- optim(c(1, 1), fn = my_fun)\n\nalpha &lt;- results$par[1]\nbeta &lt;- results$par[2]\n\nalpha\nbeta\n```\n\n[1] 5.51061\n[1] 5.96976\n\n\n\n\nConduct an empirical Bayesian analysis by computing the 95% posterior credible sets that results from analyzing each county separately using the \\(\\mathrm{Beta}(a,b)\\) prior you computed in b.\n\n\nSolution:\n\n```{r}\nemp_bayes_results &lt;- d |&gt; \n    mutate(\n        A = Approve + alpha,\n        B = Disapprove + beta,\n        Lower = qbeta(0.025, A, B),\n        Upper = qbeta(0.975, A, B)\n    ) \n\nemp_bayes_results |&gt; \n    gt() |&gt; \n    cols_hide(\n        columns = c(Approve, Disapprove, A, B)\n    ) |&gt; \n    tab_spanner(\n        label = \"95% Credible Interval\", \n        columns = c(Lower, Upper)\n    ) |&gt; \n    fmt_percent(\n        columns = c(Lower, Upper)\n    )\n```\n\n\n\n\n\n\n\nCounty\n95% Credible Interval\n\n\nLower\nUpper\n\n\n\n\n1\n14.87%\n34.13%\n\n\n2\n32.09%\n44.05%\n\n\n3\n47.48%\n63.09%\n\n\n4\n22.30%\n58.56%\n\n\n5\n41.52%\n58.15%\n\n\n6\n42.92%\n75.01%\n\n\n7\n45.48%\n62.24%\n\n\n8\n39.00%\n65.66%\n\n\n9\n38.66%\n55.70%\n\n\n10\n48.47%\n72.23%\n\n\n\n\n\n\n\n\n\nHow do the results from a. and c. differ? What are the advantages and disadvantages of these two analyses?\n\n\nSolution: The Empirical Bayes (EB) prior pulls each individual proportion slightly towards the overall mean. Since each county gets the same EB prior the effect is that counties with fewer votes get pulled more towards the overall mean, thus stabilizing the inferences in smaller counties by “borrowing” information from other counties.\nThe downside to using the EB prior is that the inference for each county is slightly biased to wards the overall mean.\nThis is an example of the bias/variance trade-off in statistics. We often want to reduce the variance in our statistical inference at the cost of increasing the bias in our inferences.\n\n\nCode\n```{r}\n#| code-fold: true\n\nemp_bayes_results |&gt; \n    mutate(\n        type = \"Empirical Bayes\"\n    ) |&gt; \n    bind_rows(\n        jeffreys_results |&gt; \n            mutate(\n                type = \"Jeffreys\"\n            )\n    ) |&gt; \n    ggplot(aes(x = County, ymin = Lower, ymax = Upper, col = type)) +\n    geom_hline(\n        yintercept = 0.48,\n        lty = 2,\n        linewidth = 0.5\n    ) +\n    geom_linerange(\n        linewidth = 1,\n        position = position_dodge(width = 0.3)\n        ) +\n    scale_x_continuous(\n        breaks = 1:10\n    ) +\n    scale_y_continuous(\n        limits = c(0, 1),\n        labels = scales::label_percent(),\n        expand = expansion()\n    ) +\n    annotate(\n        geom = \"text\",\n        x = 1.5,\n        y = 0.51,\n        label = \"Overall Mean\"\n    ) +\n    theme(legend.position = c(0.15, 0.9)) +\n    labs(\n        x = \"County\",\n        y = expression(hat(p)),\n        col = NULL,\n        title = \"Comparing the Jeffreys and Empirical Bayes intervals\",\n        subtitle = \"Empirical Bayes pulls the intervalls slightly towards the overall mean\"\n    )\n```",
    "crumbs": [
      "Solutions",
      "Homework #3 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw4/solution/index.html",
    "href": "assignments/hw4/solution/index.html",
    "title": "Homework #4 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n\n```{r}\n#| output: hide\n\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(cubature)\nlibrary(purrr)\nlibrary(scales)\nlibrary(glue)\nlibrary(rjags)\nlibrary(posterior)\nlibrary(bayesplot)\ntheme_set(theme_classic())\n```\n\n\n2\nAssume that \\(Y_i\\vert\\mu\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(\\mu,\\sigma_i^2)\\) for \\(i \\in \\{1, \\dots, n\\}\\), with \\(\\sigma_i\\) known and improper prior distribution \\(\\pi(\\mu)=1\\) for all \\(\\mu\\).\n\nGive a formula for the MAP estimator for \\(\\mu\\)\n\n\nSolution:\nWith a flat prior, our posterior distribution is simply equal to our likelihood.\n\\[\n\\begin{aligned}\np(\\mu \\vert \\mathbf Y) &= f(\\mathbf Y \\vert \\mu, \\mathbf \\sigma) \\\\\n&= \\prod_{i = 1}^n\\sigma_i^{-1} (2\\pi)^{-1/2}\\exp\\left[-\\frac{(Y_i - \\mu)^2}{2\\sigma_i^2}\\right] \\\\\n&\\propto \\prod_{i = 1}^n\\sigma_i^{-1} \\exp\\left[-\\frac{(Y_i - \\mu)^2}{2\\sigma_i^2}\\right] \\\\\n&= G(\\mu)\n\\end{aligned}\n\\]\nTake the log on both sides\n\\[\n\\begin{aligned}\n\\log G(\\mu) &= g(\\mu) \\\\\n&=\\sum_{i=1}^n-\\log\\sigma_i - \\frac{1}{2\\sigma_i^2}(Y_i - \\mu)^2 \\\\\n&= -\\sum_{i=1}^n n_i\\log\\sigma_i - \\frac{1}{2\\sigma_i^2}(Y_i^2 - 2Y_i\\mu + \\mu^2) \\\\\n\\frac{dg}{d\\mu} &= \\sum_{i=1}^n\\frac{1}{2\\sigma_i^2}(2\\mu - 2Y_i) \\\\\n&= \\sum_{i=1}^n\\frac{1}{\\sigma_i^2}(\\mu - Y_i)\n\\end{aligned}\n\\]\nTo find the MAP estimator, we find where this is equal to zero, i.e. \\(dg/d\\mu = 0\\).\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n\\frac{1}{\\sigma_i^2}\\mu &= \\sum_{i=1}^n\\frac{1}{\\sigma_i^2}Y_i \\\\\n\\rightarrow \\mu &= \\frac{\\sum_{i=1}^n\\frac{1}{\\sigma_i^2}Y_i}{\\sum_{i=1}^n\\frac{1}{\\sigma_i^2}}.\n\\end{aligned}\n\\]\nWe see that the MAP estimator is simply a weighted average of the \\(Y_i\\)’s, where each observations contribution to the mean is inversely proportional to its variance. If all \\(Y_i\\) have the same variance, then this simplifies to the usual mean.\n\n\nWe observe \\(n=3, Y_1=12, Y_2=10, Y_3=22, \\sigma_1=\\sigma_2=3\\) and \\(\\sigma_3=10\\), compute the MAP estimate of \\(\\mu\\).\n\n\nSolution:\n\nmy_MAP &lt;- function(n, Y, sigma) {\n    if (length(Y) != n) stop(\"Y should have length equal to n\")\n    if (length(sigma) != n) stop(\"sigma should have length equal to n\")\n    \n    sum(Y / sigma^2) / sum(1/sigma^2)\n}\n\nn &lt;- 3\nY &lt;- c(12, 10, 22)\nsigma &lt;- c(3, 3, 10)\n\nmu_MAP &lt;- my_MAP(n, Y, sigma)\n\nmu_MAP\n\n[1] 11.47368\n\n\n\n\nUse numerical integration to compute the posterior mean of \\(\\mu\\).\n\n\nSolution:\nThe posterior distribution, being equal to the likelihood, is normalized with respect to \\(Y\\), but not necessarily with respect to \\(\\mu\\). Therefore we first integrate over \\(\\mu\\) to calculate the normalizing constant and then we can calculate the posterior mean.\n\nunnormalized_posterior &lt;- function(mu) {\n    logpost_unnormalized &lt;- 0\n    for (i in seq_along(Y)) {\n        logpost_unnormalized &lt;- logpost_unnormalized + dnorm(Y[i], mean = mu, sd = sigma[i], log = TRUE)\n    }\n    exp(logpost_unnormalized)\n}\n\nmarginal &lt;- integrate(unnormalized_posterior, lower = -Inf, upper = Inf)$value\nmarginal\n\n[1] 0.001840506\n\n\n\nunnormalized_posterior_mean &lt;- function(mu) {\n    logpost_unnormalized &lt;- 0\n    for (i in seq_along(Y)) {\n        logpost_unnormalized &lt;- logpost_unnormalized + dnorm(Y[i], mean = mu, sd = sigma[i], log = TRUE)\n    }\n    mu * exp(logpost_unnormalized)\n}\n\npost_mean_unnorm &lt;- integrate(unnormalized_posterior_mean, lower = -Inf, upper = Inf)$value\n\nmu_MEAN &lt;- post_mean_unnorm / marginal\nmu_MEAN\n\n[1] 11.47368\n\n\n\n\nPlot the posterior distribution of \\(\\mu\\) and indicate the MAP and the posterior mean estimates on the plot.\n\n\nSolution:\n\ntibble(\n    mu = 11.47 + 7 * seq(-1, 1, length.out = 1e2)\n) |&gt; \n    mutate(\n        post = unnormalized_posterior(mu) / marginal\n    ) |&gt; \n    ggplot(aes(mu, post)) +\n    geom_area(\n        alpha = 0.5\n    ) +\n    geom_vline(xintercept = mu_MAP, lty = 4, col = \"red\") +\n    geom_vline(xintercept = mu_MEAN, lty = 2, col = \"blue\") +\n    scale_x_continuous(\n        breaks = c(mu_MEAN, seq(7, 16, by = 3)),\n        labels = label_number(accuracy = 0.01)\n    ) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = expression(mu),\n        y = expression(paste(p, \"(\", mu, \"|\", Y, \")\")),\n        title = expression(paste(\"Posterior distribution of \", mu)),\n        subtitle = \"The MAP and posterior mean are equal (expected result since posterior is symmetrical)\"\n    )\n\n\n\n\n\n\n\n4\nConsider the model\n\\[\nY_i\\vert\\sigma^2_i\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(0,\\sigma_i^2),\n\\]\nfor \\(i \\in \\{1, \\dots, n\\}\\) where\n\\[\n\\begin{gathered}\n\\sigma_i^2\\vert b \\sim \\mathrm{InvGamma}(a,b) \\\\ b\\sim\\mathrm{Gamma}(1,1)\n\\end{gathered}\n\\]\n\nDerive the full conditional posterior distributions for \\(\\sigma^2_1\\) and \\(b\\).\n\n\nSolution:\n\\[\n\\begin{aligned}\np(\\sigma^2_1\\vert Y, b) &\\propto f(Y \\vert \\sigma_1^2) \\pi(\\sigma_1^2\\vert b) \\\\\n&= \\mathrm{Normal}(Y\\vert \\sigma_1^2) \\cdot \\mathrm{InvGamma}(\\sigma_1^2\\vert a,b)\\\\\n&= {\\sigma_1}^{-1}(2\\pi)^{-1/2}\\exp\\left[-\\frac{{Y_1}^2}{2{\\sigma_1^2}}\\right] \\frac{{b}^{a}}{\\Gamma(a)}{(\\sigma_1^2)}^{-{a}-1}\\exp\\left(-{b}/{\\sigma_1^2}\\right) \\\\\n&\\propto {(\\sigma_1^2)}^{-1/2}\\exp\\left[-\\frac{{Y_1}^2}{2{\\sigma_1^2}}\\right] {(\\sigma_1^2)}^{-{a}-1}\\exp\\left(-{b}/{\\sigma_1^2}\\right) \\\\\n&= (\\sigma_1^2)^{-(a+1/2) - 1} \\exp\\left[-\\left(\\frac{Y_1^2}{2} + b\\right)/\\sigma_1^2\\right].\n\\end{aligned}\n\\]\nWe see that the full conditional posterior for \\(\\sigma_1^2\\) is an \\(\\mathrm{InvGamma}(a + 1/2, b + Y_1^2/2)\\) distribution.\n\\[\n\\begin{aligned}\np(b\\vert \\sigma_1, \\dots, \\sigma_n) &\\propto \\pi(b) \\prod_{i=1}^n\\pi(\\sigma_i^2\\vert b) \\\\\n&= \\exp\\left(-b\\right)\\prod_{i=1}^n \\frac{{b}^{a}}{\\Gamma(a)}{(\\sigma_i^2)}^{-{a}-1}\\exp\\left(-{b}/{\\sigma_i^2}\\right) \\\\\n&\\propto  \\exp\\left(-b\\right)\\prod_{i=1}^n {b}^{a}\\exp\\left(-{b}/{\\sigma_i^2}\\right) \\\\\n&= \\exp\\left(-b\\right)b^{na} \\exp\\left( \\sum_{i=1}^{n}{-b/\\sigma_i^2}\\right) \\\\\n&= b^{(1+na)-1}\\exp\\left(-b - \\sum_{i=1}^{n}{}\\frac{1}{ \\sigma_i^2}b\\right) \\\\\n&= b^{(1+na)-1} \\exp\\left(-\\left(1 + \\sum_{i=1}^{n}{}\\frac{1}{\\sigma_i^2}\\right)b\\right).\n\\end{aligned}\n\\]\nWe also see that the full conditional posterior for \\(b\\) is an \\(\\mathrm{Gamma}(1 + na, 1 + \\sum_{i=1}^{n}{}\\frac{1}{\\sigma_i^2})\\) distribution.\n\n\nWrite pseudocode for Gibbs sampling, i.e. describe in detail each step of the Gibbs sampling algorithm.\n\n\nSolution: In Gibbs sampling we iterate over each unknown parameter and sample from its full conditional posterior distribution. In our case the unknown parameters are the \\(\\sigma_i^2\\)’s and \\(b\\). To perform Gibbs sampling we would therefore do the following:\n\nset a large value for N_samples\nset a value for a\nread in data Y as length n vector\n\ninitialize b by sampling from prior\ninitialize sigma2 as a length n vector by sampling from prior\n\nfor i from 1 to N_samples:\n    for j from 1 to n:\n    sample sigma2[i] from InvGamma(a + 1/2, b + Y[i]^2/2) \nend for\nsample b from InvGamma(n*a, 1 + sum(1/sigma2))\n\n\n\nWrite your own Gibbs sampling code (not in JAGS) and plot the marginal posterior density of each parameter. Assume \\(n=10\\), \\(a=10\\) and \\(Y_i=i\\) for \\(i=1, \\dots, 10\\).\n\n\nSolution:\nWe’re going to use the posterior package to easily analyze our posterior distribution after we’ve run our Gibbs sampler. The posterior package gives us acces to a draws_df data type that we can create using the as_draws_df() function. To be able to convert our posterior into a draws_df, we want to save our results in an array with dimensions \\(\\mathrm{N_{sample}\\times N_{chain} \\times N_{variable}}\\). I am planning on performing 10000 samples in each of 4 chains and there are 11 total variables, so our array will have the shape \\(10000 \\times 4 \\times 11\\). We then have to keep track of which dimension we are indexing at each time.\nFirst we define functions for sampling from each parameter’s full conditional distribution.\n\nsample_sigma2 &lt;- function(a, b, Y) {\n    1/rgamma(length(Y), shape = a + 1/2, rate = b + Y^2/2)\n}\n\n\nsample_b &lt;- function(n, a, sigma2) {\n    rgamma(1, shape = n * a + 1, rate = 1 + sum(1/sigma2))\n}\n\nWe then write a function for Gibbs sampling that wraps around these two functions defined above. The gibbs_sampler function needs five inputs:\n\nn_samples: The number of samples we want from our posterior within each chain\nn_chain: The number of chains we want to run\nn_variables: The number of variables in our posterior\nY: Our observed data\na: For our inverse gamma prior\nn_burnin: The number of samples we want to draw as part of our burn-in. This is by default equal to n_samples\n\n\ngibbs_sampler &lt;- function(n_samples, n_chains, n_variables, Y, a, n_burnin = n_samples) {\n    \n    posterior &lt;- array(dim = c(n_samples, n_chains, n_variables))\n    dimnames(posterior)[[3]] &lt;- c(\"b\", glue(\"sigma2[{1:10}]\"))\n    \n    for (chain in seq_len(n_chains)) {\n        # Initial values for the Gibbs sampler\n        b_value &lt;- 1\n        sigma2_value &lt;- sample_sigma2(a, b_value, Y)\n        \n        # Burn-in period\n        # Run the sampler without saving our values\n        for (i in seq_len(n_burnin)) {\n            b_value &lt;- sample_b(n, a, sigma2_value)\n            sigma2_value &lt;- sample_sigma2(a, b_value, Y)\n        }\n        \n        # Running the Gibbs sampler for n_samples iterations\n        for (i in seq_len(n_samples)) {\n            # Here we perform the sampling\n            b_value &lt;- sample_b(n, a, sigma2_value)\n            sigma2_value &lt;- sample_sigma2(a, b_value, Y)\n            \n            # Here we store the samples in our array\n            # Pay close attention to the indexing\n            posterior[i, chain, 1] &lt;- b_value\n            posterior[i, chain, -1] &lt;- sigma2_value\n        }\n    }\n    # Convert our array into a draws_df\n    posterior |&gt; \n        as_draws_df()\n}\n\n\nn &lt;- 10\na &lt;- 10\nY &lt;- 1:10\n\n# 10xsigma2 + b = 11\nn_variables &lt;- n + 1\nn_chains &lt;- 4\nn_samples &lt;- 1e4\n\nposterior &lt;- gibbs_sampler(n_samples, n_chains, n_variables, Y, a)\n\nHaving done the preparatory work of converting our results into a draws_df object, we are rewarded with access the great bayesplot package for plotting our posterior.\n\nposterior |&gt; \n    mcmc_dens()\n\n\n\n\n\nsummarised_results_gibbs &lt;- posterior |&gt; \n    summarise_draws()\n\nsummarised_results_gibbs |&gt; \n    gt() |&gt; \n    fmt_number(\n        decimals = 2,\n        columns = mean:rhat\n    ) |&gt; \n    fmt_number(\n        decimals = 0,\n        columns = starts_with(\"ess\")\n    ) |&gt; \n    tab_header(\n        title = \"Summarizing the posterior distribution of our model\"\n    )\n\n\n\n\n\n\n\nSummarizing the posterior distribution of our model\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nb\n30.27\n29.99\n4.32\n4.28\n23.62\n37.80\n1.00\n13,202\n21,798\n\n\nsigma2[1]\n3.24\n3.01\n1.20\n1.01\n1.77\n5.49\n1.00\n23,158\n30,834\n\n\nsigma2[2]\n3.40\n3.16\n1.27\n1.05\n1.86\n5.75\n1.00\n24,415\n33,422\n\n\nsigma2[3]\n3.67\n3.41\n1.35\n1.12\n2.02\n6.19\n1.00\n26,382\n33,365\n\n\nsigma2[4]\n4.03\n3.75\n1.47\n1.22\n2.25\n6.76\n1.00\n27,234\n35,340\n\n\nsigma2[5]\n4.50\n4.19\n1.62\n1.34\n2.54\n7.47\n1.00\n29,581\n36,526\n\n\nsigma2[6]\n5.06\n4.72\n1.79\n1.49\n2.88\n8.41\n1.00\n29,017\n36,539\n\n\nsigma2[7]\n5.76\n5.37\n2.05\n1.69\n3.27\n9.56\n1.00\n32,461\n36,761\n\n\nsigma2[8]\n6.54\n6.08\n2.30\n1.90\n3.74\n10.82\n1.00\n31,719\n37,147\n\n\nsigma2[9]\n7.44\n6.97\n2.59\n2.19\n4.26\n12.23\n1.00\n33,021\n38,109\n\n\nsigma2[10]\n8.43\n7.89\n2.93\n2.46\n4.85\n13.87\n1.00\n34,668\n39,265\n\n\n\n\n\n\n\n\n\nRepeat the analysis with \\(a=1\\) and comment on the convergence of the MCMC chain.\n\n\nSolution: We can now reuse our code from above, only having to change the value of a\n\na &lt;- 1\n\nposterior &lt;- gibbs_sampler(n_samples, n_chains, n_variables, Y, a)\n\nHaving done the preparatory work of converting our results into a draws_df object, we are rewarded with the great functions available in the posterior package. The summarise_draws() function is a great way to easily obtain summarized results about our posterior.\nThe table below shows us that \\(\\hat R\\) and our effective sample size (ESS) are both acceptable, so it seems our sampler has converged.\nThe \\(\\hat R\\) is not the same as in the book, since the posterior package has implemented the proposed updates from Vehtari et al. (2021) that increase its ability to find faulty convergence. Have a look at the article to learn more about convergence. You can also read about \\(\\mathrm{ESS_{bulk}}\\) and \\(\\mathrm{ESS_{tail}}\\) there, but in simple terms, they calculate the ESS for the center and the tails of the posterior distributions.\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved Rˆ for Assessing Convergence of MCMC (with Discussion).” Bayesian Analysis 16 (2): 667–718. https://doi.org/10.1214/20-BA1221.\n\nsummarised_results_gibbs &lt;- posterior |&gt; \n    summarise_draws()\n\nsummarised_results_gibbs |&gt; \n    gt() |&gt; \n    fmt_number(\n        decimals = 2,\n        columns = mean:rhat\n    ) |&gt; \n    fmt_number(\n        decimals = 0,\n        columns = starts_with(\"ess\")\n    ) |&gt; \n    tab_header(\n        title = \"Summarizing the posterior distribution of our model\"\n    )\n\n\n\n\n\n\n\nSummarizing the posterior distribution of our model\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nb\n5.54\n5.29\n1.98\n1.87\n2.78\n9.18\n1.00\n22,778\n31,315\n\n\nsigma2[1]\n11.94\n4.91\n77.80\n4.07\n1.29\n34.32\n1.00\n28,978\n32,847\n\n\nsigma2[2]\n15.05\n6.18\n116.19\n5.00\n1.72\n43.32\n1.00\n32,136\n35,394\n\n\nsigma2[3]\n19.12\n8.39\n71.30\n6.70\n2.43\n58.11\n1.00\n35,603\n38,314\n\n\nsigma2[4]\n26.91\n11.36\n158.25\n9.03\n3.36\n79.20\n1.00\n37,190\n39,013\n\n\nsigma2[5]\n47.99\n15.31\n2,191.15\n12.04\n4.53\n105.45\n1.00\n38,626\n38,810\n\n\nsigma2[6]\n46.65\n19.81\n237.04\n15.55\n5.91\n136.49\n1.00\n39,356\n38,963\n\n\nsigma2[7]\n57.73\n25.41\n370.00\n19.80\n7.62\n169.80\n1.00\n39,748\n40,117\n\n\nsigma2[8]\n73.08\n31.55\n371.65\n24.81\n9.59\n214.45\n1.00\n38,877\n39,718\n\n\nsigma2[9]\n90.52\n38.77\n556.12\n30.41\n11.68\n256.58\n1.00\n39,099\n39,389\n\n\nsigma2[10]\n117.48\n46.73\n1,640.49\n36.44\n14.18\n323.50\n1.00\n39,349\n39,506\n\n\n\n\n\n\n\nWe also look at trace plots below. It is a good habit to visually inspect our chains even though calculated statistics tell us that the sampler has converged.\n\nposterior |&gt; \n    mcmc_trace(\n        regex_pars = \"sigma\",\n        transformations = \"log\"\n    ) +\n    labs(\n        title = expression(paste(\"Trace plots of log(\", sigma^2, \")\")),\n        subtitle = \"The trace plots show us that the Markov chain is stationary and the chains have converged\\nto the same value\"\n    )\n\n\n\n\n\nposterior |&gt; \n    mcmc_trace(\n        regex_pars = \"b\"\n    ) +\n    labs(\n        title = expression(paste(\"Trace plot of \", b)),\n        subtitle = \"The trace plots show us that the Markov chain is stationary and the chains have converged\\nto the same value\"\n    )\n\n\n\n\n\n\nImplement the model in c. using JAGS and compare the results with the results in c.\n\n\nSolution:\n\ndata = list(\n    Y = 1:10\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:10) {\n        Y[i] ~ dnorm(0, tau[i])\n    }\n    # Priors\n    for (i in 1:10) {\n        tau[i] ~ dgamma(10, b)\n        sigma2[i] &lt;- 1 / tau[i]\n    }\n    b ~ dgamma(1, 1)\n}\")\n\ninits &lt;- list(\n    b = 1,\n    tau = rep(1, 10)\n)\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    inits = inits,\n    n.chains = 4\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 10\n   Unobserved stochastic nodes: 11\n   Total graph size: 34\n\nInitializing model\n\nupdate(model, 10000)\n\nparams &lt;- names(inits)\nsamples &lt;- coda.samples(\n    model,\n    variable.names = c(\"b\", \"sigma2\"),\n    n.iter = 10000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\nThe posterior package allows us to take our samples object that was output by coda.samples() and use as_draws_df() to convert it into a comfortable data type.\n\nsummarised_results_jags &lt;- posterior_jags |&gt; \n    summarise_draws()\n\nsummarised_results_jags |&gt; \n    gt() |&gt; \n    fmt_number(\n        decimals = 2,\n        columns = mean:rhat\n    ) |&gt; \n    fmt_number(\n        decimals = 0,\n        columns = starts_with(\"ess\")\n    ) |&gt; \n    tab_header(\n        title = \"Summarizing the posterior distribution of our model\"\n    )\n\n\n\n\n\n\n\nSummarizing the posterior distribution of our model\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nb\n30.22\n29.93\n4.35\n4.30\n23.56\n37.81\n1.00\n12,681\n23,013\n\n\nsigma2[1]\n3.24\n3.00\n1.22\n1.02\n1.76\n5.51\n1.00\n24,648\n32,746\n\n\nsigma2[2]\n3.39\n3.15\n1.26\n1.05\n1.86\n5.75\n1.00\n24,955\n34,952\n\n\nsigma2[3]\n3.66\n3.41\n1.33\n1.13\n2.02\n6.16\n1.00\n25,124\n33,948\n\n\nsigma2[4]\n4.01\n3.74\n1.47\n1.22\n2.24\n6.75\n1.00\n26,377\n35,123\n\n\nsigma2[5]\n4.49\n4.19\n1.61\n1.35\n2.51\n7.51\n1.00\n28,630\n36,117\n\n\nsigma2[6]\n5.08\n4.72\n1.82\n1.52\n2.87\n8.53\n1.00\n30,448\n36,163\n\n\nsigma2[7]\n5.76\n5.37\n2.02\n1.68\n3.30\n9.53\n1.00\n32,158\n36,850\n\n\nsigma2[8]\n6.53\n6.09\n2.27\n1.91\n3.76\n10.76\n1.00\n34,940\n37,132\n\n\nsigma2[9]\n7.44\n6.95\n2.59\n2.17\n4.28\n12.33\n1.00\n33,573\n37,957\n\n\nsigma2[10]\n8.42\n7.86\n2.94\n2.43\n4.85\n13.91\n1.00\n35,401\n39,639\n\n\n\n\n\n\n\n\nsummarised_results_gibbs &lt;- gibbs_sampler(n_samples, n_chains, n_variables, Y, a = 10) |&gt; \n    summarise_draws()\n\nsummarised_results_combined &lt;- summarised_results_gibbs |&gt; \n    mutate(\n        type = \"Bespoke Gibbs\"\n    ) |&gt; \n    bind_rows(\n        summarised_results_jags |&gt; \n            mutate(\n                type = \"JAGS\"\n            )\n    )\n\n\nsummarised_results_combined |&gt; \n    filter(str_detect(variable, \"sigma\")) |&gt; \n    mutate(\n        variable = fct_reorder(variable, -parse_number(variable))\n    ) |&gt; \n    ggplot(aes(\n        y = variable,\n        x = mean, xmin = q5, xmax = q95,\n        col = type\n    )) +\n    geom_pointrange(\n        position = position_dodge(width = 0.7)\n    ) +\n    scale_color_brewer(palette = \"Set1\") +\n    labs(\n        x = NULL,\n        y = NULL,\n        title = \"Comparing our bespoke Gibbs sampler to JAGS\",\n        subtitle = \"The results seem to be identical\",\n        col = \"Sampler\"\n    )\n\n\n\n\n\n\nCode\nsummarised_results_combined |&gt; \n    filter(!str_detect(variable, \"sigma\")) |&gt; \n    ggplot(aes(\n        y = variable,\n        x = mean, xmin = q5, xmax = q95,\n        col = type\n    )) +\n    geom_pointrange(\n        position = position_dodge(width = 0.7)\n    ) +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"none\") +\n    labs(\n        x = NULL,\n        y = NULL,\n        title = \"Comparing our bespoke Gibbs sampler to JAGS\",\n        subtitle = \"The results seem to be identical\",\n        col = \"Sampler\"\n    )\n\n\n\n\n\n\n\n\n5\nConsider the model\n\\[\nY_i\\vert\\mu, \\sigma^2 \\sim \\mathrm{Normal}(\\mu,\\sigma^2),\\quad i=1,\\dots,n,\n\\]\nand\n\\[\nY_i\\vert\\mu,\\delta, \\sigma^2 \\sim \\mathrm{Normal}(\\mu+\\delta,\\sigma^2),\\quad i=n+1,\\dots,n+m,\n\\]\nwhere\n\\[\n\\begin{gathered}\n\\mu,\\delta\\sim\\mathrm{Normal}(0, 100^2) \\\\\n\\sigma^2\\sim\\mathrm{InvGamma}(0.01, 0.01).\n\\end{gathered}\n\\]\n\nGive an example of a real experiment for which this would be an appropriate model.\n\n\nSolution: A simple example would be a clinical trial for a pharmaceutical company wanting to know if a drug has a positive effect on patients. In this case, \\(\\mu\\) could be the average outcome of patients in the control group (who did not get the drug), and \\(\\mu + \\delta\\) the average outcome of patients in the case group (who did get the drug). The effect of the drug could then be estimated as \\(\\delta\\).\n\n\nDerive the full conditional posterior distributions for \\(\\mu\\), \\(\\delta\\), and \\(\\sigma^2\\).\n\n\nSolution:\nWe have three unknown parameters: \\(\\mu\\), \\(\\delta\\) and \\(\\sigma^2\\).\n\\[\n\\begin{aligned}\np(\\mu \\vert \\mathbf Y, \\sigma^2, \\delta) &\\propto f(\\mathbf Y \\vert \\mu, \\sigma^2, \\delta)\\pi(\\mu) \\\\\n&= \\prod_{i=1}^{n}{\\mathrm{Normal}(Y_i\\vert \\mu, \\sigma^2)} \\prod_{j=n+1}^{n+m}{\\mathrm{Normal}(Y_j\\vert \\mu, \\delta, \\sigma^2)} \\mathrm{Normal}(\\mu\\vert 0, 100^2) \\\\\n&= \\prod_{i=1}^{n}\\left({ {\\sigma}^{-1}(2\\pi)^{-1/2}\\exp\\left[-\\frac{({Y_i} - {\\mu})^2}{2{\\sigma}^2}\\right]}\\right) \\prod_{j=n+1}^{n+m}\\left({ {\\sigma}^{-1}(2\\pi)^{-1/2}\\exp\\left[-\\frac{({Y_j} - {(\\mu + \\delta)})^2}{2{\\sigma}^2}\\right]}\\right) {100}^{-1}(2\\pi)^{-1/2}\\exp\\left[-\\frac{\\mu^2}{2\\cdot {100}^2}\\right] \\\\\n&\\propto \\prod_{i=1}^{n}\\left({\\exp\\left[-\\frac{({Y_i} - {\\mu})^2}{2{\\sigma}^2}\\right]}\\right) \\prod_{j=n+1}^{n+m}\\left({ \\exp\\left[-\\frac{({Y_j} - {[\\mu + \\delta\n}])^2}{2{\\sigma}^2}\\right]}\\right) \\exp\\left[-\\frac{\\mu^2}{2\\cdot {100}^2}\\right] \\\\\n&= \\exp\\left[ -\\frac{1}{2\\sigma^2}\\left( \\sum_{i=1}^{n}{(Y_i - \\mu)^2 + \\sum_{j=n+1}^{n+m}{(Y_j - \\mu - \\delta)^2}}\\right) - \\frac{1}{2\\cdot100^2}\\mu^2\\right].\n\\end{aligned}\n\\]\nSimplifying and completing the square, we eventually end up with\n\\[\np(\\mu \\vert \\mathbf Y, \\sigma^2, \\delta) = \\mathrm{Normal}\\left( \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^{n}{Y_i} + \\frac{1}{\\sigma^2} \\sum_{j=n+1}^{n+m}{(Y_i- \\delta)}}{ \\frac{(n+m)}{\\sigma^2} + 1/100^2}, \\frac{1}{\\frac{(n+m)}{\\sigma^2} + 1/100^2}\\right).\n\\]\nSimilarly, we find out that\n\\[\np(\\delta \\vert \\mathbf Y, \\mu, \\sigma^2) = \\mathrm{Normal}\\left( \\frac{ \\frac{1}{\\sigma^2} \\sum_{j=n+1}^{n+m}{(Y_i-\\mu)}}{ \\frac{m}{\\sigma^2} + 1/100^2}, \\frac{1}{{ \\frac{m}{\\sigma^2} + 1/100^2}}\\right).\n\\]\nThe posterior for \\(\\sigma^2\\) is\n\\[\np(\\sigma^2 \\vert \\mathbf Y, \\mu, \\delta) = \\mathrm{InvGamma}\\left(0.01 + (n+m)/2, 0.01 + \\sum_{i=1}^{n}{(Y_i-\\mu)^2/2} + \\sum_{j=n+1}^{n+m}{(Y_i - \\mu - \\delta)^2/2}\\right)\n\\]\n\n\nSimulate a dataset from this model with \\(n=m=50\\), \\(\\mu=10\\), \\(\\delta=1\\) and \\(\\sigma=2\\). Write your own Gibbs sampling code (not in JAGS) to fit the model above to the simulated data and plot the marginal posterior density for each parameter. Are you able to recover the true values reasonably well?\n\n\nSolution:\n\nn &lt;- 50\nm &lt;- 50\nmu &lt;- 10\ndelta &lt;- 1\nsigma &lt;- 2\n\nY1 &lt;- rnorm(n, mu, sigma)\nY2 &lt;- rnorm(m, mu + delta, sigma)\n\n\nsample_mu &lt;- function(Y1, Y2, sigma2, delta) {\n    n &lt;- length(Y1)\n    m &lt;- length(Y2)\n    numerator &lt;- 1/sigma2 * (sum(Y1) + sum(Y2 - delta))\n    denominator &lt;- (n + m) / (sigma2) + 1/100^2\n    rnorm(\n        n = 1,\n        mean = numerator / denominator,\n        sd = 1 / sqrt(denominator)\n    )\n}\n\nsample_delta &lt;- function(Y2, mu, sigma2) {\n    m &lt;- length(Y2)\n    numerator &lt;- 1/sigma2 * sum(Y2 - mu)\n    denominator &lt;- m/sigma2 + 1/100^2\n    rnorm(\n        n = 1,\n        mean = numerator / denominator,\n        sd = 1 / sqrt(denominator)\n    )\n}\n\nsample_sigma2 &lt;- function(Y1, Y2, mu, delta) {\n    shape &lt;- 0.01 + (n + m) / 2\n    rate &lt;- 0.01 + sum((Y1 - mu)^2)/2 + sum((Y2 - mu - delta)^2)/2\n    n &lt;- length(Y1)\n    m &lt;- length(Y2)\n    1 / rgamma(\n        n = 1,\n        shape = shape,\n        rate = rate\n    )\n}\n\n\ngibbs_sampler &lt;- function(n_samples, n_chains, n_variables, Y1, Y2, n_burnin = n_samples) {\n    posterior &lt;- array(dim = c(n_samples, n_chains, n_variables))\n    dimnames(posterior)[[3]] &lt;- c(\"mu\", \"delta\", \"sigma2\")\n    \n    for (chain in seq_len(n_chains)) {\n        # Initial values for the Gibbs sampler\n        mu_value &lt;- rnorm(1, 0, 100)\n        delta_value &lt;- rnorm(1, 0, 100)\n        sigma2_value &lt;- 1 / rgamma(1, shape = 0.01, rate = 0.01)\n        \n        # Burn-in period\n        # Run the sampler without saving our values\n        for (i in seq_len(n_burnin)) {\n            mu_value &lt;- sample_mu(Y1, Y2, sigma2_value, delta_value)\n            delta_value &lt;- sample_delta(Y2, mu_value, sigma2_value)\n            sigma2_value &lt;- sample_sigma2(Y1, Y2, mu_value, delta_value)\n        }\n        \n        # Running the Gibbs sampler for n_samples iterations\n        for (i in seq_len(n_samples)) {\n            # Here we perform the sampling\n            mu_value &lt;- sample_mu(Y1, Y2, sigma2_value, delta_value)\n            delta_value &lt;- sample_delta(Y2, mu_value, sigma2_value)\n            sigma2_value &lt;- sample_sigma2(Y1, Y2, mu_value, delta_value)\n            \n            # Here we store the samples in our array\n            # Pay close attention to the indexing\n            posterior[i, chain, ] &lt;- c(mu_value, delta_value, sigma2_value)\n        }\n    }\n    # Convert our array into a draws_df\n    posterior |&gt; \n        as_draws_df()\n}\n\n\nn_samples &lt;- 1e3\nn_chains &lt;- 4\nn_variables &lt;- 3\n\nposterior &lt;- gibbs_sampler(n_samples, n_chains, n_variables, Y1, Y2)\n\nThe three plots below show us that out Gibbs sampler manages to adequately capture the true values of our parameters.\n\nposterior |&gt; \n    mcmc_areas(\n        pars = \"mu\",\n        prob = 0.95,\n        prob_outer = 0.99\n    ) +\n    geom_vline(xintercept = mu) +\n    labs(\n        title = expression(paste(\"Comparing the true value of \", mu, \" to our posterior distribution\")),\n        subtitle = \"The vertical line is the true value, the blue interval is the 95% interval and the white mass\\nis the 99% interval\"\n    )\n\n\n\n\n\nposterior |&gt; \n    mcmc_areas(\n        pars = \"delta\",\n        prob = 0.95,\n        prob_outer = 0.99\n    ) +\n    geom_vline(xintercept = delta) +\n    labs(\n        title = expression(paste(\"Comparing the true value of \", delta, \" to our posterior distribution\")),\n        subtitle = \"The vertical line is the true value, the blue interval is the 95% interval and the white mass\\nis the 99% interval\"\n    )\n\n\n\n\n\nposterior |&gt; \n    mcmc_areas(\n        pars = \"sigma2\",\n        prob = 0.95,\n        prob_outer = 0.99\n    ) +\n    geom_vline(xintercept = sigma^2) +\n    labs(\n        title = expression(paste(\"Comparing the true value of \", sigma^2, \" to our posterior distribution\")),\n        subtitle = \"The vertical line is the true value, the blue interval is the 95% interval and the white mass\\nis the 99% interval\"\n    )\n\n\n\n\n\n\nImplement this model using JAGS and compare the results with the results in .c.\n\n\nSolution:\n\ndata = list(\n    Y1 = Y1,\n    Y2 = Y2,\n    n = n,\n    m = m\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        Y1[i] ~ dnorm(mu, tau)\n    }\n    \n    for (i in 1:m) {\n        Y2[i] ~ dnorm(mu + delta, tau)\n    }\n    # Priors\n    mu ~ dnorm(0, 0.01^2)\n    delta ~ dnorm(0, 0.01^2)\n    tau ~ dgamma(0.01, 0.01)\n    sigma2 &lt;- 1 / tau\n}\")\n\ninits &lt;- list(\n    mu = mean(Y1),\n    delta = mean(Y2) - mean(Y1),\n    tau = 1 / var(Y1)\n)\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    inits = inits,\n    n.chains = 4\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 100\n   Unobserved stochastic nodes: 3\n   Total graph size: 112\n\nInitializing model\n\nupdate(model, 10000)\n\nparams &lt;- names(inits)\nsamples &lt;- coda.samples(\n    model,\n    variable.names = c(\"mu\", \"delta\", \"sigma2\"),\n    n.iter = 10000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\n\nsummarised_results_combined &lt;- posterior |&gt; \n    summarise_draws() |&gt; \n    mutate(\n        type = \"Bespoke Gibbs\"\n    ) |&gt; \n    bind_rows(\n        posterior_jags |&gt; \n            summarise_draws() |&gt; \n            mutate(\n                type = \"JAGS\"\n            )\n    ) |&gt; \n    mutate(\n        variable = ifelse(\n            variable == \"sigma2\",\n            \"sigma^2\",\n            variable\n        )\n    )\n\n\nsummarised_results_combined |&gt; \n    ggplot(aes(\n        y = type,\n        x = mean, xmin = q5, xmax = q95,\n        col = type\n    )) +\n    geom_pointrange(\n        position = position_dodge(width = 0.7)\n    ) +\n    scale_color_brewer(palette = \"Set1\") +\n    facet_wrap(\"variable\", scales = \"free_x\", nrow = 1, labeller = label_parsed) +\n    theme(legend.position = \"top\") +\n    labs(\n        x = NULL,\n        y = NULL,\n        title = \"Comparing our bespoke Gibbs sampler to JAGS\",\n        subtitle = \"The results seem to be identical\",\n        col = \"Sampler\"\n    )\n\n\n\n\n\n\n\n10\nAs discussed in section 1.6, report that the number of marine bivalve species discovered each year from 2010-2015 was 64, 13, 33, 18, 30 and 20. Denote \\(Y_t\\) as the number of species discovered in year \\(2009+t\\) (so that \\(Y_1=64\\) is the count for 2010). Use JAGS to fit the model\n\\[\n\\begin{gathered}\nY_t\\vert \\alpha, \\beta \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Poisson}(\\lambda_t)\\\\\n\\lambda_t = \\exp(\\alpha + \\beta t) \\text{ or equivalently } \\log(\\lambda_t) = \\alpha + \\beta t \\\\\n\\alpha,\\beta \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(0,10^2).\n\\end{gathered}\n\\]\nSummarize the posterior of \\(\\alpha\\) and \\(\\beta\\) and verify that the MCMC sampler has converged. Does this analysis provide evidence that the rate of discovery is changing over time?\n\ndata = list(\n    Y = c(64, 13, 33, 18, 30, 20),\n    n = 6\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        Y[i] ~ dpois(lambda[i])\n        lambda[i] &lt;- exp(alpha + beta * i)\n    }\n    \n    alpha ~ dnorm(0, 0.1^2)\n    beta ~ dnorm(0, 0.1^2)\n}\")\n\ninits &lt;- list(\n    alpha = 0,\n    beta = 0\n)\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    inits = inits,\n    n.chains = 4\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 6\n   Unobserved stochastic nodes: 2\n   Total graph size: 36\n\nInitializing model\n\nupdate(model, 10000)\n\nparams &lt;- names(inits)\nsamples &lt;- coda.samples(\n    model,\n    variable.names = params,\n    n.iter = 10000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\nThe table below shows us that that here is a high posterior probability that \\(\\beta &lt; 0\\). Thus we can infer that the rate is changing over time.\n\nposterior_jags |&gt; \n    summarise_draws() |&gt; \n    gt() |&gt; \n    tab_header(\n        title = \"Summary of our posterior distribution\"\n    ) |&gt; \n    fmt_number(\n        decimals = 2\n    )\n\n\n\n\n\n\n\nSummary of our posterior distribution\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n3.97\n3.97\n0.15\n0.15\n3.72\n4.22\n1.00\n3,459.39\n6,778.41\n\n\nbeta\n−0.18\n−0.18\n0.05\n0.05\n−0.26\n−0.11\n1.00\n3,492.57\n6,558.11",
    "crumbs": [
      "Solutions",
      "Homework #4 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw5/solution/index.html",
    "href": "assignments/hw5/solution/index.html",
    "title": "Homework #5 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n```{r}\n#| output: hide\n\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(cubature)\nlibrary(purrr)\nlibrary(scales)\nlibrary(glue)\nlibrary(rjags)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(broom)\ntheme_set(theme_classic())\n```\n\n\n1\nA clinical trial gave six subjects a placebo and six subjects a new weight loss medication. The response variable is the change in weight (pounds) from baseline (so -2.0 means the subject lost 2 pounds). The data for the 12 subjects are:\n\nd &lt;- tribble(\n    ~Placebo, ~Treatment,\n    2.0, -3.5,\n    -3.1, -1.6,\n    -1.0, -4.6,\n    0.2, -0.9,\n    0.3, -5.1,\n    0.4, 0.1\n)\n\nd |&gt; \n    gt()\n\n\n\n\n\n\n\nPlacebo\nTreatment\n\n\n\n\n2.0\n-3.5\n\n\n-3.1\n-1.6\n\n\n-1.0\n-4.6\n\n\n0.2\n-0.9\n\n\n0.3\n-5.1\n\n\n0.4\n0.1\n\n\n\n\n\n\n\nConduct a Bayesian analysis to compare the means of these two groups. Would you say the treatment is effective? Is your conclusion sensitive to the prior?\n\nSolution: We use the comparison of two normal means with equal unknown variance from page 123 in the book. The model uses a flat, Jeffreys’ prior.\n\nY1 &lt;- d$Placebo\nY2 &lt;- d$Treatment\n# Y2 is the n2-vector of data for group 2\n# Statistics from group 1\nYbar1 &lt;- mean(Y1)\ns21 &lt;- mean((Y1 - Ybar1)^2)\nn1 &lt;- length(Y1)\n# Statistics from group 2\nYbar2 &lt;- mean(Y2)\ns22 &lt;- mean((Y2 - Ybar2)^2)\nn2 &lt;- length(Y2)\ndelta_hat &lt;- Ybar2 - Ybar1\ns2 &lt;- (n1 * s21 + n2 * s22) / (n1 + n2)\nscale &lt;- sqrt(s2) * sqrt(1 / n1 + 1 / n2)\ndf &lt;- n1 + n2\ncred_int &lt;- delta_hat + scale * qt(c(0.025, 0.975), df = df)\ncred_int\n\n[1] -4.6058799 -0.1941201\n\n\nThe credible interval does not include 0, so it seems that the treatment is effective. Let’s compare to harsher priors.\nWe will fit the model\n\\[\nY_i\\vert\\mu, \\sigma^2 \\sim \\mathrm{Normal}(\\mu,\\sigma^2),\\quad i=1,\\dots,n,\n\\]\nand\n\\[\nY_i\\vert\\mu,\\delta, \\sigma^2 \\sim \\mathrm{Normal}(\\mu+\\delta,\\sigma^2),\\quad i=n+1,\\dots,n+m,\n\\]\nwhere\n\\[\n\\begin{gathered}\n\\mu,\\delta\\sim\\mathrm{Normal}(0, 100^2) \\\\\n\\sigma^2\\sim\\mathrm{InvGamma}(a, b).\n\\end{gathered}\n\\]\nwhere we let \\(a\\) and \\(b\\) vary from 0.01 to 10. Below, we create the function calculate_p_delta() that allows us to easily compare many different priors.\n\ncalculate_p_delta &lt;- function(a, b) {\n    data = list(\n        Y1 = d$Placebo,\n        Y2 = d$Treatment,\n        n = nrow(d),\n        m = nrow(d),\n        a = a,\n        b = b\n    )\n    model_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        Y1[i] ~ dnorm(mu, tau)\n    }\n    \n    for (i in 1:m) {\n        Y2[i] ~ dnorm(mu + delta, tau)\n    }\n    # Priors\n    mu ~ dnorm(0, 0.01^2)\n    delta ~ dnorm(0, 0.01^2)\n    tau ~ dgamma(a, b)\n    sigma2 &lt;- 1 / tau\n}\")\n    \n    \n    model &lt;- jags.model(\n        model_string,\n        data = data,\n        n.chains = 4,\n        quiet = TRUE\n    )\n    \n    update(model, 10000, progress.bar = \"none\")\n    \n    params &lt;- c(\"mu\", \"delta\", \"sigma2\")\n    samples &lt;- coda.samples(\n        model,\n        variable.names = params,\n        n.iter = 10000,\n        progress.bar = \"none\"\n    )\n    \n    posterior_jags &lt;- samples |&gt; \n        as_draws_df()\n    \n    p_lower &lt;- posterior_jags |&gt; \n        subset_draws(variable = \"delta\") |&gt; \n        summarise_draws(\"P(delta &lt; 0)\" = function(x) mean(x &lt; 0)) |&gt; \n        pull(`P(delta &lt; 0)`)\n    \n    sigma2 &lt;- posterior_jags |&gt; \n        subset_draws(variable = \"sigma2\") |&gt; \n        summarise_draws(mean) |&gt; \n        pull(mean)\n    tibble(\n        delta_p = p_lower,\n        sigma2 = sigma2\n    )\n}\n\nvalues &lt;- c(0.01, 0.1, 0.3, 1, 3, 5, 7, 10)\n\nresults &lt;- crossing(\n    a = values,\n    b = values\n) |&gt; \n    mutate(\n        results = map2(a, b, calculate_p_delta)\n    ) |&gt; \n    unnest(results)\n\nWe see that for higher values of \\(a\\) and \\(b\\), we become more and more sure that \\(\\delta &lt; 0\\)\n\nresults |&gt; \n    ggplot(aes(a, delta_p)) +\n    geom_line(aes(col = b, group = b)) +\n    scale_y_continuous(\n        labels = label_percent()\n    ) +\n    scale_x_continuous(\n        breaks = breaks_pretty()\n    ) +\n    labs(\n        title = expression(paste(P(delta &lt; 0), \" is very sensitive to the priors we put on our variance\")),\n        y = expression(P(delta &lt; 0))\n    )\n\n\n\n\nThis seems to be because for higher values of \\(a\\) and \\(b\\), we pull the posterior distribution of \\(\\sigma^2\\) towards lower values.\n\nresults |&gt; \n    ggplot(aes(sigma2, delta_p)) +\n    geom_line() +\n    scale_y_continuous(\n        labels = label_percent()\n    ) +\n    scale_x_continuous(\n        breaks = breaks_pretty()\n    ) +\n    labs(\n        title = expression(paste()),\n        y = expression(P(delta &lt; 0)),\n        x = expression(paste(\"Posterior mean of \", sigma^2))\n    )\n\n\n\n\n\n\n\n2\nLoad the classic Boston Housing Data in R:\n\ndata(Boston, package = \"MASS\")\nd &lt;- Boston |&gt; as_tibble()\n\nThe response variable is medv, the median value of owner-occupied homes (in $1,000s), and the other 13 variables are covariates that describe the neighborhood.\n\nFit a Bayesian linear regression model with uninformative Gaussian priors for the regression coefficients. Verify the MCMC sampler has converged, and summarize the posterior distribution of all regression coefficients.\n\n\nSolution:\n\ny &lt;- d$medv |&gt; log()\nX &lt;- d |&gt; select(-medv) |&gt; as.matrix() |&gt; scale()\n\ndata = list(\n    y = y,\n    X = X,\n    n = length(y),\n    p = ncol(X)\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        y[i] ~ dnorm(mu[i], tau)\n        mu[i] = alpha + inprod(X[i, ], beta)\n    }\n    for (i in 1:p) {\n        beta[i] ~ dnorm(0, 0.00001)\n    }\n    alpha ~ dnorm(0, 0.00001)\n    tau ~ dgamma(0.001, 0.001)\n    sigma2 &lt;- 1 / tau\n}\")\n\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    n.chains = 4,\n    quiet = TRUE\n)\n\nupdate(model, 10000, progress.bar = \"none\")\n\nparams &lt;- c(\"alpha\", \"beta\", \"sigma2\")\nsamples &lt;- coda.samples(\n    model,\n    variable.names = params,\n    n.iter = 10000,\n    progress.bar = \"none\"\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\nWe see from the table and trace plots below that our sampler has converged.\n\nposterior_jags |&gt; \n    summarise_draws() |&gt; \n    gt() |&gt; \n    fmt_number()\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n3.03\n3.03\n0.01\n0.01\n3.02\n3.05\n1.00\n39,701.44\n38,755.22\n\n\nbeta[1]\n−0.09\n−0.09\n0.01\n0.01\n−0.11\n−0.07\n1.00\n14,560.74\n23,748.08\n\n\nbeta[2]\n0.03\n0.03\n0.01\n0.01\n0.01\n0.05\n1.00\n9,752.13\n18,207.50\n\n\nbeta[3]\n0.02\n0.02\n0.02\n0.02\n−0.01\n0.05\n1.00\n5,318.65\n11,686.64\n\n\nbeta[4]\n0.03\n0.03\n0.01\n0.01\n0.01\n0.04\n1.00\n30,897.64\n37,311.90\n\n\nbeta[5]\n−0.09\n−0.09\n0.02\n0.02\n−0.12\n−0.06\n1.00\n6,543.03\n11,980.18\n\n\nbeta[6]\n0.06\n0.06\n0.01\n0.01\n0.04\n0.08\n1.00\n10,022.29\n20,177.68\n\n\nbeta[7]\n0.01\n0.01\n0.01\n0.01\n−0.02\n0.03\n1.00\n8,572.99\n16,048.64\n\n\nbeta[8]\n−0.10\n−0.10\n0.02\n0.02\n−0.13\n−0.08\n1.00\n6,817.62\n12,809.13\n\n\nbeta[9]\n0.12\n0.12\n0.02\n0.02\n0.09\n0.16\n1.00\n2,909.60\n6,358.69\n\n\nbeta[10]\n−0.11\n−0.11\n0.03\n0.03\n−0.15\n−0.06\n1.00\n2,742.38\n5,907.06\n\n\nbeta[11]\n−0.08\n−0.08\n0.01\n0.01\n−0.10\n−0.06\n1.00\n13,296.12\n23,371.60\n\n\nbeta[12]\n0.04\n0.04\n0.01\n0.01\n0.02\n0.05\n1.00\n24,180.26\n32,833.75\n\n\nbeta[13]\n−0.21\n−0.21\n0.01\n0.01\n−0.23\n−0.18\n1.00\n8,338.21\n16,662.67\n\n\nsigma2\n0.04\n0.04\n0.00\n0.00\n0.03\n0.04\n1.00\n35,405.06\n38,925.32\n\n\n\n\n\n\n\n\nposterior_jags |&gt; \n    mcmc_trace()\n\n\n\n\n\n\nPerform a classic least squares (e.g. using the lm function in R). Compare the results numerically and conceptually with the Bayesian results.\n\n\nSolution:\n\nm &lt;- lm(y ~ X)\n\nresults_ols &lt;- tidy(m, conf.int = T) |&gt; \n    filter(term != \"(Intercept)\") |&gt; \n    select(\n        term,\n        mean = estimate, \n        lower = conf.low,\n        upper = conf.high\n    ) |&gt; \n    mutate(\n        term = glue(\"beta[{row_number()}]\"),\n        type = \"OLS\"\n    )\n\n\nresults_jags &lt;- posterior_jags |&gt;\n    summarise_draws() |&gt; \n    filter(\n        str_detect(variable, \"beta\")\n    ) |&gt; \n    select(\n        term = variable,\n        mean, \n        lower = q5,\n        upper = q95\n    ) |&gt; \n    mutate(\n        type = \"JAGS\"\n    )\n\n\nresults_ols |&gt; \n    bind_rows(\n        results_jags\n    ) |&gt; \n    mutate(\n        term = fct_reorder(term, parse_number(term))\n    ) |&gt; \n    ggplot(aes(term, mean, ymin = lower, ymax = upper, col = type)) +\n    geom_pointrange(position = position_dodge(0.7)) +\n    coord_flip() +\n    labs(\n        title = \"Comparing JAGS and OLS results\"\n    )\n\n\n\n\nNumerically, we see that the results are nearly identical to the results from our JAGS sampler. This should not come as a surprise as we used nearly flat priors.\nConceptually, the Bayesian results let us infer by updating our subjective knowledge of the Boston housing data. A frequentist way of thinking would not let us say anything about these specific results, rather that if we were to repeat this analysis on similarly obtained data an infinite amount of times and based our decisions on those results, we could put bounds on how often our decisions would be wrong.\n\n\nRefit the Bayesian model with double exponential priors for the regression coefficients, and discuss how the results differ from the analysis with uninformative priors.\n\n\nSolution:\n\ny &lt;- d$medv |&gt; log()\nX &lt;- d |&gt; select(-medv) |&gt; as.matrix() |&gt; scale()\n\ndata = list(\n    y = y,\n    X = X,\n    n = length(y),\n    p = ncol(X)\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        y[i] ~ dnorm(mu[i], tau)\n        mu[i] = alpha + inprod(X[i, ], beta)\n    }\n    for (i in 1:p) {\n        beta[i] ~ ddexp(0, tau * tau_beta)\n    }\n    alpha ~ dnorm(0, 0.00001)\n    tau ~ dgamma(0.001, 0.001)\n    tau_beta ~ dgamma(0.001, 0.001)\n    sigma2 &lt;- 1 / tau\n}\")\n\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    n.chains = 4,\n    quiet = TRUE\n)\n\nupdate(model, 10000, progress.bar = \"none\")\n\nparams &lt;- c(\"alpha\", \"beta\", \"sigma2\", \"tau_beta\")\nsamples &lt;- coda.samples(\n    model,\n    variable.names = params,\n    n.iter = 10000,\n    progress.bar = \"none\"\n)\n\nposterior_laplace &lt;- samples |&gt; \n    as_draws_df()\n\n\nposterior_jags |&gt; \n    summarise_draws() |&gt; \n    gt() |&gt; \n    fmt_number()\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha\n3.03\n3.03\n0.01\n0.01\n3.02\n3.05\n1.00\n39,701.44\n38,755.22\n\n\nbeta[1]\n−0.09\n−0.09\n0.01\n0.01\n−0.11\n−0.07\n1.00\n14,560.74\n23,748.08\n\n\nbeta[2]\n0.03\n0.03\n0.01\n0.01\n0.01\n0.05\n1.00\n9,752.13\n18,207.50\n\n\nbeta[3]\n0.02\n0.02\n0.02\n0.02\n−0.01\n0.05\n1.00\n5,318.65\n11,686.64\n\n\nbeta[4]\n0.03\n0.03\n0.01\n0.01\n0.01\n0.04\n1.00\n30,897.64\n37,311.90\n\n\nbeta[5]\n−0.09\n−0.09\n0.02\n0.02\n−0.12\n−0.06\n1.00\n6,543.03\n11,980.18\n\n\nbeta[6]\n0.06\n0.06\n0.01\n0.01\n0.04\n0.08\n1.00\n10,022.29\n20,177.68\n\n\nbeta[7]\n0.01\n0.01\n0.01\n0.01\n−0.02\n0.03\n1.00\n8,572.99\n16,048.64\n\n\nbeta[8]\n−0.10\n−0.10\n0.02\n0.02\n−0.13\n−0.08\n1.00\n6,817.62\n12,809.13\n\n\nbeta[9]\n0.12\n0.12\n0.02\n0.02\n0.09\n0.16\n1.00\n2,909.60\n6,358.69\n\n\nbeta[10]\n−0.11\n−0.11\n0.03\n0.03\n−0.15\n−0.06\n1.00\n2,742.38\n5,907.06\n\n\nbeta[11]\n−0.08\n−0.08\n0.01\n0.01\n−0.10\n−0.06\n1.00\n13,296.12\n23,371.60\n\n\nbeta[12]\n0.04\n0.04\n0.01\n0.01\n0.02\n0.05\n1.00\n24,180.26\n32,833.75\n\n\nbeta[13]\n−0.21\n−0.21\n0.01\n0.01\n−0.23\n−0.18\n1.00\n8,338.21\n16,662.67\n\n\nsigma2\n0.04\n0.04\n0.00\n0.00\n0.03\n0.04\n1.00\n35,405.06\n38,925.32\n\n\n\n\n\n\n\n\nposterior_jags |&gt; \n    mcmc_dens()\n\n\n\n\nThe results are very similar to the ones in a. This is not really surprising as we are not really putting very strict priors on the coefficients, and we have a lot of data. The main difference between using double exponential priors vs normal priors comes when we use very tight priors. Double exponential priors are more prone to pull coefficients towards 0, whereas normal priors will never pull a coefficient completely towards zero.\n\n\nFit a Bayesian linear regression model in a. using only the first 500 observations and compute the posterior predictive distribution for the final 6 observations. Plot the posterior predictive distribution versus the actual value for these 6 observations and comment on whether the predictions are reasonable.\n\n\nSolution:\n\n\nX_train &lt;- X[1:500, ]\ny_train &lt;- y[1:500]\n\nn_train &lt;- length(y_train)\n\nX_test &lt;- X[501:506, ]\ny_test &lt;- y[501:506]\n\nn_test &lt;- length(y_test)\n\n\ndata &lt;- list(\n    y_train = y_train,\n    n_train = n_train,\n    n_test = n_test,\n    p = ncol(X_train),\n    X_train = X_train,\n    X_test = X_test\n)\n\nmodel_string &lt;- textConnection(\"model{\n   # Likelihood\n    for(i in 1:n_train){\n      y_train[i] ~ dnorm(mu_train[i],taue)\n      mu_train[i] &lt;- alpha+ inprod(X_train[i,], beta[])\n    }\n    \n    # Prediction\n    for(i in 1:n_test){\n      y_test[i] ~ dnorm(mu_test[i],taue)\n      mu_test[i] &lt;- alpha+ inprod(X_test[i,], beta[])\n    }\n    \n   # Priors\n    for(j in 1:p){\n      beta[j] ~ dnorm(0,0.001)\n    }\n    alpha ~ dnorm(0,0.001)\n    taue  ~ dgamma(0.1, 0.1)\n }\")\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    quiet = TRUE\n)\n\nupdate(model, 10000, progress.bar = \"none\")\n\nsamples &lt;- coda.samples(\n    model,\n    variable.names = c(\"alpha\", \"beta\", \"y_test\"),\n    n.iter = 50000, progress.bar = \"none\"\n)\n\nWe see on our plot below that our predictions are fairly good. Most of the points (5 ouf of 6) are inside our predictive interval\n\nsamples |&gt; \n    as_draws_df() |&gt; \n    subset_draws(variable = \"y_test\") |&gt; \n    mcmc_areas(\n        prob = 0.5,\n        prob_outer = 0.95\n    ) +\n    geom_segment(\n        data = tibble(\n            x = y_test,\n            xend = y_test,\n            y = 6:1,\n            yend = 6:1 + 1\n        ),\n        aes(\n            x = x,\n            xend = xend,\n            y = y,\n            yend = yend\n        )\n    ) +\n    scale_x_continuous(\n        limits = c(2.4, 3.7)\n    ) +\n    labs(\n        title = \"Compering our posterior predictive density to the test data\",\n        subtitle = \"The blue ribbon and white density are respectively 50% and 95% credible intervals.\"\n    )",
    "crumbs": [
      "Solutions",
      "Homework #5 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw2/solution/index.html",
    "href": "assignments/hw2/solution/index.html",
    "title": "Homework #2 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(glue)\ntheme_set(theme_classic())",
    "crumbs": [
      "Solutions",
      "Homework #2 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw2/solution/index.html#exercise-1",
    "href": "assignments/hw2/solution/index.html#exercise-1",
    "title": "Homework #2 (Solution)",
    "section": "Exercise 1",
    "text": "Exercise 1\nAssume \\(Y_1, \\dots, Y_n \\vert \\mu \\overset{\\mathrm{iid}}{\\sim}\\) where \\(\\sigma^2\\) is fixed and the unknown mean \\(\\mu\\) has prior \\(\\mu\\sim\\mathrm{Normal}(0, \\sigma^2/m)\\)\n\nGive a 95% posterior interval for \\(\\mu\\)\n\n\nSolution: From chapter 2.1.3 and derivations in Appendix 3 of the book, we immediately write our posterior as\n\\[\n\\begin{aligned}\n\\mu\\vert \\mathbf Y &\\sim \\mathrm{N}(w \\bar Y + (1 - w)\\mu_0, \\frac{\\sigma^2}{n + m}) \\\\\n&= \\mathrm{N}(w \\bar Y, \\frac{\\sigma^2}{n + m}), \\quad \\mu_0 = 0,\n\\end{aligned}\n\\]\nwhere \\(w = n/(n+m) \\in [0, 1]\\). A 95% posterior interval for \\(\\mu\\) is any interval, \\((l, u)\\) such that\n\\[\nP(l &lt; \\mu &lt; u) = 0.95\n\\]\nLet \\(z_\\alpha\\) denote the number such that if \\(Z \\sim \\mathrm{N}(0, 1)\\), then\n\\[\nP(Z &lt; z_\\alpha) = \\alpha.\n\\]\nWe can use R to find the values of \\(z_{0.025}\\) and \\(z_{0.975}\\)\n\n```{r}\nqnorm(0.025)\nqnorm(0.975)\n```\n\n[1] -1.959964\n[1] 1.959964\n\n\nSince \\(\\mu\\) is normally distributed with mean \\(w\\hat Y\\) and variance \\(\\sigma^2/(n+m)\\), we know that\n\\[\n\\frac{\\mu - w\\hat Y}{\\sigma/\\sqrt{n+m}} \\sim \\mathrm{N}(0, 1)\n\\] We can thus deduce our interval by writing\n\\[\n\\begin{aligned}\n&P(-1.96 &lt; \\frac{\\mu - w\\hat Y}{\\sigma/\\sqrt{n+m}} &lt; 1.96) = 0.95 \\\\\n\\rightarrow\\quad &P(-1.96\\frac{\\sigma}{\\sqrt{n+m}} &lt; \\mu - w\\hat Y &lt; 1.96\\frac{\\sigma}{\\sqrt{n+m}}) = 0.95 \\\\\n\\rightarrow\\quad &P(w\\hat Y -1.96\\frac{\\sigma}{\\sqrt{n+m}} &lt; \\mu &lt; w\\hat Y + 1.96\\frac{\\sigma}{\\sqrt{n+m}}) = 0.95.\n\\end{aligned}\n\\]\nThis gives us the 95% posterior interval\n\\[\n(w\\hat Y -1.96\\frac{\\sigma}{\\sqrt{n+m}}, w\\hat Y + 1.96\\frac{\\sigma}{\\sqrt{n+m}}).\n\\]\n\n\nSelect a value of \\(m\\) and argue that for this choice your 95% posterior credible interval has frequentist coverage 0.95 (that is, if you draw many samples of size \\(n\\) and compute the 95% interval following the formula in a. for each sample, in the long run 95% of the intervals will contain the true value of \\(\\mu\\)).\n\n\nSolution: If we let \\(m\\rightarrow 0\\), our prior variance increases to \\(\\infty\\) and the posterior mean becomes the sample mean, with 95% posterior credibility interval\n\\[\n(\\hat Y -1.96\\frac{\\sigma}{\\sqrt{n}}, \\hat Y + 1.96\\frac{\\sigma}{\\sqrt{n}}).\n\\]\nSince we assume a normal distribution, and the variance \\(\\sigma^2\\) is constant and assumed known, then the above interval is exactly the same as the confidence interval for \\(\\mu\\). Thus, according to our chosen model and using the frequentist way of thinking, the true value of mu will fall inside an interval computed according to this formula in 95% of trials (in the long run) in a repeated sampling setup.",
    "crumbs": [
      "Solutions",
      "Homework #2 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw2/solution/index.html#exercise-2",
    "href": "assignments/hw2/solution/index.html#exercise-2",
    "title": "Homework #2 (Solution)",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe Major League Baseball player Reggie Jackson is known as “Mr October” for his outstanding performances in the World Series (which takes place in October). Over his long career he played in 2820 regular-season games and hit 563 home runs in these games (a player can hit 0, 1, 2, … home runs in a game). He also played in 27 world series games and hit 10 home runs in these games.\nAssuming uninformative conjugate priors, summarize the posterior distribution of his home-run rate in the regular season and World Series. Is there sufficient evidence to claim that he performs better in the World Series?\n\nSolution: We are dealing with numbers of events over fixed periods of time, so the Poisson-gamma model is the logical choice.\nOur Poisson likelihood with rate $$ over a period of time \\(T\\) is\n\\[\nf(Y\\vert \\lambda) = \\frac{(T\\lambda)^Y}{Y!} \\exp(-T\\lambda),\n\\]\nand our Gamma prior is\n\\[\n\\pi(\\lambda \\vert a, b) = \\frac{b^a}{\\Gamma(a)}\\lambda^{a-1}\\exp\\left(-\\lambda b\\right).\n\\]\nChapter 2.1.2 in the book tells us that our posterior will take the form of a Gamma(A, B) distribution where \\(A = Y + a\\) and \\(B = T + b\\). We want to use a non-informative prior, so we choose \\(a=0.01\\) and \\(b=0.01\\).\n\nT1 &lt;- 2820\nY1 &lt;- 563\n\nT2 &lt;- 27\nY2 &lt;- 10\n\na &lt;- 0.01\nb &lt;- 0.01\n\n\nA1 &lt;- Y1 + a\nB1 &lt;- T1 + b\nA2 &lt;- Y2 + a\nB2 &lt;- T2 + b\n\nThe means for the two posterior distributions are\n\nresults &lt;- tibble(\n    Type = c(\"Regular-season\", \"World series\"),\n    A = c(A1, A2),\n    B = c(B1, B2),\n    Mean = A / B,\n    Lower = qgamma(0.025, shape = A, rate = B),\n    Upper = qgamma(0.975, shape = A, rate = B)\n) \n\n# Table\nresults |&gt; \n    gt() |&gt; \n    tab_spanner(columns = c(Lower, Upper), label = \"95% Posterior Interval\")\n\n\n\n\n\n\n\nType\nA\nB\nMean\n95% Posterior Interval\n\n\nLower\nUpper\n\n\n\n\nRegular-season\n563.01\n2820.01\n0.1996482\n0.1834953\n0.2164728\n\n\nWorld series\n10.01\n27.01\n0.3706035\n0.1777961\n0.6330224\n\n\n\n\n\n\n\nWe could also sample from the two distributions and use those samples to summarize the difference between the two distributions\n\nX1 &lt;- rgamma(n = 1e5, shape = A1, rate = B1)\nX2 &lt;- rgamma(n = 1e5, shape = A2, rate = B2)\n\np_greater &lt;- mean(X2 &gt; X1)\n\ntibble(\n    Regular = X1,\n    Worldseries = X2,\n    Difference = X2 - X1\n) |&gt; \n    ggplot(aes(x = Difference)) +\n    geom_histogram(\n        alpha = 0.5,\n        color = \"black\"\n    ) +\n    geom_vline(\n        xintercept = 0, \n        lty = 2\n    ) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = expression(theta[2]-theta[1]),\n        y = NULL,\n        title = \"How much better is Reggie Jackson's home run rate during the World Series?\",\n        subtitle = \"The evidence heavily favors the claim that Reggie Jackson does better in the World Series\"\n    )\n\n\n\n\nFinally, we calculate the probability that Reggie Jackson’s performance is better in the World Series than in the regular season (assuming our chosen model and priors).\n\nmean(X2 &gt; X1)\n\n[1] 0.95139",
    "crumbs": [
      "Solutions",
      "Homework #2 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw2/solution/index.html#exercise-4",
    "href": "assignments/hw2/solution/index.html#exercise-4",
    "title": "Homework #2 (Solution)",
    "section": "Exercise 4",
    "text": "Exercise 4\nAssume that \\(Y\\vert\\theta \\sim \\mathrm{NegBinomial}(\\theta, m)\\) and \\(\\theta\\sim\\mathrm{Beta}(a, b)\\).\n\nDerive the posterior of \\(\\theta\\)\n\n\nSolution: Our Negative Binomial likelihood can be written\n\\[\n\\begin{aligned}\nf(Y\\vert m, \\theta) &= \\binom{Y+m-1}{Y}\\theta^m(1-\\theta)^Y \\\\\n&\\propto \\theta^m(1-\\theta)^Y.\n\\end{aligned}\n\\]\nThe beta prior is written\n\\[\n\\begin{aligned}\n\\pi(\\theta \\vert a, b) \\propto \\theta^{a-1}(1-\\theta)^{b-1}.\n\\end{aligned}\n\\]\nWe see that when we multiply these two together, our posterior will become\n\\[\n\\begin{aligned}\np(\\theta \\vert Y) &\\propto f(Y\\vert\\theta)\\pi(\\theta) \\\\\n&\\propto \\theta^m(1-\\theta)^Y \\cdot \\theta^{a-1}(1 - \\theta)^{b-1} \\\\\n&= \\theta^{m + a - 1}(1 - \\theta)^{Y + b - 1},\n\\end{aligned}\n\\]\nwhich is the kernel of a Beta(m + a, Y + b) distribution.\n\n\nPlot the posterior of \\(\\theta\\) and give its 95% credible interval assuming \\(m = 5\\), \\(Y = 10\\), and \\(a=b=1\\).\n\n\nSolution:\n\n\nCode\nm &lt;- 5\nY &lt;- 10\na &lt;- 1\nb &lt;- 1\n\nA &lt;- m + a\nB &lt;- Y + b\n\nlower &lt;- qbeta(0.025, shape1 = A, shape2 = B)\nupper &lt;- qbeta(0.975, shape1 = A, shape2 = B)\n\nggplot() +\n    stat_function(\n        geom = \"area\",\n        fun = dbeta,\n        args = list(shape1 = A, shape2 = B),\n        alpha = 0.5,\n        xlim = c(lower, upper)\n    ) +\n    stat_function(\n        geom = \"area\",\n        fun = dbeta,\n        args = list(shape1 = A, shape2 = B),\n        alpha = 0.2,\n        xlim = c(0, lower)\n    ) +\n    stat_function(\n        geom = \"area\",\n        fun = dbeta,\n        args = list(shape1 = A, shape2 = B),\n        alpha = 0.2,\n        xlim = c(upper, 1)\n    ) +\n    geom_vline(\n        xintercept = lower,\n        lty = 2\n    ) +\n    geom_vline(\n        xintercept = upper,\n        lty = 2\n    ) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = expression(theta),\n        y = expression(paste(p, \"(\", theta, \"|\", Y, \")\")),\n        title = expression(paste(\"The posterior distribution of \", theta, \" after observing the data Y\")),\n        subtitle = glue(\"The posterior mean is {round(A/(A+B), 3)} and the 95% credible interval is [{round(lower, 3)}, {round(upper, 3)}]\")\n    ) +\n    theme(\n        plot.margin = margin(t = 5, r = 15, b = 5, l = 15)\n    )",
    "crumbs": [
      "Solutions",
      "Homework #2 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw2/solution/index.html#exercise-5",
    "href": "assignments/hw2/solution/index.html#exercise-5",
    "title": "Homework #2 (Solution)",
    "section": "Exercise 5",
    "text": "Exercise 5\nOver the past 50 years California has experienced an average of \\(\\lambda_0=75\\) large wildfires per year. For the next 10 years you will record the number of large fires in California and then fit a Poisson/gamma model to these data. Let the rate of large fires in this future period, \\(\\lambda\\), have prior \\(\\lambda\\sim\\mathrm{Gamma}(a,b)\\). Select \\(a\\) and \\(b\\) so that the prior is uninformative with prior variance around 100 and gives prior probability approximately \\(\\mathrm{Prob}(\\lambda &gt; \\lambda_0) = 0.5\\) so that the prior places equal probability on both hypotheses in the test for a change in the rate.\n\nSolution:\nThe variance of the Gamma(a, b) distribution is \\(a/b^2\\). To find the correct values for \\(a\\) and \\(b\\) we can create a sequence of values with variance 100 and find which pair is closes to giving us \\(P(\\lambda &gt; \\lambda_0) = 0.5\\).\nWe first use the equation for the variance to find how \\(a\\) and \\(b\\) must relate to each other.\n\\[\n\\begin{aligned}\n\\frac{a}{b^2} &= 100 \\\\\n\\rightarrow \\quad a &= 100b^2 \\\\\n\\rightarrow \\quad b &= \\sqrt{a}/10.\n\\end{aligned}\n\\]\nWe then find a pair that satisfies the second condition by performing the following:\n\nCreate a grid of values for \\(a\\)\nUse the fact that \\(b = \\sqrt a/10\\) to calculate the corresponding values for \\(b\\)\nUse the R function pgamma to calculate \\(P(\\lambda &gt; \\lambda_0)\\)\nFind the pair of values for which this probability is closest to 0.5\n\n\nmy_variance &lt;- 100\n\ntibble(\n    a = seq(0, 100, length.out = 1e4),\n    b = sqrt(a) / 10\n) |&gt; \n    mutate(\n        # Calculate the probability\n        prob_greater = pgamma(75, shape = a, rate = b),\n        # Calculate how close probability is to 0.5\n        prob_diff = abs(prob_greater - 0.5)\n    ) |&gt; \n    arrange(prob_diff) |&gt; \n    slice(1)\n\n# A tibble: 1 × 4\n      a     b prob_greater prob_diff\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1  56.9 0.754        0.500 0.0000446\n\n\nWe see that if we choose \\(a = 57\\) and \\(b = 0.75\\) we get an approximate prior variance of 100 and \\(P(\\lambda &gt; \\lambda_0) \\approx 0.5\\).",
    "crumbs": [
      "Solutions",
      "Homework #2 (Solution)"
    ]
  },
  {
    "objectID": "assignments/hw2/index.html",
    "href": "assignments/hw2/index.html",
    "title": "Homework #2",
    "section": "",
    "text": "Download the Quarto document used to render this file",
    "crumbs": [
      "Assignments",
      "Homework #2"
    ]
  },
  {
    "objectID": "assignments/hw2/index.html#exercise-1",
    "href": "assignments/hw2/index.html#exercise-1",
    "title": "Homework #2",
    "section": "Exercise 1",
    "text": "Exercise 1\nAssume \\(Y_1, \\dots, Y_n \\vert \\mu \\overset{\\mathrm{iid}}{\\sim}\\) where \\(\\sigma^2\\) is fixed and the unknown mean \\(\\mu\\) has prior \\(\\mu\\sim\\mathrm{Normal}(0, \\sigma^2/m)\\)\n\nGive a 95% posterior interval for \\(\\mu\\)\n\n\nSolution:\n\n\nSelect a value of \\(m\\) and argue that for this choice your 95% posterior credible interval ahs frequentist coverage 0.95 (that is, if you draw many samples of size \\(n\\) and compute the 95% interval following the formula in a. for each sample, in the long run 95% of the intervals will contain the true value of \\(\\mu\\)).\n\n\nSolution:",
    "crumbs": [
      "Assignments",
      "Homework #2"
    ]
  },
  {
    "objectID": "assignments/hw2/index.html#exercise-2",
    "href": "assignments/hw2/index.html#exercise-2",
    "title": "Homework #2",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe Major League Baseball player Reggie Jackson is known as “Mr October” for his outstanding performances in the World Series (which takes place in October). Over his long career he played in 2820 regular-season games and hit 563 home runs in these games (a player can hit 0, 1, 2, … home runs in a game). He also played in 27 world series games and hit 10 home runs in these games.\nAssuming uninformative conjugate priors, summarize the posterior distribution of his home-run rate in the regular season and World Series. Is there sufficient evidence to claim that he performs better in the World Series?\n\nSolution:",
    "crumbs": [
      "Assignments",
      "Homework #2"
    ]
  },
  {
    "objectID": "assignments/hw2/index.html#exercise-4",
    "href": "assignments/hw2/index.html#exercise-4",
    "title": "Homework #2",
    "section": "Exercise 4",
    "text": "Exercise 4\nAssume that \\(Y\\vert\\theta \\sim \\mathrm{NegBinomial}(\\theta, m)\\) and \\(\\theta\\sim\\mathrm{Beta}(a, b)\\).\n\nDerive the posterior of \\(\\theta\\)\n\n\nSolution:\n\n\nPlot the posterior of \\(\\theta\\) and give its 95% credible interval assuming \\(m = 5\\), \\(Y = 10\\), and \\(a=b=1\\).\n\n\nSolution:",
    "crumbs": [
      "Assignments",
      "Homework #2"
    ]
  },
  {
    "objectID": "assignments/hw2/index.html#exercise-5",
    "href": "assignments/hw2/index.html#exercise-5",
    "title": "Homework #2",
    "section": "Exercise 5",
    "text": "Exercise 5\nOver the past 50 years California ahs experienced an average of \\(\\lambda_0=75\\) large wildfires per year. For the next 10 years you will record the number of large fires in California and then fit a Poisson/gamma model to these data. Let the rate of large fires in this future period, \\(\\lambda\\), have prior \\(\\lambda\\sim\\mathrm{Gamma}(a,b)\\). Select \\(a\\) and \\(b\\) so that the prior is uninformative with prior variance around 100 and gives prior probability approximately \\(\\mathrm{Prob}(\\lambda &gt; \\lambda_0) = 0.5\\) so that the prior places equal probability on both hypotheses in the test for a change in the rate.\n\nSolution:",
    "crumbs": [
      "Assignments",
      "Homework #2"
    ]
  },
  {
    "objectID": "assignments/hw5/index.html",
    "href": "assignments/hw5/index.html",
    "title": "Homework #5",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n1\nA clinical trial gave six subjects a placebo and six subjects a new weight loss medication. The response variable is the change in weight (pounds) from baseline (so -2.0 means the subject lost 2 pounds). The data for the 12 subjects are:\n\n\n\n\n\n\n\n\nPlacebo\nTreatment\n\n\n\n\n2.0\n-3.5\n\n\n-3.1\n-1.6\n\n\n-1.0\n-4.6\n\n\n0.2\n-0.9\n\n\n0.3\n-5.1\n\n\n0.4\n0.1\n\n\n\n\n\n\n\nConduct a Bayesian analysis to compare the means of these two groups. Would you say the treatment is effective? Is your conclusion sensitive to the prior?\n\nSolution:\n\n\n\n2\nLoad the classic Boston Housing Data in R:\n\ndata(Boston, package = \"MASS\")\n\nThe response variable is medv, the median value of owner-occupied homes (in $1,000s), and the other 13 variables are covariates that describe the neighborhood.\n\nFit a Bayesian linear regression model with uninformative Gaussian priors for the regression coefficients. Verify the MCMC sampler has converged, and summarize the posterior distribution of all regression coefficients.\n\n\nSolution:\n\n\nPerform a classic least squares (e.g. using the lm function in R). Compare the results numerically and conceptually with the Bayesian results.\n\n\nSolution:\n\n\nRefit the Bayesian model with double exponential priors for the regression coefficients, and discuss how the results differ from the analysis with uninformative priors.\n\n\nSolution:\n\n\nFit a Bayesian linear regression model in a. using only the first 500 observations and compute the posterior predictive distribution for the final 6 observations. Plot the posterior predictive distribution versus the actual value for these 6 observations and comment on whether the predictions are reasonable.\n\n\nSolution:",
    "crumbs": [
      "Assignments",
      "Homework #5"
    ]
  },
  {
    "objectID": "assignments/hw4/index.html",
    "href": "assignments/hw4/index.html",
    "title": "Homework #4",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n2\nAssume that \\(Y_i\\vert\\mu\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(\\mu,\\sigma_i^2)\\) for \\(i \\in \\{1, \\dots, n\\}\\), with \\(\\sigma_i\\) known and improper prior distribution \\(\\pi(\\mu)=1\\) for all \\(\\mu\\).\n\nGive a formula for the MAP estimator for \\(\\mu\\)\n\n\nSolution:\n\n\nWe observe \\(n=3, Y_1=12, Y_2=10, Y_3=22, \\sigma_1=\\sigma_2=3\\) and \\(\\sigma_3=10\\), compute the MAP estimate of \\(\\mu\\).\n\n\nSolution:\n\n\nUse numerical integration to compute the posterior mean of \\(\\mu\\).\n\n\nSolution:\n\n\nPlot the posterior distribution of \\(\\mu\\) and indicate the MAP and the posterior mean estimates on the plot.\n\n\nSolution:\n\n\n\n4\nConsider the model\n\\[\nY_i\\vert\\sigma^2_i\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(\\mu,\\sigma_i^2),\n\\]\nfor \\(i \\in \\{1, \\dots, n\\}\\) where\n\\[\n\\sigma_i^2\\vert b \\sim \\mathrm{InvGamma}(a,b) \\\\ b\\sim\\mathrm{Gamma}(1,1)\n\\]\n\nDerive the full conditional posterior distributions for \\(\\sigma^2_1\\) and \\(b\\).\n\n\nSolution:\n\n\nWrite pseudocode for Gibbs sampling, i.e. describe in detail each step of the Gibbs sampling algorithm.\n\n\nSolution:\n\n\nWrite your own Gibbs sampling code (not in JAGS) and plot the marginal posterior density of each parameter. Assume \\(n=10\\), \\(a=10\\) and \\(Y_i=i\\) for \\(i=1, \\dots, 10\\).\n\n\nSolution:\n\n\nRepeat the analysis with \\(a=1\\) and comment on the convergence of the MCMC chain.\n\n\nSolution:\n\n\nImplement the model in c. using JAGS and compare the results with the results in c.\n\n\nSolution:\n\n\n\n5\nConsider the model\n\\[\nY_i\\vert\\mu, \\sigma^2 \\sim \\mathrm{Normal}(\\mu,\\sigma^2),\\quad i=1,\\dots,n,\n\\]\nand\n\\[\nY_i\\vert\\mu,\\delta, \\sigma^2 \\sim \\mathrm{Normal}(\\mu+\\delta,\\sigma^2),\\quad i=n+1,\\dots,n+m,\n\\]\nwhere\n\\[\n\\mu,\\delta\\sim\\mathrm{Normal}(0, 100^2) \\\\\n\\sigma^2\\sim\\mathrm{InvGamma}(0.01, 0.01).\n\\]\n\nGive an example of a real experiment for which this would be an appropriate model.\n\n\nSolution:\n\n\nDerive the full conditional posterior distributions for \\(\\mu\\), \\(\\delta\\), and \\(\\sigma^2\\).\n\n\nSolution:\n\n\nSimulate a dataset from this model with \\(n=m=50\\), \\(\\mu=10\\), \\(\\delta=1\\) and \\(\\sigma=2\\). Write your own Gibbs sampling code (not in JAGS) to fit the model above to the simulated data and plot the marginal posterior density for each parameter. Are you able to recover the true values reasonably well?\n\n\nSolution:\n\n\nImplement this model using JAGS and compare the results with the results in .c.\n\n\nSolution:\n\n\n\n10\nAs discussed in section 1.6, (Edie, Smits, and Jablonski 2017) report that the number of marine bivalve species discovered each year from 2010-2015 was 64, 13, 33, 18, 30 and 20. Denote \\(Y_t\\) as the number of species discovered in year \\(2009+t\\) (so that \\(Y_1=64\\) is the count for 2010). Use JAGS to fit the model\n\nEdie, Stewart M., Peter D. Smits, and David Jablonski. 2017. “Probabilistic Models of Species Discovery and Biodiversity Comparisons.” Proceedings of the National Academy of Sciences 114 (14): 3666–71. https://doi.org/10.1073/pnas.1616355114.\n\\[\n\\begin{gathered}\nY_t\\vert \\alpha, \\beta \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Poisson}(\\lambda_t)\\\\\n\\lambda_t = \\exp(\\alpha + \\beta t) \\text{ or equivalently } \\log(\\lambda_t) = \\alpha + \\beta t \\\\\n\\alpha,\\beta \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(0,10^2).\n\\end{gathered}\n\\]\nSummarize the posterior of \\(\\alpha\\) and \\(\\beta\\) and verify that the MCMC sampler has converged. Does this analysis provide evidence that the rate of discovery is changing over time?",
    "crumbs": [
      "Assignments",
      "Homework #4"
    ]
  },
  {
    "objectID": "assignments/hw3/index.html",
    "href": "assignments/hw3/index.html",
    "title": "Homework #3",
    "section": "",
    "text": "Download the Quarto document used to render this file",
    "crumbs": [
      "Assignments",
      "Homework #3"
    ]
  },
  {
    "objectID": "assignments/hw3/index.html#exercise-6",
    "href": "assignments/hw3/index.html#exercise-6",
    "title": "Homework #3",
    "section": "Exercise 6",
    "text": "Exercise 6\nAn assembly line relies on accurate measurements from an image-recognition algorithm at the first stage of the process. It is known that the algorithm is unbiased, so assume that measurements follow a normal distribution with mean zero,\n\\[\nY_i\\vert \\sigma^2\\overset{\\mathrm{iid}}{\\sim}\\mathrm{Normal}(0, \\sigma^2).\n\\]\nSome errors are permissible, but if \\(\\sigma\\) exceeds the threshold \\(c\\) then the algorithm must be replaced.\nYou make \\(n = 20\\) measurements and observe\n\\[\n\\sum_{i=1}^n Y_i = -2 \\quad \\mathrm{and} \\quad \\sum_{i=1}^n Y_i^2 = 15,\n\\]\nand conduct a Bayesian analysis with \\(\\mathrm{InvGamma}(a,b)\\) prior. compute the posterior probability that \\(\\sigma &gt; c\\) for:\n\n\\(c=1\\) and \\(a=b=0.1\\)\n\n\nSolution:\n\n\n\\(c=1\\) and \\(a=b=1.0\\)\n\n\nSolution:\n\n\n\\(c=2\\) and \\(a=b=0.1\\)\n\n\nSolution:\n\n\n\\(c=2\\) and \\(a=b=1.0\\)\n\n\nSolution:",
    "crumbs": [
      "Assignments",
      "Homework #3"
    ]
  },
  {
    "objectID": "assignments/hw3/index.html#exercise-16",
    "href": "assignments/hw3/index.html#exercise-16",
    "title": "Homework #3",
    "section": "Exercise 16",
    "text": "Exercise 16\nSay \\(Y\\vert\\lambda \\sim \\mathrm{Gamma}(1, \\lambda).\\)\n\nDerive and plot the Jeffreys’ prior for \\(\\lambda\\)\n\n\nSolution:\n\n\nIs this prior proper?\n\n\nSolution: This is not a proper prior since it does not integrate to one.\n\n\nDerive the posterior and give conditions on \\(Y\\) to ensure it is proper.\n\n\nSolution:",
    "crumbs": [
      "Assignments",
      "Homework #3"
    ]
  },
  {
    "objectID": "assignments/hw3/index.html#exercise-18",
    "href": "assignments/hw3/index.html#exercise-18",
    "title": "Homework #3",
    "section": "Exercise 18",
    "text": "Exercise 18\nThe data in the table below are the result of a survey of commuters in 10 counties likely to be affected by a proposed addition of a high occupancy vehicle (HOV) lane.\n\ntibble::tribble(\n    ~County, ~Approve, ~Disapprove,\n    1, 12, 50,\n    2, 90, 150,\n    3, 80, 63,\n    4, 5, 10,\n    5, 63, 63,\n    6, 15, 8,\n    7, 67, 56,\n    8, 22, 19,\n    9, 56, 63,\n    10, 33, 19\n)\n\n# A tibble: 10 × 3\n   County Approve Disapprove\n    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1      1      12         50\n 2      2      90        150\n 3      3      80         63\n 4      4       5         10\n 5      5      63         63\n 6      6      15          8\n 7      7      67         56\n 8      8      22         19\n 9      9      56         63\n10     10      33         19\n\n\n\nAnalyze the data in each county sepparately using the Jeffreys’ prior distribution and report the posterior 95% credible set for each county.\n\n\nSolution:\n\n\nLet \\(\\hat p_i\\) be the sample proportion of commuters in county \\(i\\) that approve of the HIV lane (e.g. \\(\\hat p_1 = 12/(12+50)=0.194\\)). Select \\(a\\) abd \\(b\\) so that the mean and variance of the \\(\\mathrm{Beta}(a,b)\\) distribution match the mean and variance of the sample proportions \\(\\hat p_1, \\dots, \\hat p_{10}\\).\n\n\nSolution:\n\n\nConduct an empirical Bayesian analysis by computing the 95% posterior credible sets that results from analyzing each coutny separately using the \\(\\mathrm{Beta}(a,b)\\) prior you computed in b.\n\n\nSolution:\n\n\nHow do the results from a. and c. differ? What are the advantages and disadvantages of these two analyses?\n\n\nSolution:",
    "crumbs": [
      "Assignments",
      "Homework #3"
    ]
  },
  {
    "objectID": "assignments/hw6/index.html",
    "href": "assignments/hw6/index.html",
    "title": "Homework #6",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n4\nDownload the US gun control data from the book’s website. For state \\(i\\), let \\(Y_i\\) be the number of homicides and \\(N_i\\) be the population.\n\nFit the model \\(Y_i \\vert \\beta \\sim \\mathrm{Poisson}(N_i\\lambda_i)\\) where \\(\\log(\\lambda_i) = \\mathbf X_i\\beta\\). Use uninformative priors and \\(p = 7\\) covariates in \\(\\mathbf X_i\\): the intercept, the five confounders \\(\\mathbf Z_i\\), and the total number of gun laws in state \\(i\\). Provide justification that the MCMC sampler has converged and sufficiently explored the posterior distribution and summarize the posterior of \\(\\beta\\)\n\n\nSolution:\n\n\nFit a Negative binomial regression model and compare with the results from Poisson regression.\n\n\nSolution:\n\n\nFor the Poisson model in (a), compute the posterior predictive distribution for each state with the number of gun laws set to zero. Repeat this with the number of gun laws set to 25 (the maximum number). According to these calculations, how would the number of deaths nationwide be affected by these policy changes? Do you trust these projections?\n\n\nSolution:\n\n\n\n7\nConsider the one-way random effects model\n\\[\nY_{ij} \\vert \\alpha_i, \\sigma^2 \\sim \\mathrm{Normal}(\\alpha_i, \\sigma^2)\n\\]\nand\n\\[\n\\alpha_i \\sim \\mathrm{Normal}(0, \\tau^2)\n\\]\nfor \\(i = 1, \\dots, n\\) and \\(j = 1, \\dots, m\\). Assuming conjugate priors \\(\\sigma^2, \\tau^2 \\sim \\mathrm{InvGamma}(a, b)\\) derive the full conditional distributions of \\(\\alpha_1\\), \\(\\sigma^2\\) and \\(\\tau^2\\) and outline (but do not code) an MCMC algorithm to sample from the posterior.\n\nSolution:\n\n\n\n8\nLoad the gambia data from the geoR package in R. The response variable \\(Y_i\\) is the binary indicator that child \\(i\\) tested positive for malaria (pos) and the remaining seven variables are covariates.\n\nFit the logistic regression model\n\n\\[\n\\mathrm{logit[Prob(Y_i=1)]} = \\sum_{j=1}^p X_{ij}\\beta_j\n\\]\nwith uninformative priors for the \\(\\beta_j\\). Verify that the MCMC sampler has converged and summarize the effects of the covariates.\n\nSolution:\n\n\nIn this dataset, the 2035 children reside in \\(L = 65\\) unique locations (defined by the x and y coordinates in the dataset). Let \\(s_i \\in \\{1, \\dots, L\\}\\) be the label of the location for observation \\(i\\). Fit the random effects logistic regression model\n\n\\[\n\\begin{gathered}\n\\mathrm{logit[Prob(Y_i=1)]} = \\sum_{j=1}^p X_{ij}\\beta_j + \\alpha_{s_i} \\\\\n\\alpha_l \\sim \\mathrm{Normal}(0, \\tau^2)\n\\end{gathered}\n\\]\nand the \\(\\beta_j\\) and \\(\\tau^2\\) have uninformative priors. Verify that the MCMC sampler has converged; explain why random effects might be needed here; discuss and explain any differences in the posteriors of the regression coefficients that occur when random effects are added to the model; plot the posterior means of the \\(\\alpha_l\\) by their spatial locations and suggest how this map might be useful to malaria researchers.",
    "crumbs": [
      "Assignments",
      "Homework #6"
    ]
  },
  {
    "objectID": "assignments/hw1/index.html",
    "href": "assignments/hw1/index.html",
    "title": "Homework #1",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n1. Sample survey\nSuppose we are going to sample 100 individuals from a county (with population size much larger than 100) and ask each sampled person whether they support policy Z or not. Let \\(Y_i = 1\\) if person \\(i\\) in the sample supports the policy, and \\(Y_i = 0\\) otherwise.\n\nAssume \\(Y_1, \\dots, Y_{100}\\) are, conditoinal on \\(\\theta\\), i.i.d. binary random variables with expectation \\(\\theta\\). Write down the joint distribution of \\(\\mathrm{Pr}(Y_1 = y_1, \\dots, Y_{100} = y_{100})\\) in a compact form. Also write down the form of \\(\\mathrm{Pr}(\\sum_{i_1}^{100}Y_i=y)\\).\n\nSolution:\n\n\nFor the moment, suppose you believed that \\(\\theta \\in \\{0, 0.1, \\dots, 0.9, 1.0\\}\\). Given that the results of the survey were \\(\\sum_{i=1}^{100}Y_i=73\\), compute \\(\\mathrm{Pr}(\\sum_{i=1}^{100}Y_i=73\\vert\\theta)\\) for each of these \\(11\\) values of \\(\\theta\\) and plot these probabilities as a function of \\(\\theta\\) (point mass at each value of \\(\\theta\\)).\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nNow suppose you originally had no prior information to believe one of these \\(\\theta\\)-values over another, and thus \\(\\mathrm{Pr}(\\theta=0.0) = \\mathrm{Pr}(\\theta=0.1) = \\dots = \\mathrm{Pr}(\\theta=0.9) = \\mathrm{Pr}(\\theta = 1.0) = \\frac{1}{11}\\). Use Bayes’ rule to compute \\(p(\\theta\\vert \\sum_{i=1}^{100}Y_i=73)\\) for each \\(\\theta\\)-value. Make a plot of this posterior distribution as a function of \\(\\theta\\) (point mass at each value of \\(\\theta\\)).\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nNow suppose you allow \\(\\theta\\) to be any value in the interval \\([0, 1]\\). Using the uniform prior density for \\(\\theta\\), namely, \\(p(\\theta) = 1\\), derive and plot the posterior density of \\(\\theta\\) as a function of \\(\\theta\\). According to the posterior density, what is the probability of \\(\\theta &gt; 0.8\\)?\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nWhy are the heights of posterior densities in c. and d. not the same?\n\nSolution:\n\n\n\n2. Random numbers, probability density functions (pdf) and cumulative density functions (cdf)\nThe goal of this exercise is to generate random numbers, plot the histogram, the empirical pdf and cdf for these numbers, and see how they compare to the theoretical pdf and cdf. The goal is also to compare the sample mean and standard deviation to the theoretical mean and standard deviation.\n\nGenerate \\(B = 3000\\) numbers fro the gamma distribution with parameters \\(\\alpha = 2\\) and \\(\\beta = 0.1\\). Compute the sample mean and the sample standard deviation and compare to the theoretical mean and standard deviation.\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nPlot the theoretical density (pdf) of the gamma distribution. Plot the empirical density based on the data on the same graph. Plot the histogram of the data on another graph.\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nPlot the theoretical cumulative density function (cdf) of the gamma distribution. Plot the empirical cumulative density based on the data on the same graph.\n\nSolution:\n\n```{r}\n# Include your code here\n```",
    "crumbs": [
      "Assignments",
      "Homework #1"
    ]
  },
  {
    "objectID": "assignments/hw7/index.html",
    "href": "assignments/hw7/index.html",
    "title": "Homework #7",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n```{r}\n#| output: hide\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(rjags)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(gt)\ntheme_set(theme_classic())\n```\n\n\n2\nDownload the airquality dataset in R. Fit the following model to the data\n\\[\n\\mathrm{ozone_i} \\sim \\mathrm{Normal}(\\beta_1 + \\beta_2\\mathrm{solar.R_i} + \\beta_3\\mathrm{temp_i} + \\beta_4\\mathrm{wind_i}, \\sigma^2)\n\\]\nUse posterior predictive checks to verify that the model fits well. If you find model misspecification, suggest (but do not fit) alternatives.\n\nSolution:\n\n\n\n4\nUse the “Mr. October” data. Compare the two models:\n\nusing Bayes factors, DIC and WAIC. Assume the Uniform(0, c) prior for all \\(\\lambda_j\\) and compare the results for \\(c = 1\\) and \\(c = 10\\).\n\nSolution:\n\n\n\n10\nDownload the WWWusage dataset in R. Using data from times \\(t = 5, \\dots, 100\\) as outcomes, fit the autoregressive model\n\\[\nY_t \\vert Y_{t-1}, \\dots, Y_1 \\sim \\mathrm{Normal}(\\beta_0 + \\beta_1 Y_{t-1} + \\dots + \\beta_L Y_{t-L}, \\sigma^2),\n\\]\nwhere \\(Y_t\\) is the WWW usage at time \\(t\\). Compare the models with \\(L = 1, 2, 3, 4\\) and select the best time lag L.\n\nSolution:"
  }
]