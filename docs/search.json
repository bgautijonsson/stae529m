[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Data Analysis (STÆ529M)",
    "section": "",
    "text": "Bayesian Data Analysis (STÆ529M)\nThis page contains Quarto templates for the assignments as well as solutions when they have been made available.\nOn each assignment/solution’s page there is a button that can be clicked in order to download the template for that assignment/solution. The student can then write their solution into the premade Quarto template and render it to HTML for turnin if they wish.\n\n\nQuarto\nAll of the templates are written usen Quarto Markdown Documents (.qmd). Quarto is basically the same as RMarkdown so you should not have a hard time switching over to using Quarto. The good thing about Quarto is that you can easily use it to create website. This website was created using Quarto, for example.\nI would recommend you look at the following materials to acquaint yourselves with the use of Quarto:\n\nThe Quarto Webpage\n\nQuarto for Academics\nBeautiful reports and presentations with Quarto\n\n\n\n\nGitHub\nThis whole page is built from a GitHub repository at github.com/bgautijonsson/stae529m. Instead of manually downloading each file you can clone the repository and pull the contents regularly to your computer to receive the newest versions of all documents.\nGit, GitHub and version control in general are essential skills so I would recommend you practice their use.\nI recommend the following material to get acquainted with Git and GitHub.\n\nGit and Github with R\nGit in general"
  },
  {
    "objectID": "assignments/hw1/solution/index.html",
    "href": "assignments/hw1/solution/index.html",
    "title": "Homework #1 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n1. Sample survey\nSuppose we are going to sample 100 individuals from a county (with population size much larger than 100) and ask each sampled person whether they support policy Z or not. Let \\(Y_i = 1\\) if person \\(i\\) in the sample supports the policy, and \\(Y_i = 0\\) otherwise.\n\nAssume \\(Y_1, \\dots, Y_{100}\\) are, conditoinal on \\(\\theta\\), i.i.d. binary random variables with expectation \\(\\theta\\). Write down the joint distribution of \\(\\mathrm{Pr}(Y_1 = y_1, \\dots, Y_{100} = y_{100})\\) in a compact form. Also write down the form of \\(\\mathrm{Pr}(\\sum_{i_1}^{100}Y_i=y)\\).\n\n\nSolution: First, we write down the distribution of a single observation, \\(Y_i\\):\n\\[\nP(Y_i = 1\\vert \\theta) = \\theta \\quad \\mathrm{and} \\quad P(Y_i = 0|\\theta) = (1 - \\theta).\n\\]\nWe can write these two cases together as follows\n\\[\nPr(Y_i = y_i) = \\theta^{y_i}(1 - \\theta)^{1 - y_i}.\n\\]\nWe recognize this as the Bernoulli distribution. The joint distribution is obtained by multiplying together all the observations\n\\[\nPr(Y_1 = y_1, \\dots, Y_{100} = y_{100}) = \\prod_{i=1}^{100}\\theta^{y_i}(1 - \\theta)^{1 - y_i} = \\theta^{\\sum_i y_i}(1 - \\theta)^{100 - \\sum_i y_i}.\n\\]\nThis is the distribution of any specific sequence of \\(y_i\\)’s. Let \\(X = \\sum_{i=1}^{100}y_i\\). There are \\(\\binom{100}{X} = \\frac{100!}{X!(100-X)!}\\) ways to choose a group of size \\(X\\) from a group of size \\(100\\). Then we can write the distribution of \\(X\\) as\n\\[\nPr(X = x) = \\binom{100}{x} \\theta^x(1 - \\theta)^{100 - x}\n\\]\n\n\nFor the moment, suppose you believed that \\(\\theta \\in \\{0, 0.1, \\dots, 0.9, 1.0\\}\\). Given that the results of the survey were \\(\\sum_{i=1}^{100}Y_i=73\\), compute \\(\\mathrm{Pr}(\\sum_{i=1}^{100}Y_i=73\\vert\\theta)\\) for each of these \\(11\\) values of \\(\\theta\\) and plot these probabilities as a function of \\(\\theta\\) (point mass at each value of \\(\\theta\\)).\n\n\nSolution: The equation above now becomes\n\\[\nPr(X = 73|\\theta) = \\binom{100}{73} \\theta^{73}(1 - \\theta)^{27}\n\\]\nWhen evaluating this expression, we do our calculations on the log scale to avoid numerical underflow. This gives us\n\\[\nPr(X = 73|\\theta) = \\binom{100}{73} e^{73 \\ln(\\theta) + 27 \\ln(1 - \\theta)}.\n\\]\nWe could easily calculate the binomial coefficient on the log scale, but I trust that our programming language of choice has an efficient and safe implementation.\n\n```{r}\n#| layout: [[2, 4]]\n# Include your code here\n\nmy_likelihood &lt;- function(theta) {\n    choose(100, 73) * exp(73 * log(theta) + 27 * log(1 - theta))\n}\n\nthetas &lt;- seq(0, 1, by = 0.1)\nresult &lt;- my_likelihood(thetas)\n\ndata.frame(theta = thetas, likelihood = result)\n\nplot(\n    thetas, \n    result, \n    xlab = expression(theta), \n    ylab = expression(Pr(X==73*\"|\"*theta))\n)\n```\n\n\n\n   theta   likelihood\n1    0.0 0.000000e+00\n2    0.1 1.114936e-50\n3    0.2 4.378461e-30\n4    0.3 8.515317e-19\n5    0.4 1.750513e-11\n6    0.5 1.512525e-06\n7    0.6 2.204769e-03\n8    0.7 7.196692e-02\n9    0.8 2.168109e-02\n10   0.9 8.758007e-07\n11   1.0 0.000000e+00\n\n\n\n\n\n\n\n\n\nNow suppose you originally had no prior information to believe one of these \\(\\theta\\)-values over another, and thus \\(\\mathrm{Pr}(\\theta=0.0) = \\mathrm{Pr}(\\theta=0.1) = \\dots = \\mathrm{Pr}(\\theta=0.9) = \\mathrm{Pr}(\\theta = 1.0) = \\frac{1}{11}\\). Use Bayes’ rule to compute \\(p(\\theta\\vert \\sum_{i=1}^{100}Y_i=73)\\) for each \\(\\theta\\)-value. Make a plot of this posterior distribution as a function of \\(\\theta\\) (point mass at each value of \\(\\theta\\)).\n\n\nSolution: First we write out all the equations that we know, before piecing them together:\n\\[\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{Pr(X=73|\\theta)\\pi(\\theta)}{m(X=73)} \\\\\nPr(X=73|\\theta) &= \\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\\\\n\\pi(\\theta) &= \\frac{1}{11} \\\\\nm(X=73) &= \\sum_\\theta\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}.\n\\end{aligned}\n\\]\nThis gives us\n\\[\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}\n}{\\sum_\\theta\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}} \\\\\n&= \\frac{\\theta^{73}(1-\\theta)^{27}}{\\sum_\\theta \\theta^{73}(1-\\theta)^{27}}\n\\end{aligned}\n\\]\nOnce again doing our calculations on the log scale, we get\n\\[\np(\\theta \\vert X = 73) = \\frac{e^{73\\ln(\\theta) + 27\\ln(1 - \\theta)}}{\\sum_\\theta e^{73\\ln(\\theta) + 27\\ln(1 - \\theta)}}\n\\]\n\n```{r}\n#| layout: [[2, 4]]\n# Include your code here\n\nmy_posterior &lt;- function(theta) {\n    theta_seq &lt;- seq(0, 1, by = 0.1)\n    C &lt;- sum(exp(73 * log(theta_seq) + 27 * log(1 - theta_seq)))\n    exp(73 * log(theta) + 27 * log(1 - theta)) / C\n}\n\nthetas &lt;- seq(0, 1, by = 0.1)\nposterior &lt;- my_posterior(thetas)\n\ndata.frame(theta = thetas, posterior = posterior)\n\nplot(\n    thetas, \n    posterior, \n    xlab = expression(theta), \n    ylab = expression(p(theta*\"|\"*Y))\n)\n```\n\n\n\n   theta    posterior\n1    0.0 0.000000e+00\n2    0.1 1.163146e-49\n3    0.2 4.567788e-29\n4    0.3 8.883524e-18\n5    0.4 1.826206e-10\n6    0.5 1.577927e-05\n7    0.6 2.300105e-02\n8    0.7 7.507881e-01\n9    0.8 2.261859e-01\n10   0.9 9.136709e-06\n11   1.0 0.000000e+00\n\n\n\n\n\n\n\n\n\nNow suppose you allow \\(\\theta\\) to be any value in the interval \\([0, 1]\\). Using the uniform prior density for \\(\\theta\\), namely, \\(p(\\theta) = 1\\), derive and plot the posterior density of \\(\\theta\\) as a function of \\(\\theta\\). According to the posterior density, what is the probability of \\(\\theta &gt; 0.8\\)?\n\n\nSolution: This time, our posterior is seen to be\n\\[\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot 1\n}{\\int_0^1\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot 1 d\\theta} \\\\\n&= \\frac{\\theta^{73}(1-\\theta)^{27}}{\\int_0^1 \\theta^{73}(1-\\theta)^{27}d\\theta} \\\\\n&= C \\cdot \\theta^{73}(1-\\theta)^{27}\n\\end{aligned}\n\\]\nWe can use the fact that the equation above should integrate to one to find out what \\(C\\) is.\n\\[\n\\int_0^1 C\\cdot \\theta^{73}(1 - \\theta)^{27} = 1 \\\\\n\\int_0^1 \\theta^{73}(1 - \\theta)^{27} = \\frac1C.\n\\]\nFrom working with the Beta distribution, we know that\n\\[\n\\int_0^1\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1 - x)^{\\beta-1} = 1.\n\\]\nUsing this, we see that\n\\[\nC = \\frac{\\Gamma(73 + 27 + 2)}{\\Gamma(73 + 1)\\Gamma(27 + 1)},\n\\]\ngiving us\n\\[\np(\\theta \\vert X = 73) = \\frac{\\Gamma(73 + 27 + 2)}{\\Gamma(73 + 1)\\Gamma(27 + 1)} \\theta^{73}(1 - \\theta)^{27}.\n\\]\nThis means that after observing the data, our uncertainty about \\(\\theta\\) is described by the Beta distribution with \\(\\alpha = 73+1\\) and \\(\\beta = 27+1\\).\n\\[\n\\theta\\vert Y \\sim \\mathrm{Beta}(73 + 1, 27 + 1)\n\\]\nWe’ll use R’s built in dbeta (d for density) function to evaluate this.\n\n```{r}\n#| layout: [[2, 4]]\n# Include your code here\nthetas &lt;- seq(0, 1, by = 0.1)\nposterior &lt;- dbeta(thetas, 73 + 1, 27 + 1)\n\ndata.frame(theta = thetas, posterior = posterior)\n\nplot(\n    thetas, \n    posterior, \n    xlab = expression(theta), \n    ylab = expression(p(theta*\"|\"*Y))\n)\n```\n\n\n\n   theta    posterior\n1    0.0 0.000000e+00\n2    0.1 1.126085e-48\n3    0.2 4.422245e-28\n4    0.3 8.600470e-17\n5    0.4 1.768018e-09\n6    0.5 1.527650e-04\n7    0.6 2.226817e-01\n8    0.7 7.268659e+00\n9    0.8 2.189790e+00\n10   0.9 8.845588e-05\n11   1.0 0.000000e+00\n\n\n\n\n\n\n\n\n1 - pbeta(0.8, 73 + 1, 27 + 1)\n\n[1] 0.03848785\n\n\n\n\nWhy are the heights of posterior densities in c. and d. not the same?\n\n\nSolution: The density in c. belongs to a discrete variable and the density in d. belongs to a continuous variable. This means the two functions have different meanings. Discrete random variables have densities (often called mass functions) that can be interpreted as probabilities, but that is not the case for continuous random variables. Thus the density of a continuous random variable can be greater than one as long as it integrates to one over it’s domain.\n\n\n\n2. Random numbers, probability density functions (pdf) and cumulative density functions (cdf)\nThe goal of this exercise is to generate random numbers, plot the histogram, the empirical pdf and cdf for these numbers, and see how they compare to the theoretical pdf and cdf. The goal is also to compare the sample mean and standard deviation to the theoretical mean and standard deviation.\n\nGenerate \\(B = 3000\\) numbers from the gamma distribution with parameters \\(\\alpha = 2\\) and \\(\\beta = 0.1\\). Compute the sample mean and the sample standard deviation and compare to the theoretical mean and standard deviation.\n\n\nSolution:\n\n```{r}\n# Include your code here\nB &lt;- 3000\nalpha &lt;- 2\nbeta &lt;- 0.1\nX &lt;- rgamma(n = B, shape = alpha, rate = beta)\n\ntheoretical_mean &lt;- alpha / beta\ntheoretical_sd &lt;- sqrt(alpha / beta^2)\n\nobs_mean &lt;- mean(X)\nobs_sd &lt;- sd(X)\n\ndata.frame(\n    theoretical_mean,\n    obs_mean,\n    theoretical_sd,\n    obs_sd\n)\n```\n\n  theoretical_mean obs_mean theoretical_sd   obs_sd\n1               20 19.68904       14.14214 14.06394\n\n\n\n\nPlot the theoretical density (pdf) of the gamma distribution. Plot the empirical density based on the data on the same graph. Plot the histogram of the data on another graph.\n\n\nSolution:\nUsing base R:\n\n```{r}\n# Include your code here\n\nx_dens &lt;- density(X, from = 0)\n\nplot(\n    x_dens, \n    main = \"Comparing the empirical and theoretical densities\", \n    col = \"blue\"\n)\ncurve(\n    dgamma(x, shape = alpha, rate = beta), \n    from = 0, to = 120, \n    add = TRUE, \n    col = \"red\"\n)\nlegend(\n    \"top\", \n    c(\"Empirical\", \"Theoretical\"), \n    col = c(\"blue\", \"red\"), \n    lty = 1\n)\n```\n\n\n\n\nUsing the tidyverse:\n\n```{r}\n#| output: false\nlibrary(dplyr)\nlibrary(ggplot2)\ntheme_set(theme_classic())\n```\n\n\n```{r}\ntibble(\n    X = X\n) |&gt; \n    ggplot(aes(X)) +\n    geom_density(\n        bounds = c(0, Inf),\n        aes(color = \"Empirical\", lty = \"Empirical\")\n        ) +\n    stat_function(\n        fun = function(x) dgamma(x, shape =alpha, rate = beta),\n        geom = \"line\",\n        aes(color = \"Theoretical\", lty = \"Theoretical\")\n    ) +\n    labs(\n        color = \"Type\",\n        linetype = \"Type\",\n        x = \"X\",\n        y = \"Density\",\n        title = \"Comparing the empirical and theoretical densities\"\n    )\n```\n\n\n\n\n\n\nPlot the theoretical cumulative density function (cdf) of the gamma distribution. Plot the empirical cumulative density based on the data on the same graph.\n\n\nSolution:\nUsing base R:\n\n```{r}\n# Include your code here\nX_sorted &lt;- sort(X)\nquantile &lt;- seq_along(X_sorted) / (length(X_sorted) + 1)\n\nplot(\n    X_sorted, quantile, \n    type = \"l\", \n    col = \"blue\", \n    main = \"Comparing the empirical and theoretical CDF\",\n    xlab = \"x\",\n    ylab = expression(P(X&lt;x))\n)\ncurve(\n    pgamma(x, shape = alpha, rate = beta), \n    from = 0, to = 120, \n    add = TRUE, \n    col = \"red\"\n)\nlegend(\n    \"right\", \n    c(\"Empirical\", \"Theoretical\"),\n    col = c(\"blue\", \"red\"), \n    lty = 1\n)\n```\n\n\n\n\nUsing the tidyverse:\n\n```{r}\nlibrary(dplyr)\nlibrary(ggplot2)\ntibble(\n    X = X\n) |&gt; \n    arrange(X) |&gt; \n    mutate(\n        empirical = row_number() / (n() + 1)\n    ) |&gt; \n    ggplot(aes(X)) +\n    geom_line(\n        aes(y = empirical, color = \"Empirical\", lty = \"Empirical\")\n        ) +\n    stat_function(\n        fun = function(x) pgamma(x, shape =alpha, rate = beta),\n        geom = \"line\",\n        aes(color = \"Theoretical\", lty = \"Theoretical\")\n    ) +\n    labs(\n        color = \"Type\",\n        linetype = \"Type\",\n        x = \"X\",\n        y = \"Density\",\n        title = \"Comparing the empirical and theoretical CDF\"\n    )\n```"
  },
  {
    "objectID": "assignments/hw3/solution/index.html",
    "href": "assignments/hw3/solution/index.html",
    "title": "Homework #3 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file\n```{r}\n#| output: hide\n\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(invgamma)\ntheme_set(theme_classic())\n```"
  },
  {
    "objectID": "assignments/hw3/solution/index.html#exercise-6",
    "href": "assignments/hw3/solution/index.html#exercise-6",
    "title": "Homework #3 (Solution)",
    "section": "Exercise 6",
    "text": "Exercise 6\nAn assembly line relies on accurate measurements from an image-recognition algorithm at the first stage of the process. It is known that the algorithm is unbiased, so assume that measurements follow a normal distribution with mean zero,\n\\[\nY_i\\vert \\sigma^2\\overset{\\mathrm{iid}}{\\sim}\\mathrm{Normal}(0, \\sigma^2).\n\\]\nSome errors are permissible, but if \\(\\sigma\\) exceeds the threshold \\(c\\) then the algorithm must be replaced.\nYou make \\(n = 20\\) measurements and observe\n\\[\n\\sum_{i=1}^n Y_i = -2 \\quad \\mathrm{and} \\quad \\sum_{i=1}^n Y_i^2 = 15,\n\\]\nand conduct a Bayesian analysis with \\(\\mathrm{InvGamma}(a,b)\\) prior. compute the posterior probability that \\(\\sigma &gt; c\\) for:\n\n\\(c=1\\) and \\(a=b=0.1\\)\n\n\nSolution:\nOur likelihood is\n\\[\n\\begin{aligned}\nf(\\mathbf Y\\vert \\sigma^2) &\\propto \\prod_{i=1}^n \\frac1\\sigma \\exp\\left[-\\frac{(Y_i - \\mu)^2}{2\\sigma^2}\\right] \\\\\n&\\propto (\\sigma^2)^{-n/2}\\exp\\left(-\\frac{\\mathrm{SSE}}{2\\sigma^2}\n\\right).\n\\end{aligned}\n\\]\nIn our case, we can write out SSE as\n\\[\n\\begin{aligned}\n\\mathrm{SSE} &= \\sum_{i=1}^n (Y_i - \\mu)^2 \\\\\n&= \\sum_{i=1}^n Y_i^2 - 2 Y_i\\mu + \\mu^2 \\\\\n&= \\sum_{i=1}^n Y_i^2, \\quad\\quad \\mu = 0.\n\\end{aligned}\n\\]\nOur inverse gamma prior is\n\\[\n\\pi(\\sigma^2) \\propto (\\sigma^2)^{-(a+1)}\\exp\\left(- \\frac{b}{\\sigma^2}\\right).\n\\]\nCombining our likelihood and prior we get the posterior\n\\[\n\\begin{aligned}\np(\\sigma^2\\vert\\mathbf Y) &\\propto f(\\mathbf Y\\vert \\sigma^2)\\pi(\\sigma^2) \\\\\n&\\propto (\\sigma^2)^{-(A+1)}\\exp\\left(- \\frac{B}{\\sigma^2}\\right),\n\\end{aligned}\n\\]\nwhere \\(A = n/2 + a\\) and \\(B = \\mathrm{SSE}/2 + b\\), and we therefore see that\n\\[\n\\sigma^2 \\vert \\mathbf Y \\sim \\mathrm{InvGamma}(A, B).\n\\]\nBy using the right values for \\(n\\), SSE, \\(a\\), \\(b\\), and \\(c\\) we can easily use R’s inbuilt pgamma function to answer the question. To use the pgamma function, we just need to remember that if\n\\[\nX \\sim \\mathrm{Gamma}(\\alpha, \\beta),\n\\]\nthen\n\\[\n\\frac1X \\sim \\mathrm{InvGamma}(\\alpha, \\beta).\n\\]\nThus, since \\(\\sigma^2\\) is assumed to follow an inverse gamma distribution we can answer the question by calculating\n\\[\nP\\left(\\frac1{\\sigma^2} &lt; \\frac 1{c^2}\\right).\n\\]\n\n```{r}\nn &lt;- 20\nSSE &lt;- 15\nc &lt;- 1\na &lt;- 0.1\nb &lt;- 0.1\n\nA &lt;- n/2 + a\nB &lt;- SSE/2 + b\n\npgamma(1/c^2, shape = A, rate = B)\n```\n\n[1] 0.2249838\n\n\nWe could also use the pinvgamma function from the invgamma package\n\n1 - pinvgamma(c^2, shape = A, rate = B)\n\n[1] 0.2249838\n\n\n\n\nCode\nplot_solution &lt;- function(A, B, c) {\n    p1 &lt;- ggplot() +\n        stat_function(\n            geom = \"area\",\n            fun = dinvgamma,\n            xlim = c(0, c^2),\n            args = list(shape = A, rate = B),\n            alpha = 0.4\n        ) +\n        stat_function(\n            geom = \"area\",\n            fun = dinvgamma,\n            xlim = c(c^2, qinvgamma(0.99999, shape = A, rate = B)),\n            args = list(shape = A, rate = B)\n        ) +\n        annotate(\n            geom = \"text\",\n            x = 1.1 * c^2 + 0.1,\n            y = dgamma(1/c^2, shape = A, rate = B) + 0.05,\n            label = expression(sigma^2&gt;c^2)\n        ) +\n        coord_cartesian(expand = FALSE) +\n        labs(\n            subtitle = \"Inverse gamma\",\n            x = expression(sigma^2)\n        )\n    \n    p2 &lt;- ggplot() +\n        stat_function(\n            geom = \"area\",\n            fun = dgamma,\n            xlim = c(0, 1/c^2),\n            args = list(shape = A, rate = B)\n        ) +\n        stat_function(\n            geom = \"area\",\n            fun = dgamma,\n            xlim = c(1/c^2, qgamma(0.99999, shape = A , rate = B)),\n            args = list(shape = A, rate = B),\n            alpha = 0.4\n        ) +\n        annotate(\n            geom = \"text\",\n            x = 1/ (c^2 * 1.3) - 0.1,\n            y = dgamma(1/c^2, shape = A, rate = B) + 0.05,\n            label = expression(frac(1,sigma^2)&lt;frac(1,c^2))\n        ) +\n        coord_cartesian(expand = FALSE, clip = \"off\") +\n        labs(\n            subtitle = \"Gamma\",\n            x = expression(frac(1,sigma^2))\n        )\n    \n    p1 + p2 +\n        plot_annotation(\n            title = \"Comparing solutions using the Gamma or Inverse Gamma\"\n        ) &\n        theme(\n            axis.text.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.title.y = element_blank(),\n            axis.line.y = element_blank()\n        )\n    \n}\n\nplot_solution(A, B, c)\n\n\n\n\n\n\n\n\\(c=1\\) and \\(a=b=1.0\\)\n\n\nSolution:\n\n```{r}\nn &lt;- 20\nSSE &lt;- 15\nc &lt;- 1\na &lt;- 1\nb &lt;- 1\n\nA &lt;- n/2 + a\nB &lt;- SSE/2 + b\n\npgamma(1/c^2, shape = A, rate = B)\n```\n\n[1] 0.236638\n\n\n\n1 - pinvgamma(c^2, shape = A, rate = B)\n\n[1] 0.236638\n\n\n\n\nCode\nplot_solution(A, B, c)\n\n\n\n\n\n\n\n\\(c=2\\) and \\(a=b=0.1\\)\n\n\nSolution:\n\n```{r}\nn &lt;- 20\nSSE &lt;- 15\nc &lt;- 2\na &lt;- 0.1\nb &lt;- 0.1\n\nA &lt;- n/2 + a\nB &lt;- SSE/2 + b\n\npgamma(1/c^2, shape = A, rate = B)\n```\n\n[1] 2.560058e-05\n\n\n\n1 - pinvgamma(c^2, shape = A, rate = B)\n\n[1] 2.560058e-05\n\n\n\n\nCode\nplot_solution(A, B, c)\n\n\n\n\n\n\n\n\\(c=2\\) and \\(a=b=1.0\\)\n\n\nSolution:\n\n```{r}\nn &lt;- 20\nSSE &lt;- 15\nc &lt;- 2\na &lt;- 1\nb &lt;- 1\n\nA &lt;- n/2 + a\nB &lt;- SSE/2 + b\n\npgamma(1/c^2, shape = A, rate = B)\n```\n\n[1] 1.44581e-05\n\n\n\n1 - pinvgamma(c^2, shape = A, rate = B)\n\n[1] 1.44581e-05\n\n\n\n\nCode\nplot_solution(A, B, c)\n\n\n\n\n\n\nFor each \\(c\\), compute the ratio of probabilities for the two priors. Which, if any of the results are sensitive to the prior?\n\nSolution: In the table below, we compare the two choices of priors for each value of \\(c\\). We see that the ration between the two probabilities is close to \\(1\\) for \\(c=1\\), so it is relatively insensitive to the priors. On the other hand the ratio is \\(0.56\\) for \\(c=2\\) which tells us that our inference is sensitive to the choice of prior distribution when \\(c=2\\).\n\n\nCode\ncrossing(\n    c = c(1, 2),\n    ab = c(0.1, 1)\n) |&gt; \n    mutate(\n        A = n/2 + ab,\n        B = SSE/2 + ab,\n        prob = 1 - pinvgamma(c^2, shape = A, rate = B)\n    ) |&gt; \n    select(c, ab, prob) |&gt; \n    pivot_wider(names_from = ab, values_from = prob) |&gt; \n    mutate(\n        ratio = `1` / `0.1`\n    ) |&gt; \n    gt() |&gt; \n    tab_spanner(\n        label = \"Probability for each prior\",\n        columns = c(`0.1`, `1`)\n    ) |&gt; \n    cols_label(\n        `0.1` = \"a,b=0.1\",\n        `1` = \"a,b=1\"\n    ) |&gt; \n    fmt_number(\n        decimals = 5, rows = 2\n    ) |&gt; \n    fmt_number(\n        decimals = 3, rows = 1\n    ) |&gt; \n    fmt_number(decimals = 0, columns = 1)\n\n\n\n\n\n\n  \n    \n    \n      c\n      \n        Probability for each prior\n      \n      ratio\n    \n    \n      a,b=0.1\n      a,b=1\n    \n  \n  \n    1\n0.225\n0.237\n1.052\n    2\n0.00003\n0.00001\n0.56476"
  },
  {
    "objectID": "assignments/hw3/solution/index.html#exercise-16",
    "href": "assignments/hw3/solution/index.html#exercise-16",
    "title": "Homework #3 (Solution)",
    "section": "Exercise 16",
    "text": "Exercise 16\nSay \\(Y\\vert\\lambda \\sim \\mathrm{Gamma}(1, \\lambda).\\)\n\nDerive and plot the Jeffreys’ prior for \\(\\lambda\\)\n\n\nSolution: Jeffreys’ prior is\n\\[\n\\pi(\\theta) \\propto \\sqrt{I(\\theta)},\n\\]\nwhere \\(I(\\theta)\\) is the expected Fisher information, defined as\n\\[\nI(\\theta) = -E\\left( \\frac{d^2\\log f(Y\\vert \\theta)}{d\\theta^2}\\right).\n\\]\nThe PDF of the Gamma distribution can be written\n\\[\nf(x \\vert a, b) = \\frac{b^a}{\\Gamma(a)}x^{a-1}\\exp\\left(-xb\\right).\n\\]\nIn our case, \\(a=1\\) and \\(b=\\lambda\\)\n\\[\n\\begin{aligned}\nf(Y\\vert \\lambda) &= \\frac{\\lambda^1}{\\Gamma(1)}Y^0\\exp(-\\lambda Y) \\\\\n&= \\lambda \\exp(-\\lambda Y).\n\\end{aligned}\n\\]\nTaking the log we get\n\\[\n\\begin{aligned}\nl(Y\\vert\\theta) &=\\log f(Y\\vert\\lambda) \\\\\n&= \\log\\lambda - \\lambda Y \\\\\n\\frac{dl}{d\\lambda} &= \\frac1\\lambda - Y \\\\\n\\frac{dl^2}{d\\lambda^2} &= -\\frac{1}{\\lambda^2} \\\\\n&= -I(\\lambda).\n\\end{aligned}\n\\]\nWe thus see that\n\\[\n\\begin{aligned}\n\\pi(\\theta) &= \\sqrt{I(\\theta)} \\\\\n&= \\sqrt{\\frac{1}{\\lambda^2}} \\\\\n&= \\frac1\\lambda\n\\end{aligned}\n\\]\n\nggplot() +\n    stat_function(\n        geom = \"area\",\n        fun = function(x) 1 / x,\n        xlim = c(0, 20),\n        col = \"black\",\n        alpha = 0.5\n    ) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = expression(lambda),\n        y = expression(paste(pi, \"(\", lambda, \")\"))\n    )\n\n\n\n\n\n\nIs this prior proper?\n\n\nSolution: This is not a proper prior since it does not integrate to one.\n\n\nDerive the posterior and give conditions on \\(Y\\) to ensure it is proper.\n\n\nSolution:\nOur posterior is\n\\[\n\\begin{aligned}\np(\\lambda\\vert Y) &= \\frac{f(\\mathbf y\\vert \\lambda)\\pi(\\lambda)}{\\int_0^\\infty f(\\mathbf y\\vert \\lambda)\\pi(\\lambda)d\\lambda} \\\\\n&= \\frac{\\lambda \\exp(-\\lambda Y) \\cdot \\frac1\\lambda}{\\int_0^\\infty \\lambda \\exp(-\\lambda Y) \\cdot \\frac1\\lambda d\\lambda} \\\\\n&= \\frac{\\exp(-\\lambda Y)}{\\int_0^\\infty \\exp(-\\lambda Y) d\\lambda}\n\\end{aligned}\n\\]\nThe integral of \\(\\exp(-\\lambda Y)\\) from \\(0\\) to \\(\\infty\\) is\n\\[\n\\int_0^\\infty \\exp(-\\lambda Y)d\\lambda =  -\\frac1Y \\left[\\exp(-\\lambda Y) \\right]_0^\\infty.\n\\]\nIf \\(Y&gt;0\\), then this becomes \\(\\frac1Y\\), giving us the posterior distribution\n\\[\np(\\lambda\\vert Y) = Y \\exp(-Y\\lambda),\n\\]\nwhich is proper, since we assumed that \\(Y&gt;0\\).\nIf \\(Y = 0\\), then our posterior distribution will not integrate to one and thus will not be proper. We therefore require that \\(Y &gt; 0\\). Luckily \\(Y=0\\) with probability 0 as Y is a continuous variable."
  },
  {
    "objectID": "assignments/hw3/solution/index.html#exercise-18",
    "href": "assignments/hw3/solution/index.html#exercise-18",
    "title": "Homework #3 (Solution)",
    "section": "Exercise 18",
    "text": "Exercise 18\nThe data in the table below are the result of a survey of commuters in 10 counties likely to be affected by a proposed addition of a high occupancy vehicle (HOV) lane.\n\nd &lt;- tribble(\n    ~County, ~Approve, ~Disapprove,\n    1, 12, 50,\n    2, 90, 150,\n    3, 80, 63,\n    4, 5, 10,\n    5, 63, 63,\n    6, 15, 8,\n    7, 67, 56,\n    8, 22, 19,\n    9, 56, 63,\n    10, 33, 19\n)\n\nd |&gt; \n    gt()\n\n\n\n\n\n  \n    \n    \n      County\n      Approve\n      Disapprove\n    \n  \n  \n    1\n12\n50\n    2\n90\n150\n    3\n80\n63\n    4\n5\n10\n    5\n63\n63\n    6\n15\n8\n    7\n67\n56\n    8\n22\n19\n    9\n56\n63\n    10\n33\n19\n  \n  \n  \n\n\n\n\n\nAnalyze the data in each county separately using the Jeffreys’ prior distribution and report the posterior 95% credible set for each county.\n\n\nSolution: We assume the data follow a binomial distribution\n\\[\nf(Y\\vert p) = \\binom{N}{Y}p^Y(1-p)^{N-Y}.\n\\]\nTo derive Jeffreys’ prior we first find the expected Fisher information\n\\[\n\\begin{aligned}\nl(Y\\vert p) &= \\log f(Y\\vert p) \\\\\n&= \\log\\binom{N}{Y} + Y\\log p + (N - Y)\\log(1 - p) \\\\\n\\frac{dl}{dp} &= \\frac Yp + \\frac{N-Y}{1-p} \\\\\n\\frac{dl^2}{dp^2} &= -\\frac{Y}{p^2} - \\frac{N - Y}{(1 - p)^2}.\n\\end{aligned}\n\\]\nThe expected value of \\(Y\\) is \\(Np\\) and the expected value of \\(N - Y\\) is \\(N - Np\\).\n\\[\n\\begin{aligned}\nI(p) &= \\frac{Np}{p^2} + \\frac{N - Np}{(1 - p)^2} \\\\\n&= \\frac{N}{p} + \\frac{N}{1 - p} \\\\\n&= \\frac{N}{p(1 - p)}.\n\\end{aligned}\n\\]\nJeffreys’ prior is therefore\n\\[\n\\pi(p) \\propto \\sqrt{\\frac{n}{p(1 - p)}} \\propto p^{1/2-1}(1-p)^{1/2-1},\n\\]\nwhich is the kernel of a \\(\\mathrm{Beta}(0.5, 0.5)\\) PDF. We can thus see that Jeffreys’ prior for a binomial proportion is a \\(\\mathrm{Beta}(0.5, 0.5)\\) distribution, and the posterior will be a \\(\\mathrm{Beta}(0.5 + Y, 0.5 + (N - Y))\\) distribution.\n\n```{r}\njeffreys_results &lt;- d |&gt; \n    mutate(\n        A = Approve + 0.5,\n        B = Disapprove + 0.5,\n        Lower = qbeta(0.025, A, B),\n        Upper = qbeta(0.975, A, B)\n    ) \n\njeffreys_results |&gt; \n    gt() |&gt; \n    cols_hide(\n        columns = c(Approve, Disapprove, A, B)\n    ) |&gt; \n    tab_spanner(\n        label = \"95% Credible Interval\", \n        columns = c(Lower, Upper)\n    ) |&gt; \n    fmt_percent(\n        columns = c(Lower, Upper)\n    )\n```\n\n\n\n\n\n  \n    \n    \n      County\n      \n        95% Credible Interval\n      \n    \n    \n      Lower\n      Upper\n    \n  \n  \n    1\n11.04%\n30.45%\n    2\n31.55%\n43.74%\n    3\n47.76%\n63.89%\n    4\n14.03%\n58.42%\n    5\n41.35%\n58.65%\n    6\n44.89%\n81.98%\n    7\n45.65%\n63.08%\n    8\n38.58%\n68.24%\n    9\n38.25%\n56.01%\n    10\n49.93%\n75.54%\n  \n  \n  \n\n\n\n\n\n\nLet \\(\\hat p_i\\) be the sample proportion of commuters in county \\(i\\) that approve of the HIV lane (e.g. \\(\\hat p_1 = 12/(12+50)=0.194\\)). Select \\(a\\) abd \\(b\\) so that the mean and variance of the \\(\\mathrm{Beta}(a,b)\\) distribution match the mean and variance of the sample proportions \\(\\hat p_1, \\dots, \\hat p_{10}\\).\n\n\nSolution:\n\nd |&gt; \n    mutate(\n        p = Approve / (Approve + Disapprove)\n    ) |&gt; \n    summarise(\n        mean = mean(p),\n        var = var(p)\n    ) |&gt; \n    gt() |&gt; \n    cols_label(\n        mean = \"Mean\",\n        var = \"Variance\"\n    )\n\n\n\n\n\n  \n    \n    \n      Mean\n      Variance\n    \n  \n  \n    0.4800001\n0.02025887\n  \n  \n  \n\n\n\n\nSo we want the following to be approximately true\n\\[\n\\frac{\\alpha}{\\alpha + \\beta} = 0.48 \\qquad \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} =0.02\n\\]\nWe can solve this simply by creating a target function and using R’s built-in optim function\n\n```{r}\nmy_fun &lt;- function(pars) {\n    \n    alpha &lt;- pars[1]\n    beta &lt;- pars[2]\n    \n    # Calculate difference between current mean and desired mean\n    diff_mean &lt;- alpha / (alpha + beta) - 0.48\n    \n    # Calculate difference between current variance and desired variance\n    diff_var &lt;- (alpha * beta) / ((alpha + beta)^2 * (alpha + beta + 1)) - 0.02\n    \n    # Return the squared differences of both\n    diff_mean^2 + diff_var^2\n}\n\nresults &lt;- optim(c(1, 1), fn = my_fun)\n\nalpha &lt;- results$par[1]\nbeta &lt;- results$par[2]\n\nalpha\nbeta\n```\n\n[1] 5.51061\n[1] 5.96976\n\n\n\n\nConduct an empirical Bayesian analysis by computing the 95% posterior credible sets that results from analyzing each county separately using the \\(\\mathrm{Beta}(a,b)\\) prior you computed in b.\n\n\nSolution:\n\n```{r}\nemp_bayes_results &lt;- d |&gt; \n    mutate(\n        A = Approve + alpha,\n        B = Disapprove + beta,\n        Lower = qbeta(0.025, A, B),\n        Upper = qbeta(0.975, A, B)\n    ) \n\nemp_bayes_results |&gt; \n    gt() |&gt; \n    cols_hide(\n        columns = c(Approve, Disapprove, A, B)\n    ) |&gt; \n    tab_spanner(\n        label = \"95% Credible Interval\", \n        columns = c(Lower, Upper)\n    ) |&gt; \n    fmt_percent(\n        columns = c(Lower, Upper)\n    )\n```\n\n\n\n\n\n  \n    \n    \n      County\n      \n        95% Credible Interval\n      \n    \n    \n      Lower\n      Upper\n    \n  \n  \n    1\n14.87%\n34.13%\n    2\n32.09%\n44.05%\n    3\n47.48%\n63.09%\n    4\n22.30%\n58.56%\n    5\n41.52%\n58.15%\n    6\n42.92%\n75.01%\n    7\n45.48%\n62.24%\n    8\n39.00%\n65.66%\n    9\n38.66%\n55.70%\n    10\n48.47%\n72.23%\n  \n  \n  \n\n\n\n\n\n\nHow do the results from a. and c. differ? What are the advantages and disadvantages of these two analyses?\n\n\nSolution: The Empirical Bayes (EB) prior pulls each individual proportion slightly towards the overall mean. Since each county gets the same EB prior the effect is that counties with fewer votes get pulled more towards the overall mean, thus stabilizing the inferences in smaller counties by “borrowing” information from other counties.\nThe downside to using the EB prior is that the inference for each county is slightly biased to wards the overall mean.\nThis is an example of the bias/variance trade-off in statistics. We often want to reduce the variance in our statistical inference at the cost of increasing the bias in our inferences.\n\n\nCode\n```{r}\n#| code-fold: true\n\nemp_bayes_results |&gt; \n    mutate(\n        type = \"Empirical Bayes\"\n    ) |&gt; \n    bind_rows(\n        jeffreys_results |&gt; \n            mutate(\n                type = \"Jeffreys\"\n            )\n    ) |&gt; \n    ggplot(aes(x = County, ymin = Lower, ymax = Upper, col = type)) +\n    geom_hline(\n        yintercept = 0.48,\n        lty = 2,\n        linewidth = 0.5\n    ) +\n    geom_linerange(\n        linewidth = 1,\n        position = position_dodge(width = 0.3)\n        ) +\n    scale_x_continuous(\n        breaks = 1:10\n    ) +\n    scale_y_continuous(\n        limits = c(0, 1),\n        labels = scales::label_percent(),\n        expand = expansion()\n    ) +\n    annotate(\n        geom = \"text\",\n        x = 1.5,\n        y = 0.51,\n        label = \"Overall Mean\"\n    ) +\n    theme(legend.position = c(0.15, 0.9)) +\n    labs(\n        x = \"County\",\n        y = expression(hat(p)),\n        col = NULL,\n        title = \"Comparing the Jeffreys and Empirical Bayes intervals\",\n        subtitle = \"Empirical Bayes pulls the intervalls slightly towards the overall mean\"\n    )\n```"
  },
  {
    "objectID": "assignments/hw4/solution/index.html",
    "href": "assignments/hw4/solution/index.html",
    "title": "Homework #4 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n\n```{r}\n#| output: hide\n\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(cubature)\nlibrary(purrr)\nlibrary(scales)\nlibrary(glue)\nlibrary(rjags)\nlibrary(posterior)\nlibrary(bayesplot)\ntheme_set(theme_classic())\n```\n\n\n2\nAssume that \\(Y_i\\vert\\mu\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(\\mu,\\sigma_i^2)\\) for \\(i \\in \\{1, \\dots, n\\}\\), with \\(\\sigma_i\\) known and improper prior distribution \\(\\pi(\\mu)=1\\) for all \\(\\mu\\).\n\nGive a formula for the MAP estimator for \\(\\mu\\)\n\n\nSolution:\nWith a flat prior, our posterior distribution is simply equal to our likelihood.\n\\[\n\\begin{aligned}\np(\\mu \\vert \\mathbf Y) &= f(\\mathbf Y \\vert \\mu, \\mathbf \\sigma) \\\\\n&= \\prod_{i = 1}^n\\sigma_i^{-1} (2\\pi)^{-1/2}\\exp\\left[-\\frac{(Y_i - \\mu)^2}{2\\sigma_i^2}\\right] \\\\\n&\\propto \\prod_{i = 1}^n\\sigma_i^{-1} \\exp\\left[-\\frac{(Y_i - \\mu)^2}{2\\sigma_i^2}\\right] \\\\\n&= G(\\mu)\n\\end{aligned}\n\\]\nTake the log on both sides\n\\[\n\\begin{aligned}\n\\log G(\\mu) &= g(\\mu) \\\\\n&=\\sum_{i=1}^n-\\log\\sigma_i - \\frac{1}{2\\sigma_i^2}(Y_i - \\mu)^2 \\\\\n&= -\\sum_{i=1}^n n_i\\log\\sigma_i - \\frac{1}{2\\sigma_i^2}(Y_i^2 - 2Y_i\\mu + \\mu^2) \\\\\n\\frac{dg}{d\\mu} &= \\sum_{i=1}^n\\frac{1}{2\\sigma_i^2}(2\\mu - 2Y_i) \\\\\n&= \\sum_{i=1}^n\\frac{1}{\\sigma_i^2}(\\mu - Y_i)\n\\end{aligned}\n\\]\nTo find the MAP estimator, we find where this is equal to zero, i.e. \\(dg/d\\mu = 0\\).\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n\\frac{1}{\\sigma_i^2}\\mu &= \\sum_{i=1}^n\\frac{1}{\\sigma_i^2}Y_i \\\\\n\\rightarrow \\mu &= \\frac{\\sum_{i=1}^n\\frac{1}{\\sigma_i^2}Y_i}{\\sum_{i=1}^n\\frac{1}{\\sigma_i^2}}.\n\\end{aligned}\n\\]\nWe that the MAP estimator is simply a weighted average of the \\(Y_i\\)’s, where each observations contribution to the mean is inversely proportional to its variance. If all \\(Y_i\\) have the same variance, then this simplifies to the usual mean.\n\n\nWe observe \\(n=3, Y_1=12, Y_2=10, Y_3=22, \\sigma_1=\\sigma_2=3\\) and \\(\\sigma_3=10\\), compute the MAP estimate of \\(\\mu\\).\n\n\nSolution:\n\nmy_MAP &lt;- function(n, Y, sigma) {\n    if (length(Y) != n) stop(\"Y should have length equal to n\")\n    if (length(sigma) != n) stop(\"sigma should have length equal to n\")\n    \n    sum(Y / sigma^2) / sum(1/sigma^2)\n}\n\nn &lt;- 3\nY &lt;- c(12, 10, 22)\nsigma &lt;- c(3, 3, 10)\n\nmu_MAP &lt;- my_MAP(n, Y, sigma)\n\nmu_MAP\n\n[1] 11.47368\n\n\n\n\nUse numerical integration to compute the posterior mean of \\(\\mu\\).\n\n\nSolution:\nThe posterior distribution, being equal to the likelihood, is normalized with respect to \\(Y\\), but not necessarily with respect to \\(\\mu\\). Therefore we first integrate over \\(\\mu\\) to calculate the normalizing constant and then we can calculate the posterior mean.\n\nposterior_unnormalized &lt;- function(mu) {\n    n &lt;- length(mu) \n    out &lt;- numeric(n)\n    \n    for (i in seq_len(n)) {\n        out &lt;- exp(sum(dnorm(Y, mean = mu[i], sd = sigma, log = TRUE)))\n    }\n    \n    out\n}\n\nmarginal &lt;- adaptIntegrate(\n    posterior_unnormalized,\n    lowerLimit = -Inf,\n    upperLimit = Inf\n)$int\n\n\nposterior &lt;- function(mu) {\n    n &lt;- length(mu) \n    out &lt;- numeric(n)\n    \n    for (i in seq_len(n)) {\n        out[i] &lt;- posterior_unnormalized(mu[i]) / marginal\n    }\n    \n    out\n}\n\n\nmy_mean &lt;- function(mu) {\n    n &lt;- length(mu) \n    out &lt;- numeric(n)\n    \n    for (i in seq_len(n)) {\n        out[i] &lt;- mu[i] * posterior(mu[i])\n    }\n    \n    out\n}\n\nmu_MEAN &lt;- adaptIntegrate(\n    my_mean,\n    lowerLimit = -Inf,\n    upperLimit = Inf\n)$int\n\nmu_MEAN\n\n[1] 11.47368\n\n\n\n\nPlot the posterior distribution of \\(\\mu\\) and indicate the MAP and the posterior mean estimates on the plot.\n\n\nSolution:\n\ntibble(\n    mu = 11.47 + 7 * seq(-1, 1, length.out = 1e2)\n) |&gt; \n    mutate(\n        post = posterior(mu)\n    ) |&gt; \n    ggplot(aes(mu, post)) +\n    geom_area(\n        alpha = 0.5\n    ) +\n    geom_vline(xintercept = mu_MAP, lty = 4, col = \"red\") +\n    geom_vline(xintercept = mu_MEAN, lty = 2, col = \"blue\") +\n    scale_x_continuous(\n        breaks = c(mu_MEAN, seq(7, 16, by = 3)),\n        labels = label_number(accuracy = 0.01)\n    ) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = expression(mu),\n        y = expression(paste(p, \"(\", mu, \"|\", Y, \")\")),\n        title = expression(paste(\"Posterior distribution of \", mu)),\n        subtitle = \"The MAP and posterior mean are equal\"\n    )\n\n\n\n\n\n\n\n4\nConsider the model\n\\[\nY_i\\vert\\sigma^2_i\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(0,\\sigma_i^2),\n\\]\nfor \\(i \\in \\{1, \\dots, n\\}\\) where\n\\[\n\\begin{gathered}\n\\sigma_i^2\\vert b \\sim \\mathrm{InvGamma}(a,b) \\\\ b\\sim\\mathrm{Gamma}(1,1)\n\\end{gathered}\n\\]\n\nDerive the full conditional posterior distributions for \\(\\sigma^2_1\\) and \\(b\\).\n\n\nSolution:\n\\[\n\\begin{aligned}\np(\\sigma^1_2\\vert Y, b) &\\propto f(Y \\vert \\sigma_1^2) \\pi(\\sigma_1^2\\vert b) \\\\\n&= \\mathrm{Normal}(Y\\vert \\sigma_1^2) \\cdot \\mathrm{InvGamma}(\\sigma_1^2\\vert a,b)\\\\\n&= {\\sigma_1}^{-1}(2\\pi)^{-1/2}\\exp\\left[-\\frac{{Y_1}^2}{2{\\sigma_1^2}}\\right] \\frac{{b}^{a}}{\\Gamma(a)}{(\\sigma_1^2)}^{-{a}-1}\\exp\\left(-{b}/{\\sigma_1^2}\\right) \\\\\n&\\propto {(\\sigma_1^2)}^{-1/2}\\exp\\left[-\\frac{{Y_1}^2}{2{\\sigma_1^2}}\\right] {(\\sigma_1^2)}^{-{a}-1}\\exp\\left(-{b}/{\\sigma_1^2}\\right) \\\\\n&= (\\sigma_1^2)^{-(a+1/2) - 1} \\exp\\left[-\\left(\\frac{Y_1^2}{2} + b\\right)/\\sigma_1^2\\right].\n\\end{aligned}\n\\]\nWe see that the full conditional posterior for \\(\\sigma_1^2\\) is an \\(\\mathrm{InvGamma}(a + 1/2, b + Y_1^2/2)\\) distribution.\n\\[\n\\begin{aligned}\np(b\\vert \\sigma_1, \\dots, \\sigma_n) &\\propto \\pi(b) \\prod_{i=1}^n\\pi(\\sigma_i^2\\vert b) \\\\\n&= \\exp\\left(-b\\right)\\prod_{i=1}^n \\frac{{b}^{a}}{\\Gamma(a)}{(\\sigma_i^2)}^{-{a}-1}\\exp\\left(-{b}/{\\sigma_i^2}\\right) \\\\\n&\\propto  \\exp\\left(-b\\right)\\prod_{i=1}^n {b}^{a}\\exp\\left(-{b}/{\\sigma_i^2}\\right) \\\\\n&= \\exp\\left(-b\\right)b^{na} \\exp\\left( \\sum_{i=1}^{n}{-b/\\sigma_i^2}\\right) \\\\\n&= b^{(1+na)-1}\\exp\\left(-b - \\sum_{i=1}^{n}{}\\frac{1}{ \\sigma_i^2}b\\right) \\\\\n&= b^{(1+na)-1} \\exp\\left(-\\left(1 + \\sum_{i=1}^{n}{}\\frac{1}{\\sigma_i^2}\\right)b\\right).\n\\end{aligned}\n\\]\nWe also see that the full conditional posterior for \\(b\\) is an \\(\\mathrm{Gamma}(1 + na, 1 + \\sum_{i=1}^{n}{}\\frac{1}{\\sigma_i^2})\\) distribution.\n\n\nWrite pseudocode for Gibbs sampling, i.e. describe in detail each step of the Gibbs sampling algorithm.\n\n\nSolution: In Gibbs sampling we iterate over each unknown parameter and sample from its full conditional posterior distribution. In our case the unknown parameters are the \\(\\sigma_i^2\\)’s and \\(b\\). To perform Gibbs sampling we would therefore do the following:\n\nset a large value for N_samples\nset a value for a\nread in data Y as length n vector\n\ninitialize b by sampling from prior\ninitialize sigma2 as a length n vector by sampling from prior\n\nfor i from 1 to N_samples:\n    for j from 1 to n:\n    sample sigma2[i] from InvGamma(a + 1/2, b + Y[i]^2/2) \nend for\nsample b from InvGamma(n*a, 1 + sum(1/sigma2))\n\n\n\nWrite your own Gibbs sampling code (not in JAGS) and plot the marginal posterior density of each parameter. Assume \\(n=10\\), \\(a=10\\) and \\(Y_i=i\\) for \\(i=1, \\dots, 10\\).\n\n\nSolution:\nWe’re going to use the posterior package to easily analyze our posterior distribution after we’ve run our Gibbs sampler. The posterior package gives us acces to a draws_df data type that we can create using the as_draws_df() function. To be able to convert our posterior into a draws_df, we want to save our results in an array with dimensions \\(\\mathrm{N_{sample}\\times N_{chain} \\times N_{variable}}\\). I am planning on performing 10000 samples in each of 4 chains and there are 11 total variables, so our array will have the shape \\(10000 \\times 4 \\times 11\\). We then have to keep track of which dimension we are indexing at each time.\nFirst we define functions for sampling from each parameter’s full conditional distribution.\n\nsample_sigma2 &lt;- function(a, b, Y) {\n    1/rgamma(length(Y), shape = a + 1/2, rate = b + Y^2/2)\n}\n\n\nsample_b &lt;- function(n, a, sigma2) {\n    rgamma(1, shape = n * a + 1, rate = 1 + sum(1/sigma2))\n}\n\nWe then write a function for Gibbs sampling that wraps around these two functions defined above. The gibbs_sampler function needs five inputs:\n\nn_samples: The number of samples we want from our posterior within each chain\nn_chain: The number of chains we want to run\nn_variables: The number of variables in our posterior\nY: Our observed data\na: For our inverse gamma prior\nn_burnin: The number of samples we want to draw as part of our burn-in. This is by default equal to n_samples\n\n\ngibbs_sampler &lt;- function(n_samples, n_chains, n_variables, Y, a, n_burnin = n_samples) {\n    \n    posterior &lt;- array(dim = c(n_samples, n_chains, n_variables))\n    dimnames(posterior)[[3]] &lt;- c(\"b\", glue(\"sigma[{1:10}]\"))\n    \n    for (chain in seq_len(n_chains)) {\n        # Initial values for the Gibbs sampler\n        b_value &lt;- 1\n        sigma2_value &lt;- sample_sigma2(a, b_value, Y)\n        \n        # Burn-in period\n        # Run the sampler without saving our values\n        for (i in seq_len(n_burnin)) {\n            b_value &lt;- sample_b(n, a, sigma2_value)\n            sigma2_value &lt;- sample_sigma2(a, b_value, Y)\n        }\n        \n        # Running the Gibbs sampler for n_samples iterations\n        for (i in seq_len(n_samples)) {\n            # Here we perform the sampling\n            b_value &lt;- sample_b(n, a, sigma2_value)\n            sigma2_value &lt;- sample_sigma2(a, b_value, Y)\n            \n            # Here we store the samples in our array\n            # Pay close attention to the indexing\n            posterior[i, chain, 1] &lt;- b_value\n            posterior[i, chain, -1] &lt;- sigma2_value\n        }\n    }\n    # Convert our array into a draws_df\n    posterior |&gt; \n        as_draws_df()\n}\n\n\nn &lt;- 10\na &lt;- 10\nY &lt;- 1:10\n\n# 10xsigma2 + b = 11\nn_variables &lt;- n + 1\nn_chains &lt;- 4\nn_samples &lt;- 1e4\n\nposterior &lt;- gibbs_sampler(n_samples, n_chains, n_variables, Y, a)\n\nHaving done the preparatory work of converting our results into a draws_df object, we are rewarded with access the great bayesplot package for plotting our posterior.\n\nposterior |&gt; \n    mcmc_dens()\n\n\n\n\n\nsummarised_results_gibbs &lt;- posterior |&gt; \n    summarise_draws()\n\nsummarised_results_gibbs |&gt; \n    gt() |&gt; \n    fmt_number(\n        decimals = 2,\n        columns = mean:rhat\n    ) |&gt; \n    fmt_number(\n        decimals = 0,\n        columns = starts_with(\"ess\")\n    ) |&gt; \n    tab_header(\n        title = \"Summarizing the posterior distribution of our model\"\n    )\n\n\n\n\n\n  \n    \n      Summarizing the posterior distribution of our model\n    \n    \n    \n      variable\n      mean\n      median\n      sd\n      mad\n      q5\n      q95\n      rhat\n      ess_bulk\n      ess_tail\n    \n  \n  \n    b\n30.31\n30.06\n4.37\n4.34\n23.63\n37.94\n1.00\n12,650\n23,456\n    sigma[1]\n3.24\n3.01\n1.22\n1.02\n1.76\n5.48\n1.00\n23,394\n32,619\n    sigma[2]\n3.40\n3.17\n1.26\n1.05\n1.86\n5.72\n1.00\n23,240\n31,956\n    sigma[3]\n3.67\n3.41\n1.35\n1.12\n2.02\n6.22\n1.00\n23,434\n34,528\n    sigma[4]\n4.04\n3.76\n1.46\n1.24\n2.25\n6.76\n1.00\n27,360\n35,019\n    sigma[5]\n4.52\n4.20\n1.64\n1.36\n2.54\n7.55\n1.00\n28,972\n35,099\n    sigma[6]\n5.09\n4.73\n1.82\n1.51\n2.88\n8.51\n1.00\n30,080\n35,262\n    sigma[7]\n5.76\n5.38\n2.01\n1.70\n3.29\n9.52\n1.00\n31,927\n37,516\n    sigma[8]\n6.56\n6.12\n2.30\n1.91\n3.74\n10.85\n1.00\n33,774\n36,945\n    sigma[9]\n7.46\n6.97\n2.59\n2.18\n4.28\n12.29\n1.00\n33,696\n37,902\n    sigma[10]\n8.45\n7.88\n2.93\n2.46\n4.89\n13.92\n1.00\n36,205\n38,387\n  \n  \n  \n\n\n\n\n\n\nRepeat the analysis with \\(a=1\\) and comment on the convergence of the MCMC chain.\n\n\nSolution: We can now reuse our code from above, only having to change the value of a\n\na &lt;- 1\n\nposterior &lt;- gibbs_sampler(n_samples, n_chains, n_variables, Y, a)\n\nHaving done the preparatory work of converting our results into a draws_df object, we are rewarded with the great functions available in the posterior package. The summarise_draws() function is a great way to easily obtain summarized results about our posterior.\nThe table below shows us that \\(\\hat R\\) and our effective sample size (ESS) are both acceptable, so it seems our sampler has converged.\nThe \\(\\hat R\\) is not the same as in the book, since the posterior package has implemented the proposed updates from Vehtari et al. (2021) that increase its ability to find faulty convergence. Have a look at the article to learn more about convergence. You can also read about \\(\\mathrm{ESS_{bulk}}\\) and \\(\\mathrm{ESS_{tail}}\\) there, but in simple terms, they calculate the ESS for the center and the tails of the posterior distributions.\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved Rˆ for Assessing Convergence of MCMC (with Discussion).” Bayesian Analysis 16 (2): 667–718. https://doi.org/10.1214/20-BA1221.\n\nsummarised_results_gibbs &lt;- posterior |&gt; \n    summarise_draws()\n\nsummarised_results_gibbs |&gt; \n    gt() |&gt; \n    fmt_number(\n        decimals = 2,\n        columns = mean:rhat\n    ) |&gt; \n    fmt_number(\n        decimals = 0,\n        columns = starts_with(\"ess\")\n    ) |&gt; \n    tab_header(\n        title = \"Summarizing the posterior distribution of our model\"\n    )\n\n\n\n\n\n  \n    \n      Summarizing the posterior distribution of our model\n    \n    \n    \n      variable\n      mean\n      median\n      sd\n      mad\n      q5\n      q95\n      rhat\n      ess_bulk\n      ess_tail\n    \n  \n  \n    b\n5.54\n5.27\n1.97\n1.85\n2.80\n9.18\n1.00\n22,252\n31,095\n    sigma[1]\n11.33\n4.87\n42.86\n4.03\n1.29\n34.27\n1.00\n29,404\n34,006\n    sigma[2]\n15.93\n6.22\n250.47\n5.04\n1.74\n43.12\n1.00\n32,105\n35,484\n    sigma[3]\n22.15\n8.40\n380.42\n6.76\n2.43\n57.66\n1.00\n35,205\n38,400\n    sigma[4]\n30.77\n11.34\n887.42\n9.00\n3.33\n77.21\n1.00\n36,617\n39,118\n    sigma[5]\n34.70\n15.05\n146.41\n11.80\n4.48\n102.09\n1.00\n39,052\n38,400\n    sigma[6]\n47.44\n20.00\n493.02\n15.63\n6.01\n135.47\n1.00\n37,692\n38,685\n    sigma[7]\n57.16\n25.46\n242.84\n19.96\n7.59\n167.98\n1.00\n38,983\n37,764\n    sigma[8]\n72.49\n31.67\n286.76\n24.65\n9.55\n216.06\n1.00\n38,562\n39,211\n    sigma[9]\n91.08\n38.77\n702.18\n30.26\n11.73\n258.83\n1.00\n39,571\n39,969\n    sigma[10]\n108.75\n46.92\n831.57\n36.62\n14.12\n309.89\n1.00\n39,833\n40,098\n  \n  \n  \n\n\n\n\n\n\nImplement the model in c. using JAGS and compare the results with the results in c.\n\n\nSolution:\n\ndata = list(\n    Y = 1:10\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:10) {\n        Y[i] ~ dnorm(0, tau[i])\n    }\n    # Priors\n    for (i in 1:10) {\n        tau[i] ~ dgamma(10, b)\n        sigma[i] &lt;- 1 / tau[i]\n    }\n    b ~ dgamma(1, 1)\n}\")\n\ninits &lt;- list(\n    b = 1,\n    tau = rep(1, 10)\n)\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    inits = inits,\n    n.chains = 4\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 10\n   Unobserved stochastic nodes: 11\n   Total graph size: 34\n\nInitializing model\n\nupdate(model, 10000)\n\nparams &lt;- names(inits)\nsamples &lt;- coda.samples(\n    model,\n    variable.names = c(\"b\", \"sigma\"),\n    n.iter = 10000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\nThe posterior package allows us to take our samples object that was output by coda.samples() and use as_draws_df() to convert it into a comfortable data type.\n\nsummarised_results_jags &lt;- posterior_jags |&gt; \n    summarise_draws()\n\nsummarised_results_jags |&gt; \n    gt() |&gt; \n    fmt_number(\n        decimals = 2,\n        columns = mean:rhat\n    ) |&gt; \n    fmt_number(\n        decimals = 0,\n        columns = starts_with(\"ess\")\n    ) |&gt; \n    tab_header(\n        title = \"Summarizing the posterior distribution of our model\"\n    )\n\n\n\n\n\n  \n    \n      Summarizing the posterior distribution of our model\n    \n    \n    \n      variable\n      mean\n      median\n      sd\n      mad\n      q5\n      q95\n      rhat\n      ess_bulk\n      ess_tail\n    \n  \n  \n    b\n30.24\n29.99\n4.33\n4.29\n23.54\n37.79\n1.00\n12,830\n22,620\n    sigma[1]\n3.24\n3.00\n1.21\n1.02\n1.75\n5.51\n1.00\n22,595\n30,887\n    sigma[2]\n3.39\n3.15\n1.26\n1.06\n1.86\n5.74\n1.00\n24,261\n31,995\n    sigma[3]\n3.65\n3.40\n1.33\n1.12\n2.02\n6.13\n1.00\n26,036\n33,313\n    sigma[4]\n4.03\n3.73\n1.46\n1.22\n2.25\n6.76\n1.00\n26,940\n33,350\n    sigma[5]\n4.52\n4.20\n1.63\n1.37\n2.51\n7.57\n1.00\n28,984\n35,915\n    sigma[6]\n5.09\n4.74\n1.81\n1.52\n2.88\n8.45\n1.00\n30,182\n36,100\n    sigma[7]\n5.78\n5.37\n2.06\n1.70\n3.28\n9.61\n1.00\n33,773\n37,433\n    sigma[8]\n6.55\n6.11\n2.28\n1.90\n3.75\n10.81\n1.00\n33,039\n38,003\n    sigma[9]\n7.43\n6.96\n2.58\n2.19\n4.28\n12.23\n1.00\n35,171\n38,084\n    sigma[10]\n8.46\n7.91\n2.93\n2.45\n4.87\n13.86\n1.00\n34,756\n38,592\n  \n  \n  \n\n\n\n\n\nsummarised_results_gibbs &lt;- gibbs_sampler(n_samples, n_chains, n_variables, Y, a = 10) |&gt; \n    summarise_draws()\n\nsummarised_results_combined &lt;- summarised_results_gibbs |&gt; \n    mutate(\n        type = \"Bespoke Gibbs\"\n    ) |&gt; \n    bind_rows(\n        summarised_results_jags |&gt; \n            mutate(\n                type = \"JAGS\"\n            )\n    )\n\n\nsummarised_results_combined |&gt; \n    filter(str_detect(variable, \"sigma\")) |&gt; \n    mutate(\n        variable = fct_reorder(variable, -parse_number(variable))\n    ) |&gt; \n    ggplot(aes(\n        y = variable,\n        x = mean, xmin = q5, xmax = q95,\n        col = type\n    )) +\n    geom_pointrange(\n        position = position_dodge(width = 0.7)\n    ) +\n    scale_color_brewer(palette = \"Set1\") +\n    labs(\n        x = NULL,\n        y = NULL,\n        title = \"Comparing our bespoke Gibbs sampler to JAGS\",\n        subtitle = \"The results seem to be identical\",\n        col = \"Sampler\"\n    )\n\n\n\n\n\n\nCode\nsummarised_results_combined |&gt; \n    filter(!str_detect(variable, \"sigma\")) |&gt; \n    ggplot(aes(\n        y = variable,\n        x = mean, xmin = q5, xmax = q95,\n        col = type\n    )) +\n    geom_pointrange(\n        position = position_dodge(width = 0.7)\n    ) +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"none\") +\n    labs(\n        x = NULL,\n        y = NULL,\n        title = \"Comparing our bespoke Gibbs sampler to JAGS\",\n        subtitle = \"The results seem to be identical\",\n        col = \"Sampler\"\n    )\n\n\n\n\n\n\n\n\n5\nConsider the model\n\\[\nY_i\\vert\\mu, \\sigma^2 \\sim \\mathrm{Normal}(\\mu,\\sigma^2),\\quad i=1,\\dots,n,\n\\]\nand\n\\[\nY_i\\vert\\mu,\\delta, \\sigma^2 \\sim \\mathrm{Normal}(\\mu+\\delta,\\sigma^2),\\quad i=n+1,\\dots,n+m,\n\\]\nwhere\n\\[\n\\begin{gathered}\n\\mu,\\delta\\sim\\mathrm{Normal}(0, 100^2) \\\\\n\\sigma^2\\sim\\mathrm{InvGamma}(0.01, 0.01).\n\\end{gathered}\n\\]\n\nGive an example of a real experiment for which this would be an appropriate model.\n\n\nSolution: A simple example would be a clinical trial for a pharmaceutical company wanting to know if a drug has a positive effect on patients. In this case, \\(\\mu\\) could be the average outcome of patients in the control group (who did not get the drug), and \\(\\mu + \\delta\\) the average outcome of patients in the case group (who did get the drug). The effect of the drug could then be estimated as \\(\\delta\\).\n\n\nDerive the full conditional posterior distributions for \\(\\mu\\), \\(\\delta\\), and \\(\\sigma^2\\).\n\n\nSolution:\nWe have three unknown parameters: \\(\\mu\\), \\(\\delta\\) and \\(\\sigma^2\\).\n\\[\n\\begin{aligned}\np(\\mu \\vert \\mathbf Y, \\sigma^2, \\delta) &\\propto f(\\mathbf Y \\vert \\mu, \\sigma^2, \\delta)\\pi(\\mu) \\\\\n&= \\prod_{i=1}^{n}{\\mathrm{Normal}(Y_i\\vert \\mu, \\sigma^2)} \\prod_{j=n+1}^{n+m}{\\mathrm{Normal}(Y_j\\vert \\mu, \\delta, \\sigma^2)} \\mathrm{Normal}(\\mu\\vert 0, 100^2) \\\\\n&= \\prod_{i=1}^{n}\\left({ {\\sigma}^{-1}(2\\pi)^{-1/2}\\exp\\left[-\\frac{({Y_i} - {\\mu})^2}{2{\\sigma}^2}\\right]}\\right) \\prod_{j=n+1}^{n+m}\\left({ {\\sigma}^{-1}(2\\pi)^{-1/2}\\exp\\left[-\\frac{({Y_j} - {(\\mu + \\delta)})^2}{2{\\sigma}^2}\\right]}\\right) {100}^{-1}(2\\pi)^{-1/2}\\exp\\left[-\\frac{\\mu^2}{2\\cdot {100}^2}\\right] \\\\\n&\\propto \\prod_{i=1}^{n}\\left({\\exp\\left[-\\frac{({Y_i} - {\\mu})^2}{2{\\sigma}^2}\\right]}\\right) \\prod_{j=n+1}^{n+m}\\left({ \\exp\\left[-\\frac{({Y_j} - {[\\mu + \\delta\n}])^2}{2{\\sigma}^2}\\right]}\\right) \\exp\\left[-\\frac{\\mu^2}{2\\cdot {100}^2}\\right] \\\\\n&= \\exp\\left[ -\\frac{1}{2\\sigma^2}\\left( \\sum_{i=1}^{n}{(Y_i - \\mu)^2 + \\sum_{j=n+1}^{n+m}{(Y_j - \\mu - \\delta)^2}}\\right) - \\frac{1}{2\\cdot100^2}\\mu^2\\right].\n\\end{aligned}\n\\]\nSimplifying and completing the square, we eventually end up with\n\\[\np(\\mu \\vert \\mathbf Y, \\sigma^2, \\delta) = \\mathrm{Normal}\\left( \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^{n}{Y_i} + \\frac{1}{\\sigma^2} \\sum_{j=n+1}^{n+m}{(Y_i- \\delta)}}{ \\frac{(n+m)}{\\sigma^2} + 1/100^2}, \\frac{1}{\\frac{(n+m)}{\\sigma^2} + 1/100^2}\\right).\n\\]\nSimilarly, we find out that\n\\[\np(\\delta \\vert \\mathbf Y, \\mu, \\sigma^2) = \\mathrm{Normal}\\left( \\frac{ \\frac{1}{\\sigma^2} \\sum_{j=n+1}^{n+m}{(Y_i-\\mu)}}{ \\frac{m}{\\sigma^2} + 1/100^2}, \\frac{1}{{ \\frac{m}{\\sigma^2} + 1/100^2}}\\right).\n\\]\nThe posterior for \\(\\sigma^2\\) is\n\\[\np(\\sigma^2 \\vert \\mathbf Y, \\mu, \\delta) = \\mathrm{InvGamma}\\left(0.01 + (n+m)/2, 0.01 + \\sum_{i=1}^{n}{(Y_i-\\mu)^2/2} + \\sum_{j=n+1}^{n+m}{(Y_i - \\mu - \\delta)^2/2}\\right)\n\\]\n\n\nSimulate a dataset from this model with \\(n=m=50\\), \\(\\mu=10\\), \\(\\delta=1\\) and \\(\\sigma=2\\). Write your own Gibbs sampling code (not in JAGS) to fit the model above to the simulated data and plot the marginal posterior density for each parameter. Are you able to recover the true values reasonably well?\n\n\nSolution:\n\nn &lt;- 50\nm &lt;- 50\nmu &lt;- 10\ndelta &lt;- 1\nsigma &lt;- 2\n\nY1 &lt;- rnorm(n, mu, sigma)\nY2 &lt;- rnorm(m, mu + delta, sigma)\n\n\nsample_mu &lt;- function(Y1, Y2, sigma2, delta) {\n    n &lt;- length(Y1)\n    m &lt;- length(Y2)\n    numerator &lt;- 1/sigma2 * (sum(Y1) + sum(Y2 - delta))\n    denominator &lt;- (n + m) / (sigma2) + 1/100^2\n    rnorm(\n        n = 1,\n        mean = numerator / denominator,\n        sd = 1 / sqrt(denominator)\n    )\n}\n\nsample_delta &lt;- function(Y2, mu, sigma2) {\n    m &lt;- length(Y2)\n    numerator &lt;- 1/sigma2 * sum(Y2 - mu)\n    denominator &lt;- m/sigma2 + 1/100^2\n    rnorm(\n        n = 1,\n        mean = numerator / denominator,\n        sd = 1 / sqrt(denominator)\n    )\n}\n\nsample_sigma2 &lt;- function(Y1, Y2, mu, delta) {\n    shape &lt;- 0.01 + (n + m) / 2\n    rate &lt;- 0.01 + sum((Y1 - mu)^2)/2 + sum((Y2 - mu - delta)^2)/2\n    n &lt;- length(Y1)\n    m &lt;- length(Y2)\n    1 / rgamma(\n        n = 1,\n        shape = shape,\n        rate = rate\n    )\n}\n\n\ngibbs_sampler &lt;- function(n_samples, n_chains, n_variables, Y1, Y2, n_burnin = n_samples) {\n    posterior &lt;- array(dim = c(n_samples, n_chains, n_variables))\n    dimnames(posterior)[[3]] &lt;- c(\"mu\", \"delta\", \"sigma2\")\n    \n    for (chain in seq_len(n_chains)) {\n        # Initial values for the Gibbs sampler\n        mu_value &lt;- rnorm(1, 0, 100)\n        delta_value &lt;- rnorm(1, 0, 100)\n        sigma2_value &lt;- 1 / rgamma(1, shape = 0.01, rate = 0.01)\n        \n        # Burn-in period\n        # Run the sampler without saving our values\n        for (i in seq_len(n_burnin)) {\n            mu_value &lt;- sample_mu(Y1, Y2, sigma2_value, delta_value)\n            delta_value &lt;- sample_delta(Y2, mu_value, sigma2_value)\n            sigma2_value &lt;- sample_sigma2(Y1, Y2, mu_value, delta_value)\n        }\n        \n        # Running the Gibbs sampler for n_samples iterations\n        for (i in seq_len(n_samples)) {\n            # Here we perform the sampling\n            mu_value &lt;- sample_mu(Y1, Y2, sigma2_value, delta_value)\n            delta_value &lt;- sample_delta(Y2, mu_value, sigma2_value)\n            sigma2_value &lt;- sample_sigma2(Y1, Y2, mu_value, delta_value)\n            \n            # Here we store the samples in our array\n            # Pay close attention to the indexing\n            posterior[i, chain, ] &lt;- c(mu_value, delta_value, sigma2_value)\n        }\n    }\n    # Convert our array into a draws_df\n    posterior |&gt; \n        as_draws_df()\n}\n\n\nn_samples &lt;- 1e3\nn_chains &lt;- 4\nn_variables &lt;- 3\n\nposterior &lt;- gibbs_sampler(n_samples, n_chains, n_variables, Y1, Y2)\n\n\nposterior |&gt; \n    mcmc_areas(\n        pars = \"mu\"\n    ) +\n    geom_vline(xintercept = mu)\n\n\n\n\n\nposterior |&gt; \n    mcmc_areas(\n        pars = \"delta\"\n    ) +\n    geom_vline(xintercept = delta)\n\n\n\n\n\nposterior |&gt; \n    mcmc_areas(\n        pars = \"sigma2\"\n    ) +\n    geom_vline(xintercept = sigma^2)\n\n\n\n\n\n\nImplement this model using JAGS and compare the results with the results in .c.\n\n\nSolution:\n\ndata = list(\n    Y1 = Y1,\n    Y2 = Y2,\n    n = n,\n    m = m\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        Y1[i] ~ dnorm(mu, tau)\n    }\n    \n    for (i in 1:m) {\n        Y2[i] ~ dnorm(mu + delta, tau)\n    }\n    # Priors\n    mu ~ dnorm(0, 0.01^2)\n    delta ~ dnorm(0, 0.01^2)\n    tau ~ dgamma(0.01, 0.01)\n    sigma2 &lt;- 1 / tau\n}\")\n\ninits &lt;- list(\n    mu = mean(Y1),\n    delta = mean(Y2) - mean(Y1),\n    tau = 1 / var(Y1)\n)\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    inits = inits,\n    n.chains = 4\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 100\n   Unobserved stochastic nodes: 3\n   Total graph size: 112\n\nInitializing model\n\nupdate(model, 10000)\n\nparams &lt;- names(inits)\nsamples &lt;- coda.samples(\n    model,\n    variable.names = params,\n    n.iter = 10000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\n\n\n\n10\nAs discussed in section 1.6, report that the number of marine bivalve species discovered each year from 2010-2015 was 64, 13, 33, 18, 30 and 20. Denote \\(Y_t\\) as the number of species discovered in year \\(2009+t\\) (so that \\(Y_1=64\\) is the count for 2010). Use JAGS to fit the model\n\\[\n\\begin{gathered}\nY_t\\vert \\alpha, \\beta \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Poisson}(\\lambda_t)\\\\\n\\lambda_t = \\exp(\\alpha + \\beta t) \\text{ or equivalently } \\log(\\lambda_t) = \\alpha + \\beta t \\\\\n\\alpha,\\beta \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(0,10^2).\n\\end{gathered}\n\\]\nSummarize the posterior of \\(\\alpha\\) and \\(\\beta\\) and verify that the MCMC sampler has converged. Does this analysis provide evidence that the rate of discovery is changing over time?\n\ndata = list(\n    Y = c(64, 13, 33, 18, 30, 20),\n    n = 6\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        Y[i] ~ dpois(lambda[i])\n        lambda[i] &lt;- exp(alpha + beta * i)\n    }\n    \n    alpha ~ dnorm(0, 0.1^2)\n    beta ~ dnorm(0, 0.1^2)\n}\")\n\ninits &lt;- list(\n    alpha = 0,\n    beta = 0\n)\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    inits = inits,\n    n.chains = 4\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 6\n   Unobserved stochastic nodes: 2\n   Total graph size: 36\n\nInitializing model\n\nupdate(model, 10000)\n\nparams &lt;- names(inits)\nsamples &lt;- coda.samples(\n    model,\n    variable.names = params,\n    n.iter = 10000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\n\nposterior_jags |&gt; \n    summarise_draws() |&gt; \n    gt()\n\n\n\n\n\n  \n    \n    \n      variable\n      mean\n      median\n      sd\n      mad\n      q5\n      q95\n      rhat\n      ess_bulk\n      ess_tail\n    \n  \n  \n    alpha\n3.9683149\n3.9691353\n0.15364657\n0.15374219\n3.7126345\n4.2189888\n1.000975\n3579.708\n6825.735\n    beta\n-0.1804355\n-0.1800088\n0.04481662\n0.04476587\n-0.2547523\n-0.1072434\n1.001058\n3563.981\n6835.332\n  \n  \n  \n\n\n\n\n\nposterior_jags |&gt; \n    mcmc_areas(\n        pars = \"beta\"\n    )"
  },
  {
    "objectID": "assignments/hw5/solution/index.html",
    "href": "assignments/hw5/solution/index.html",
    "title": "Homework #5",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n```{r}\n#| output: hide\n\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(cubature)\nlibrary(purrr)\nlibrary(scales)\nlibrary(glue)\nlibrary(rjags)\nlibrary(posterior)\nlibrary(bayesplot)\ntheme_set(theme_classic())\n```\n\n\n1\nA clinical trial gave six subjects a placebo and six subjects a new weight loss medication. The response variable is the change in weight (pounds) from baseline (so -2.0 means the subject lost 2 pounds). The data for the 12 subjects are:\n\nd &lt;- tribble(\n    ~Placebo, ~Treatment,\n    2.0, -3.5,\n    -3.1, -1.6,\n    -1.0, -4.6,\n    0.2, -0.9,\n    0.3, -5.1,\n    0.4, 0.1\n)\n\nd |&gt; \n    gt::gt()\n\n\n\n\n\n  \n    \n    \n      Placebo\n      Treatment\n    \n  \n  \n    2.0\n-3.5\n    -3.1\n-1.6\n    -1.0\n-4.6\n    0.2\n-0.9\n    0.3\n-5.1\n    0.4\n0.1\n  \n  \n  \n\n\n\n\nConduct a Bayesian analysis to compare the means of these two groups. Would you say the treatment is effective? Is your conclusion sensitive to the prior?\n\nSolution: We can use the model we defined in last week’s homework to analyse these data:\n\\[\nY_i\\vert\\mu, \\sigma^2 \\sim \\mathrm{Normal}(\\mu,\\sigma^2),\\quad i=1,\\dots,n,\n\\]\nand\n\\[\nY_i\\vert\\mu,\\delta, \\sigma^2 \\sim \\mathrm{Normal}(\\mu+\\delta,\\sigma^2),\\quad i=n+1,\\dots,n+m,\n\\]\nwhere\n\\[\n\\begin{gathered}\n\\mu,\\delta\\sim\\mathrm{Normal}(0, 100^2) \\\\\n\\sigma^2\\sim\\mathrm{InvGamma}(0.01, 0.01).\n\\end{gathered}\n\\]\n\ndata = list(\n    Y1 = d$Placebo,\n    Y2 = d$Treatment,\n    n = nrow(d),\n    m = nrow(d)\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        Y1[i] ~ dnorm(mu, tau)\n    }\n    \n    for (i in 1:m) {\n        Y2[i] ~ dnorm(mu + delta, tau)\n    }\n    # Priors\n    mu ~ dnorm(0, 0.01^2)\n    delta ~ dnorm(0, 0.01^2)\n    tau ~ dgamma(0.01, 0.01)\n    sigma2 &lt;- 1 / tau\n}\")\n\ninits &lt;- list(\n    mu = mean(d$Placebo),\n    delta = mean(d$Treatment) - mean(d$Placebo),\n    tau = 1 / var(d$Placebo)\n)\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    inits = inits,\n    n.chains = 4\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 12\n   Unobserved stochastic nodes: 3\n   Total graph size: 24\n\nInitializing model\n\nupdate(model, 10000)\n\nparams &lt;- names(inits)\nsamples &lt;- coda.samples(\n    model,\n    variable.names = params,\n    n.iter = 10000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\n\nposterior_jags |&gt; \n    summarise_draws() |&gt; \n    gt() |&gt; \n    fmt_number(\n        columns = mean:rhat,\n        decimals = 3\n    ) |&gt; \n    fmt_number(\n        columns = starts_with(\"ess\"),\n        decimals = 0\n    )\n\n\n\n\n\n  \n    \n    \n      variable\n      mean\n      median\n      sd\n      mad\n      q5\n      q95\n      rhat\n      ess_bulk\n      ess_tail\n    \n  \n  \n    delta\n−2.411\n−2.409\n1.243\n1.154\n−4.432\n−0.399\n1.001\n13,160\n21,158\n    mu\n−0.194\n−0.196\n0.881\n0.819\n−1.608\n1.224\n1.000\n13,230\n21,213\n    tau\n0.273\n0.255\n0.122\n0.115\n0.108\n0.500\n1.000\n25,626\n28,448\n  \n  \n  \n\n\n\n\n\nposterior_jags |&gt; \n    mcmc_areas(\n        pars = \"delta\"\n    ) +\n    labs(\n        title = expression(paste(\"Posterior distribution of \", delta))\n    )\n\n\n\n\n\np_lower &lt;- posterior_jags |&gt; \n    subset_draws(variable = \"delta\") |&gt; \n    summarise_draws(\"P(delta &lt; 0)\" = function(x) mean(x &lt; 0))\n\np_lower |&gt; \n    gt() |&gt; \n    fmt_percent(\n        columns = 2\n    )\n\n\n\n\n\n  \n    \n    \n      variable\n      P(delta &lt; 0)\n    \n  \n  \n    delta\n97.32%\n  \n  \n  \n\n\n\n\n\ndata = list(\n    Y1 = d$Placebo,\n    Y2 = d$Treatment,\n    n = nrow(d),\n    m = nrow(d)\n)\nmodel_string &lt;- textConnection(\"model{\n    # Likelihood\n    for (i in 1:n) {\n        Y1[i] ~ dnorm(mu, tau[1])\n    }\n    \n    for (i in 1:m) {\n        Y2[i] ~ dnorm(mu + delta, tau[2])\n    }\n    # Priors\n    mu ~ dnorm(0, 0.01^2)\n    delta ~ dnorm(0, 0.01^2)\n    for (i in 1:2) {\n        tau[i] ~ dgamma(0.01, 0.01)\n        sigma2[i] &lt;- 1 / tau[i]\n    }\n    \n}\")\n\ninits &lt;- list(\n    mu = mean(d$Placebo),\n    delta = mean(d$Treatment) - mean(d$Placebo),\n    tau = rep(1 / var(d$Placebo), 2)\n)\n\nmodel &lt;- jags.model(\n    model_string,\n    data = data,\n    inits = inits,\n    n.chains = 4\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 12\n   Unobserved stochastic nodes: 4\n   Total graph size: 26\n\nInitializing model\n\nupdate(model, 10000)\n\nparams &lt;- names(inits)\nsamples &lt;- coda.samples(\n    model,\n    variable.names = params,\n    n.iter = 10000\n)\n\nposterior_jags &lt;- samples |&gt; \n    as_draws_df()\n\n\nposterior_jags |&gt; \n    summarise_draws() |&gt; \n    gt() |&gt; \n    fmt_number(\n        columns = mean:rhat,\n        decimals = 3\n    ) |&gt; \n    fmt_number(\n        columns = starts_with(\"ess\"),\n        decimals = 0\n    )\n\n\n\n\n\n  \n    \n    \n      variable\n      mean\n      median\n      sd\n      mad\n      q5\n      q95\n      rhat\n      ess_bulk\n      ess_tail\n    \n  \n  \n    delta\n−2.398\n−2.388\n1.420\n1.268\n−4.712\n−0.135\n1.000\n13,677\n18,588\n    mu\n−0.199\n−0.202\n0.901\n0.743\n−1.614\n1.217\n1.000\n12,057\n14,829\n    tau[1]\n0.341\n0.298\n0.216\n0.191\n0.078\n0.760\n1.000\n22,234\n24,083\n    tau[2]\n0.225\n0.196\n0.142\n0.126\n0.052\n0.500\n1.000\n27,739\n30,821\n  \n  \n  \n\n\n\n\n\nposterior_jags |&gt; \n    mcmc_areas(\n        pars = \"delta\"\n    ) +\n    labs(\n        title = expression(paste(\"Posterior distribution of \", delta))\n    )\n\n\n\n\n\np_lower &lt;- posterior_jags |&gt; \n    subset_draws(variable = \"delta\") |&gt; \n    summarise_draws(\"P(delta &lt; 0)\" = function(x) mean(x &lt; 0))\n\np_lower |&gt; \n    gt() |&gt; \n    fmt_percent(\n        columns = 2\n    )\n\n\n\n\n\n  \n    \n    \n      variable\n      P(delta &lt; 0)\n    \n  \n  \n    delta\n95.75%\n  \n  \n  \n\n\n\n\n\n\n\n2\nLoad the classic Boston Housing Data in R:\n\ndata(Boston, package = \"MASS\")\n\nThe response variable is medv, the median value of owner-occupied homes (in $1,000s), and the other 13 variables are covariates that describe the neighborhood.\n\nFit a Bayesian linear regression model with uninformative Gaussian priors for the regression coefficients. Verify the MCMC sampler has converged, and summarize the posterior distribution of all regression coefficients.\n\n\nSolution: We use the Jeffreys prior\n\\[\ngamma\n\\]\n\\[\np(\\beta, \\sigma^2) \\propto \\sigma^{-2},\n\\]\nwhich gives us the posterior distributions\n\\[\n\\begin{aligned}\n\\beta \\vert \\sigma^2, \\mathbf Y &\\sim \\mathrm{Normal}(\\hat \\beta, V_\\beta \\sigma^2) \\\\\n\\sigma^2 \\vert \\mathbf Y &\\sim \\mathrm{}\n\\end{aligned}\n\\]\n\\[\n\\hat \\beta = \\left(X^T X\\right)^{-1}X^T y \\quad \\text{and} \\quad V_\\beta = \\left(X^TX\\right)^{-1}.\n\\]\nThis will give us a proper posterior distribution, since we have many more data points than we have parameters in our model.\n\nY &lt;- Boston$medv\nX &lt;- Boston |&gt; select(-medv) |&gt; as.matrix()\n\nbeta_mean &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\n\n\n\nPerform a classic least squares (e.g. using the lm function in R). Compare the results numerically and conceptually with the Bayesian results.\n\n\nSolution:\n\n\nRefit the Bayesian model with double exponential priors for the regression coefficients, and discuss how the results differ from the analysis with uninformative priors.\n\n\nSolution:\n\n\nFit a Bayesian linear regression model in a. using only the first 500 observations and compute the posterior predictive distribution for the final 6 observations. Plot the posterior predictive distribution versus the actual value for these 6 observations and comment on whether the predictions are reasonable.\n\n\nSolution:"
  },
  {
    "objectID": "assignments/hw2/solution/index.html",
    "href": "assignments/hw2/solution/index.html",
    "title": "Homework #2 (Solution)",
    "section": "",
    "text": "Download the Quarto document used to render this file\nlibrary(tidyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(glue)\ntheme_set(theme_classic())"
  },
  {
    "objectID": "assignments/hw2/solution/index.html#exercise-1",
    "href": "assignments/hw2/solution/index.html#exercise-1",
    "title": "Homework #2 (Solution)",
    "section": "Exercise 1",
    "text": "Exercise 1\nAssume \\(Y_1, \\dots, Y_n \\vert \\mu \\overset{\\mathrm{iid}}{\\sim}\\) where \\(\\sigma^2\\) is fixed and the unknown mean \\(\\mu\\) has prior \\(\\mu\\sim\\mathrm{Normal}(0, \\sigma^2/m)\\)\n\nGive a 95% posterior interval for \\(\\mu\\)\n\n\nSolution: From chapter 2.1.3 and derivations in Appendix 3 of the book, we immediately write our posterior as\n\\[\n\\begin{aligned}\n\\mu\\vert \\mathbf Y &\\sim \\mathrm{N}(w \\bar Y + (1 - w)\\mu_0, \\frac{\\sigma^2}{n + m}) \\\\\n&= \\mathrm{N}(w \\bar Y, \\frac{\\sigma^2}{n + m}), \\quad \\mu_0 = 0,\n\\end{aligned}\n\\]\nwhere \\(w = n/(n+m) \\in [0, 1]\\). A 95% posterior interval for \\(\\mu\\) is any interval, \\((l, u)\\) such that\n\\[\nP(l &lt; \\mu &lt; u) = 0.95\n\\]\nLet \\(z_\\alpha\\) denote the number such that if \\(Z \\sim \\mathrm{N}(0, 1)\\), then\n\\[\nP(Z &lt; z_\\alpha) = \\alpha.\n\\]\nWe can use R to find the values of \\(z_{0.025}\\) and \\(z_{0.975}\\)\n\n```{r}\nqnorm(0.025)\nqnorm(0.975)\n```\n\n[1] -1.959964\n[1] 1.959964\n\n\nSince \\(\\mu\\) is normally distributed with mean \\(w\\hat Y\\) and variance \\(\\sigma^2/(n+m)\\), we know that\n\\[\n\\frac{\\mu - w\\hat Y}{\\sigma/\\sqrt{n+m}} \\sim \\mathrm{N}(0, 1)\n\\] We can thus deduce our interval by writing\n\\[\n\\begin{aligned}\n&P(-1.96 &lt; \\frac{\\mu - w\\hat Y}{\\sigma/\\sqrt{n+m}} &lt; 1.96) = 0.95 \\\\\n\\rightarrow\\quad &P(-1.96\\frac{\\sigma}{\\sqrt{n+m}} &lt; \\mu - w\\hat Y &lt; 1.96\\frac{\\sigma}{\\sqrt{n+m}}) = 0.95 \\\\\n\\rightarrow\\quad &P(w\\hat Y -1.96\\frac{\\sigma}{\\sqrt{n+m}} &lt; \\mu &lt; w\\hat Y + 1.96\\frac{\\sigma}{\\sqrt{n+m}}) = 0.95.\n\\end{aligned}\n\\]\nThis gives us the 95% posterior interval\n\\[\n(w\\hat Y -1.96\\frac{\\sigma}{\\sqrt{n+m}}, w\\hat Y + 1.96\\frac{\\sigma}{\\sqrt{n+m}}).\n\\]\n\n\nSelect a value of \\(m\\) and argue that for this choice your 95% posterior credible interval has frequentist coverage 0.95 (that is, if you draw many samples of size \\(n\\) and compute the 95% interval following the formula in a. for each sample, in the long run 95% of the intervals will contain the true value of \\(\\mu\\)).\n\n\nSolution: If we let \\(m\\rightarrow 0\\), our prior variance increases to \\(\\infty\\) and the posterior mean becomes the sample mean, with 95% posterior credibility interval\n\\[\n(\\hat Y -1.96\\frac{\\sigma}{\\sqrt{n}}, \\hat Y + 1.96\\frac{\\sigma}{\\sqrt{n}}).\n\\]\nSince we assume a normal distribution, and the variance \\(\\sigma^2\\) is constant and assumed known, then the above interval is exactly the same as the confidence interval for \\(\\mu\\). Thus, according to our chosen model and using the frequentist way of thinking, the true value of mu will fall inside an interval computed according to this formula in 95% of trials (in the long run) in a repeated sampling setup."
  },
  {
    "objectID": "assignments/hw2/solution/index.html#exercise-2",
    "href": "assignments/hw2/solution/index.html#exercise-2",
    "title": "Homework #2 (Solution)",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe Major League Baseball player Reggie Jackson is known as “Mr October” for his outstanding performances in the World Series (which takes place in October). Over his long career he played in 2820 regular-season games and hit 563 home runs in these games (a player can hit 0, 1, 2, … home runs in a game). He also played in 27 world series games and hit 10 home runs in these games.\nAssuming uninformative conjugate priors, summarize the posterior distribution of his home-run rate in the regular season and World Series. Is there sufficient evidence to claim that he performs better in the World Series?\n\nSolution: We are dealing with numbers of events over fixed periods of time, so the Poisson-gamma model is the logical choice.\nOur Poisson likelihood with rate $$ over a period of time \\(T\\) is\n\\[\nf(Y\\vert \\lambda) = \\frac{(T\\lambda)^Y}{Y!} \\exp(-T\\lambda),\n\\]\nand our Gamma prior is\n\\[\n\\pi(\\lambda \\vert a, b) = \\frac{b^a}{\\Gamma(a)}\\lambda^{a-1}\\exp\\left(-\\lambda b\\right).\n\\]\nChapter 2.1.2 in the book tells us that our posterior will take the form of a Gamma(A, B) distribution where \\(A = Y + a\\) and \\(B = T + b\\). We want to use a non-informative prior, so we choose \\(a=0.01\\) and \\(b=0.01\\).\n\nT1 &lt;- 2820\nY1 &lt;- 563\n\nT2 &lt;- 27\nY2 &lt;- 10\n\na &lt;- 0.01\nb &lt;- 0.01\n\n\nA1 &lt;- Y1 + a\nB1 &lt;- T1 + b\nA2 &lt;- Y2 + a\nB2 &lt;- T2 + b\n\nThe means for the two posterior distributions are\n\nresults &lt;- tibble(\n    Type = c(\"Regular-season\", \"World series\"),\n    A = c(A1, A2),\n    B = c(B1, B2),\n    Mean = A / B,\n    Lower = qgamma(0.025, shape = A, rate = B),\n    Upper = qgamma(0.975, shape = A, rate = B)\n) \n\n# Table\nresults |&gt; \n    gt() |&gt; \n    tab_spanner(columns = c(Lower, Upper), label = \"95% Posterior Interval\")\n\n\n\n\n\n  \n    \n    \n      Type\n      A\n      B\n      Mean\n      \n        95% Posterior Interval\n      \n    \n    \n      Lower\n      Upper\n    \n  \n  \n    Regular-season\n563.01\n2820.01\n0.1996482\n0.1834953\n0.2164728\n    World series\n10.01\n27.01\n0.3706035\n0.1777961\n0.6330224\n  \n  \n  \n\n\n\n\nWe could also sample from the two distributions and use those samples to summarize the difference between the two distributions\n\nX1 &lt;- rgamma(n = 1e5, shape = A1, rate = B1)\nX2 &lt;- rgamma(n = 1e5, shape = A2, rate = B2)\n\np_greater &lt;- mean(X2 &gt; X1)\n\ntibble(\n    Regular = X1,\n    Worldseries = X2,\n    Difference = X2 - X1\n) |&gt; \n    ggplot(aes(x = Difference)) +\n    geom_histogram(\n        alpha = 0.5,\n        color = \"black\"\n    ) +\n    geom_vline(\n        xintercept = 0, \n        lty = 2\n    ) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = expression(theta[2]-theta[1]),\n        y = NULL,\n        title = \"How much better is Reggie Jackson's home run rate during the World Series?\",\n        subtitle = \"The evidence heavily favors the claim that Reggie Jackson does better in the World Series\"\n    )\n\n\n\n\nFinally, we calculate the probability that Reggie Jackson’s performance is better in the World Series than in the regular season (assuming our chosen model and priors).\n\nmean(X2 &gt; X1)\n\n[1] 0.95078"
  },
  {
    "objectID": "assignments/hw2/solution/index.html#exercise-4",
    "href": "assignments/hw2/solution/index.html#exercise-4",
    "title": "Homework #2 (Solution)",
    "section": "Exercise 4",
    "text": "Exercise 4\nAssume that \\(Y\\vert\\theta \\sim \\mathrm{NegBinomial}(\\theta, m)\\) and \\(\\theta\\sim\\mathrm{Beta}(a, b)\\).\n\nDerive the posterior of \\(\\theta\\)\n\n\nSolution: Our Negative Binomial likelihood can be written\n\\[\n\\begin{aligned}\nf(Y\\vert m, \\theta) &= \\binom{Y+m-1}{Y}\\theta^m(1-\\theta)^Y \\\\\n&\\propto \\theta^m(1-\\theta)^Y.\n\\end{aligned}\n\\]\nThe beta prior is written\n\\[\n\\begin{aligned}\n\\pi(\\theta \\vert a, b) \\propto \\theta^{a-1}(1-\\theta)^{b-1}.\n\\end{aligned}\n\\]\nWe see that when we multiply these two together, our posterior will become\n\\[\n\\begin{aligned}\np(\\theta \\vert Y) &\\propto f(Y\\vert\\theta)\\pi(\\theta) \\\\\n&\\propto \\theta^m(1-\\theta)^Y \\cdot \\theta^{a-1}(1 - \\theta)^{b-1} \\\\\n&= \\theta^{m + a - 1}(1 - \\theta)^{Y + b - 1},\n\\end{aligned}\n\\]\nwhich is the kernel of a Beta(m + a, Y + b) distribution.\n\n\nPlot the posterior of \\(\\theta\\) and give its 95% credible interval assuming \\(m = 5\\), \\(Y = 10\\), and \\(a=b=1\\).\n\n\nSolution:\n\n\nCode\nm &lt;- 5\nY &lt;- 10\na &lt;- 1\nb &lt;- 1\n\nA &lt;- m + a\nB &lt;- Y + b\n\nlower &lt;- qbeta(0.025, shape1 = A, shape2 = B)\nupper &lt;- qbeta(0.975, shape1 = A, shape2 = B)\n\nggplot() +\n    stat_function(\n        geom = \"area\",\n        fun = dbeta,\n        args = list(shape1 = A, shape2 = B),\n        alpha = 0.5,\n        xlim = c(lower, upper)\n    ) +\n    stat_function(\n        geom = \"area\",\n        fun = dbeta,\n        args = list(shape1 = A, shape2 = B),\n        alpha = 0.2,\n        xlim = c(0, lower)\n    ) +\n    stat_function(\n        geom = \"area\",\n        fun = dbeta,\n        args = list(shape1 = A, shape2 = B),\n        alpha = 0.2,\n        xlim = c(upper, 1)\n    ) +\n    geom_vline(\n        xintercept = lower,\n        lty = 2\n    ) +\n    geom_vline(\n        xintercept = upper,\n        lty = 2\n    ) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n        x = expression(theta),\n        y = expression(paste(p, \"(\", theta, \"|\", Y, \")\")),\n        title = expression(paste(\"The posterior distribution of \", theta, \" after observing the data Y\")),\n        subtitle = glue(\"The posterior mean is {round(A/(A+B), 3)} and the 95% credible interval is [{round(lower, 3)}, {round(upper, 3)}]\")\n    ) +\n    theme(\n        plot.margin = margin(t = 5, r = 15, b = 5, l = 15)\n    )"
  },
  {
    "objectID": "assignments/hw2/solution/index.html#exercise-5",
    "href": "assignments/hw2/solution/index.html#exercise-5",
    "title": "Homework #2 (Solution)",
    "section": "Exercise 5",
    "text": "Exercise 5\nOver the past 50 years California has experienced an average of \\(\\lambda_0=75\\) large wildfires per year. For the next 10 years you will record the number of large fires in California and then fit a Poisson/gamma model to these data. Let the rate of large fires in this future period, \\(\\lambda\\), have prior \\(\\lambda\\sim\\mathrm{Gamma}(a,b)\\). Select \\(a\\) and \\(b\\) so that the prior is uninformative with prior variance around 100 and gives prior probability approximately \\(\\mathrm{Prob}(\\lambda &gt; \\lambda_0) = 0.5\\) so that the prior places equal probability on both hypotheses in the test for a change in the rate.\n\nSolution:\nThe variance of the Gamma(a, b) distribution is \\(a/b^2\\). To find the correct values for \\(a\\) and \\(b\\) we can create a sequence of values with variance 100 and find which pair is closes to giving us \\(P(\\lambda &gt; \\lambda_0) = 0.5\\).\nWe first use the equation for the variance to find how \\(a\\) and \\(b\\) must relate to each other.\n\\[\n\\begin{aligned}\n\\frac{a}{b^2} &= 100 \\\\\n\\rightarrow \\quad a &= 100b^2 \\\\\n\\rightarrow \\quad b &= \\sqrt{a}/10.\n\\end{aligned}\n\\]\nWe then find a pair that satisfies the second condition by performing the following:\n\nCreate a grid of values for \\(a\\)\nUse the fact that \\(b = \\sqrt a/10\\) to calculate the corresponding values for \\(b\\)\nUse the R function pgamma to calculate \\(P(\\lambda &gt; \\lambda_0)\\)\nFind the pair of values for which this probability is closest to 0.5\n\n\nmy_variance &lt;- 100\n\ntibble(\n    a = seq(0, 100, length.out = 1e4),\n    b = sqrt(a) / 10\n) |&gt; \n    mutate(\n        # Calculate the probability\n        prob_greater = pgamma(75, shape = a, rate = b),\n        # Calculate how close probability is to 0.5\n        prob_diff = abs(prob_greater - 0.5)\n    ) |&gt; \n    arrange(prob_diff) |&gt; \n    slice(1)\n\n# A tibble: 1 × 4\n      a     b prob_greater prob_diff\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1  56.9 0.754        0.500 0.0000446\n\n\nWe see that if we choose \\(a = 57\\) and \\(b = 0.75\\) we get an approximate prior variance of 100 and \\(P(\\lambda &gt; \\lambda_0) \\approx 0.5\\)."
  },
  {
    "objectID": "assignments/hw2/index.html",
    "href": "assignments/hw2/index.html",
    "title": "Homework #2",
    "section": "",
    "text": "Download the Quarto document used to render this file"
  },
  {
    "objectID": "assignments/hw2/index.html#exercise-1",
    "href": "assignments/hw2/index.html#exercise-1",
    "title": "Homework #2",
    "section": "Exercise 1",
    "text": "Exercise 1\nAssume \\(Y_1, \\dots, Y_n \\vert \\mu \\overset{\\mathrm{iid}}{\\sim}\\) where \\(\\sigma^2\\) is fixed and the unknown mean \\(\\mu\\) has prior \\(\\mu\\sim\\mathrm{Normal}(0, \\sigma^2/m)\\)\n\nGive a 95% posterior interval for \\(\\mu\\)\n\n\nSolution:\n\n\nSelect a value of \\(m\\) and argue that for this choice your 95% posterior credible interval ahs frequentist coverage 0.95 (that is, if you draw many samples of size \\(n\\) and compute the 95% interval following the formula in a. for each sample, in the long run 95% of the intervals will contain the true value of \\(\\mu\\)).\n\n\nSolution:"
  },
  {
    "objectID": "assignments/hw2/index.html#exercise-2",
    "href": "assignments/hw2/index.html#exercise-2",
    "title": "Homework #2",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe Major League Baseball player Reggie Jackson is known as “Mr October” for his outstanding performances in the World Series (which takes place in October). Over his long career he played in 2820 regular-season games and hit 563 home runs in these games (a player can hit 0, 1, 2, … home runs in a game). He also played in 27 world series games and hit 10 home runs in these games.\nAssuming uninformative conjugate priors, summarize the posterior distribution of his home-run rate in the regular season and World Series. Is there sufficient evidence to claim that he performs better in the World Series?\n\nSolution:"
  },
  {
    "objectID": "assignments/hw2/index.html#exercise-4",
    "href": "assignments/hw2/index.html#exercise-4",
    "title": "Homework #2",
    "section": "Exercise 4",
    "text": "Exercise 4\nAssume that \\(Y\\vert\\theta \\sim \\mathrm{NegBinomial}(\\theta, m)\\) and \\(\\theta\\sim\\mathrm{Beta}(a, b)\\).\n\nDerive the posterior of \\(\\theta\\)\n\n\nSolution:\n\n\nPlot the posterior of \\(\\theta\\) and give its 95% credible interval assuming \\(m = 5\\), \\(Y = 10\\), and \\(a=b=1\\).\n\n\nSolution:"
  },
  {
    "objectID": "assignments/hw2/index.html#exercise-5",
    "href": "assignments/hw2/index.html#exercise-5",
    "title": "Homework #2",
    "section": "Exercise 5",
    "text": "Exercise 5\nOver the past 50 years California ahs experienced an average of \\(\\lambda_0=75\\) large wildfires per year. For the next 10 years you will record the number of large fires in California and then fit a Poisson/gamma model to these data. Let the rate of large fires in this future period, \\(\\lambda\\), have prior \\(\\lambda\\sim\\mathrm{Gamma}(a,b)\\). Select \\(a\\) and \\(b\\) so that the prior is uninformative with prior variance around 100 and gives prior probability approximately \\(\\mathrm{Prob}(\\lambda &gt; \\lambda_0) = 0.5\\) so that the prior places equal probability on both hypotheses in the test for a change in the rate.\n\nSolution:"
  },
  {
    "objectID": "assignments/hw5/index.html",
    "href": "assignments/hw5/index.html",
    "title": "Homework #5",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n1\nA clinical trial gave six subjects a placebo and six subjects a new weight loss medication. The response variable is the change in weight (pounds) from baseline (so -2.0 means the subject lost 2 pounds). The data for the 12 subjects are:\n\n\n\n\n\n\n  \n    \n    \n      Placebo\n      Treatment\n    \n  \n  \n    2.0\n-3.5\n    -3.1\n-1.6\n    -1.0\n-4.6\n    0.2\n-0.9\n    0.3\n-5.1\n    0.4\n0.1\n  \n  \n  \n\n\n\n\nConduct a Bayesian analysis to compare the means of these two groups. Would you say the treatment is effective? Is your conclusion sensitive to the prior?\n\nSolution:\n\n\n\n2\nLoad the classic Boston Housing Data in R:\n\ndata(Boston, package = \"MASS\")\n\nThe response variable is medv, the median value of owner-occupied homes (in $1,000s), and the other 13 variables are covariates that describe the neighborhood.\n\nFit a Bayesian linear regression model with uninformative Gaussian priors for the regression coefficients. Verify the MCMC sampler has converged, and summarize the posterior distribution of all regression coefficients.\n\n\nSolution:\n\n\nPerform a classic least squares (e.g. using the lm function in R). Compare the results numerically and conceptually with the Bayesian results.\n\n\nSolution:\n\n\nRefit the Bayesian model with double exponential priors for the regression coefficients, and discuss how the results differ from the analysis with uninformative priors.\n\n\nSolution:\n\n\nFit a Bayesian linear regression model in a. using only the first 500 observations and compute the posterior predictive distribution for the final 6 observations. Plot the posterior predictive distribution versus the actual value for these 6 observations and comment on whether the predictions are reasonable.\n\n\nSolution:"
  },
  {
    "objectID": "assignments/hw4/index.html",
    "href": "assignments/hw4/index.html",
    "title": "Homework #4",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n2\nAssume that \\(Y_i\\vert\\mu\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(\\mu,\\sigma_i^2)\\) for \\(i \\in \\{1, \\dots, n\\}\\), with \\(\\sigma_i\\) known and improper prior distribution \\(\\pi(\\mu)=1\\) for all \\(\\mu\\).\n\nGive a formula for the MAP estimator for \\(\\mu\\)\n\n\nSolution:\n\n\nWe observe \\(n=3, Y_1=12, Y_2=10, Y_3=22, \\sigma_1=\\sigma_2=3\\) and \\(\\sigma_3=10\\), compute the MAP estimate of \\(\\mu\\).\n\n\nSolution:\n\n\nUse numerical integration to compute the posterior mean of \\(\\mu\\).\n\n\nSolution:\n\n\nPlot the posterior distribution of \\(\\mu\\) and indicate the MAP and the posterior mean estimates on the plot.\n\n\nSolution:\n\n\n\n4\nConsider the model\n\\[\nY_i\\vert\\sigma^2_i\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(\\mu,\\sigma_i^2),\n\\]\nfor \\(i \\in \\{1, \\dots, n\\}\\) where\n\\[\n\\sigma_i^2\\vert b \\sim \\mathrm{InvGamma}(a,b) \\\\ b\\sim\\mathrm{Gamma}(1,1)\n\\]\n\nDerive the full conditional posterior distributions for \\(\\sigma^2_1\\) and \\(b\\).\n\n\nSolution:\n\n\nWrite pseudocode for Gibbs sampling, i.e. describe in detail each step of the Gibbs sampling algorithm.\n\n\nSolution:\n\n\nWrite your own Gibbs sampling code (not in JAGS) and plot the marginal posterior density of each parameter. Assume \\(n=10\\), \\(a=10\\) and \\(Y_i=i\\) for \\(i=1, \\dots, 10\\).\n\n\nSolution:\n\n\nRepeat the analysis with \\(a=1\\) and comment on the convergence of the MCMC chain.\n\n\nSolution:\n\n\nImplement the model in c. using JAGS and compare the results with the results in c.\n\n\nSolution:\n\n\n\n5\nConsider the model\n\\[\nY_i\\vert\\mu, \\sigma^2 \\sim \\mathrm{Normal}(\\mu,\\sigma^2),\\quad i=1,\\dots,n,\n\\]\nand\n\\[\nY_i\\vert\\mu,\\delta, \\sigma^2 \\sim \\mathrm{Normal}(\\mu+\\delta,\\sigma^2),\\quad i=n+1,\\dots,n+m,\n\\]\nwhere\n\\[\n\\mu,\\delta\\sim\\mathrm{Normal}(0, 100^2) \\\\\n\\sigma^2\\sim\\mathrm{InvGamma}(0.01, 0.01).\n\\]\n\nGive an example of a real experiment for which this would be an appropriate model.\n\n\nSolution:\n\n\nDerive the full conditional posterior distributions for \\(\\mu\\), \\(\\delta\\), and \\(\\sigma^2\\).\n\n\nSolution:\n\n\nSimulate a dataset from this model with \\(n=m=50\\), \\(\\mu=10\\), \\(\\delta=1\\) and \\(\\sigma=2\\). Write your own Gibbs sampling code (not in JAGS) to fit the model above to the simulated data and plot the marginal posterior density for each parameter. Are you able to recover the true values reasonably well?\n\n\nSolution:\n\n\nImplement this model using JAGS and compare the results with the results in .c.\n\n\nSolution:\n\n\n\n10\nAs discussed in section 1.6, (Edie, Smits, and Jablonski 2017) report that the number of marine bivalve species discovered each year from 2010-2015 was 64, 13, 33, 18, 30 and 20. Denote \\(Y_t\\) as the number of species discovered in year \\(2009+t\\) (so that \\(Y_1=64\\) is the count for 2010). Use JAGS to fit the model\n\nEdie, Stewart M., Peter D. Smits, and David Jablonski. 2017. “Probabilistic Models of Species Discovery and Biodiversity Comparisons.” Proceedings of the National Academy of Sciences 114 (14): 3666–71. https://doi.org/10.1073/pnas.1616355114.\n\\[\n\\begin{gathered}\nY_t\\vert \\alpha, \\beta \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Poisson}(\\lambda_t)\\\\\n\\lambda_t = \\exp(\\alpha + \\beta t) \\text{ or equivalently } \\log(\\lambda_t) = \\alpha + \\beta t \\\\\n\\alpha,\\beta \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Normal}(0,10^2).\n\\end{gathered}\n\\]\nSummarize the posterior of \\(\\alpha\\) and \\(\\beta\\) and verify that the MCMC sampler has converged. Does this analysis provide evidence that the rate of discovery is changing over time?"
  },
  {
    "objectID": "assignments/hw3/index.html",
    "href": "assignments/hw3/index.html",
    "title": "Homework #3",
    "section": "",
    "text": "Download the Quarto document used to render this file"
  },
  {
    "objectID": "assignments/hw3/index.html#exercise-6",
    "href": "assignments/hw3/index.html#exercise-6",
    "title": "Homework #3",
    "section": "Exercise 6",
    "text": "Exercise 6\nAn assembly line relies on accurate measurements from an image-recognition algorithm at the first stage of the process. It is known that the algorithm is unbiased, so assume that measurements follow a normal distribution with mean zero,\n\\[\nY_i\\vert \\sigma^2\\overset{\\mathrm{iid}}{\\sim}\\mathrm{Normal}(0, \\sigma^2).\n\\]\nSome errors are permissible, but if \\(\\sigma\\) exceeds the threshold \\(c\\) then the algorithm must be replaced.\nYou make \\(n = 20\\) measurements and observe\n\\[\n\\sum_{i=1}^n Y_i = -2 \\quad \\mathrm{and} \\quad \\sum_{i=1}^n Y_i^2 = 15,\n\\]\nand conduct a Bayesian analysis with \\(\\mathrm{InvGamma}(a,b)\\) prior. compute the posterior probability that \\(\\sigma &gt; c\\) for:\n\n\\(c=1\\) and \\(a=b=0.1\\)\n\n\nSolution:\n\n\n\\(c=1\\) and \\(a=b=1.0\\)\n\n\nSolution:\n\n\n\\(c=2\\) and \\(a=b=0.1\\)\n\n\nSolution:\n\n\n\\(c=2\\) and \\(a=b=1.0\\)\n\n\nSolution:"
  },
  {
    "objectID": "assignments/hw3/index.html#exercise-16",
    "href": "assignments/hw3/index.html#exercise-16",
    "title": "Homework #3",
    "section": "Exercise 16",
    "text": "Exercise 16\nSay \\(Y\\vert\\lambda \\sim \\mathrm{Gamma}(1, \\lambda).\\)\n\nDerive and plot the Jeffreys’ prior for \\(\\lambda\\)\n\n\nSolution:\n\n\nIs this prior proper?\n\n\nSolution: This is not a proper prior since it does not integrate to one.\n\n\nDerive the posterior and give conditions on \\(Y\\) to ensure it is proper.\n\n\nSolution:"
  },
  {
    "objectID": "assignments/hw3/index.html#exercise-18",
    "href": "assignments/hw3/index.html#exercise-18",
    "title": "Homework #3",
    "section": "Exercise 18",
    "text": "Exercise 18\nThe data in the table below are the result of a survey of commuters in 10 counties likely to be affected by a proposed addition of a high occupancy vehicle (HOV) lane.\n\ntibble::tribble(\n    ~County, ~Approve, ~Disapprove,\n    1, 12, 50,\n    2, 90, 150,\n    3, 80, 63,\n    4, 5, 10,\n    5, 63, 63,\n    6, 15, 8,\n    7, 67, 56,\n    8, 22, 19,\n    9, 56, 63,\n    10, 33, 19\n)\n\n# A tibble: 10 × 3\n   County Approve Disapprove\n    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1      1      12         50\n 2      2      90        150\n 3      3      80         63\n 4      4       5         10\n 5      5      63         63\n 6      6      15          8\n 7      7      67         56\n 8      8      22         19\n 9      9      56         63\n10     10      33         19\n\n\n\nAnalyze the data in each county sepparately using the Jeffreys’ prior distribution and report the posterior 95% credible set for each county.\n\n\nSolution:\n\n\nLet \\(\\hat p_i\\) be the sample proportion of commuters in county \\(i\\) that approve of the HIV lane (e.g. \\(\\hat p_1 = 12/(12+50)=0.194\\)). Select \\(a\\) abd \\(b\\) so that the mean and variance of the \\(\\mathrm{Beta}(a,b)\\) distribution match the mean and variance of the sample proportions \\(\\hat p_1, \\dots, \\hat p_{10}\\).\n\n\nSolution:\n\n\nConduct an empirical Bayesian analysis by computing the 95% posterior credible sets that results from analyzing each coutny separately using the \\(\\mathrm{Beta}(a,b)\\) prior you computed in b.\n\n\nSolution:\n\n\nHow do the results from a. and c. differ? What are the advantages and disadvantages of these two analyses?\n\n\nSolution:"
  },
  {
    "objectID": "assignments/hw1/index.html",
    "href": "assignments/hw1/index.html",
    "title": "Homework #1",
    "section": "",
    "text": "Download the Quarto document used to render this file \n\n1. Sample survey\nSuppose we are going to sample 100 individuals from a county (with population size much larger than 100) and ask each sampled person whether they support policy Z or not. Let \\(Y_i = 1\\) if person \\(i\\) in the sample supports the policy, and \\(Y_i = 0\\) otherwise.\n\nAssume \\(Y_1, \\dots, Y_{100}\\) are, conditoinal on \\(\\theta\\), i.i.d. binary random variables with expectation \\(\\theta\\). Write down the joint distribution of \\(\\mathrm{Pr}(Y_1 = y_1, \\dots, Y_{100} = y_{100})\\) in a compact form. Also write down the form of \\(\\mathrm{Pr}(\\sum_{i_1}^{100}Y_i=y)\\).\n\nSolution:\n\n\nFor the moment, suppose you believed that \\(\\theta \\in \\{0, 0.1, \\dots, 0.9, 1.0\\}\\). Given that the results of the survey were \\(\\sum_{i=1}^{100}Y_i=73\\), compute \\(\\mathrm{Pr}(\\sum_{i=1}^{100}Y_i=73\\vert\\theta)\\) for each of these \\(11\\) values of \\(\\theta\\) and plot these probabilities as a function of \\(\\theta\\) (point mass at each value of \\(\\theta\\)).\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nNow suppose you originally had no prior information to believe one of these \\(\\theta\\)-values over another, and thus \\(\\mathrm{Pr}(\\theta=0.0) = \\mathrm{Pr}(\\theta=0.1) = \\dots = \\mathrm{Pr}(\\theta=0.9) = \\mathrm{Pr}(\\theta = 1.0) = \\frac{1}{11}\\). Use Bayes’ rule to compute \\(p(\\theta\\vert \\sum_{i=1}^{100}Y_i=73)\\) for each \\(\\theta\\)-value. Make a plot of this posterior distribution as a function of \\(\\theta\\) (point mass at each value of \\(\\theta\\)).\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nNow suppose you allow \\(\\theta\\) to be any value in the interval \\([0, 1]\\). Using the uniform prior density for \\(\\theta\\), namely, \\(p(\\theta) = 1\\), derive and plot the posterior density of \\(\\theta\\) as a function of \\(\\theta\\). According to the posterior density, what is the probability of \\(\\theta &gt; 0.8\\)?\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nWhy are the heights of posterior densities in c. and d. not the same?\n\nSolution:\n\n\n\n2. Random numbers, probability density functions (pdf) and cumulative density functions (cdf)\nThe goal of this exercise is to generate random numbers, plot the histogram, the empirical pdf and cdf for these numbers, and see how they compare to the theoretical pdf and cdf. The goal is also to compare the sample mean and standard deviation to the theoretical mean and standard deviation.\n\nGenerate \\(B = 3000\\) numbers fro the gamma distribution with parameters \\(\\alpha = 2\\) and \\(\\beta = 0.1\\). Compute the sample mean and the sample standard deviation and compare to the theoretical mean and standard deviation.\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nPlot the theoretical density (pdf) of the gamma distribution. Plot the empirical density based on the data on the same graph. Plot the histogram of the data on another graph.\n\nSolution:\n\n```{r}\n# Include your code here\n```\n\n\n\nPlot the theoretical cumulative density function (cdf) of the gamma distribution. Plot the empirical cumulative density based on the data on the same graph.\n\nSolution:\n\n```{r}\n# Include your code here\n```"
  }
]