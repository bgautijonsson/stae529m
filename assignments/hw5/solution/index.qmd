---
title: "Homework #5"
subtitle: "Exercises from Chapter 4.6: 1 and 2"
description: |
    **Assigned:** Friday, September 29th, 2023 <br>
    **Due:** Friday, October 6th, 2023
author: 
    -   name: Brynjólfur Gauti Guðrúnar Jónsson
        email: brynjolfur@hi.is
        url: https://www.hi.is/staff/brynjolfur
        affiliation: 
        -   name: School of Engineering and Natural Sciences, University of Iceland
            city: Reykjavík
            url: https://english.hi.is/school_of_engineering_and_natural_sciences
# Change this to pdf if you'd rather work in that format
format: html 
editor: source
title-block-banner: true
#### The setting below makes our HTML document self-contained so that we can turn it in ####
#### UNCOMMENT THE LONE BELOW SO THAT YOU CAN TURN IN YOUR HTML DOCUMENT ###################
# embed-resources: true 
---

{{< downloadthis index.qmd dname="hw5" label="Download the Quarto document used to render this file" type=primary class=downloadbutton >}}

```{r}
#| echo: fenced
#| output: hide

library(tidyr)
library(dplyr, warn.conflicts = FALSE)
library(stringr)
library(forcats)
library(ggplot2)
library(readr)
library(patchwork)
library(gt)
library(cubature)
library(purrr)
library(scales)
library(glue)
library(rjags)
library(posterior)
library(bayesplot)
theme_set(theme_classic())
```

# 1

A clinical trial gave six subjects a placebo and six subjects a new weight loss medication. The response variable is the change in weight *(pounds)* from baseline *(so -2.0 means the subject lost 2 pounds)*. The data for the 12 subjects are:

```{r}
d <- tribble(
    ~Placebo, ~Treatment,
    2.0, -3.5,
    -3.1, -1.6,
    -1.0, -4.6,
    0.2, -0.9,
    0.3, -5.1,
    0.4, 0.1
)

d |> 
    gt::gt()
```

Conduct a Bayesian analysis to compare the means of these two groups. Would you say the treatment is effective? Is your conclusion sensitive to the prior?

---

**Solution:** We can use the model we defined in last week's homework to analyse these data:

$$
Y_i\vert\mu, \sigma^2 \sim \mathrm{Normal}(\mu,\sigma^2),\quad i=1,\dots,n,
$$

and

$$
Y_i\vert\mu,\delta, \sigma^2 \sim \mathrm{Normal}(\mu+\delta,\sigma^2),\quad i=n+1,\dots,n+m,
$$

where

$$
\begin{gathered}
\mu,\delta\sim\mathrm{Normal}(0, 100^2) \\
\sigma^2\sim\mathrm{InvGamma}(0.01, 0.01).
\end{gathered}
$$

```{r}
data = list(
    Y1 = d$Placebo,
    Y2 = d$Treatment,
    n = nrow(d),
    m = nrow(d)
)
model_string <- textConnection("model{
    # Likelihood
    for (i in 1:n) {
        Y1[i] ~ dnorm(mu, tau)
    }
    
    for (i in 1:m) {
        Y2[i] ~ dnorm(mu + delta, tau)
    }
    # Priors
    mu ~ dnorm(0, 0.01^2)
    delta ~ dnorm(0, 0.01^2)
    tau ~ dgamma(0.01, 0.01)
    sigma2 <- 1 / tau
}")

inits <- list(
    mu = mean(d$Placebo),
    delta = mean(d$Treatment) - mean(d$Placebo),
    tau = 1 / var(d$Placebo)
)

model <- jags.model(
    model_string,
    data = data,
    inits = inits,
    n.chains = 4
)

update(model, 10000)

params <- names(inits)
samples <- coda.samples(
    model,
    variable.names = params,
    n.iter = 10000
)

posterior_jags <- samples |> 
    as_draws_df()
```


```{r}
posterior_jags |> 
    summarise_draws() |> 
    gt() |> 
    fmt_number(
        columns = mean:rhat,
        decimals = 3
    ) |> 
    fmt_number(
        columns = starts_with("ess"),
        decimals = 0
    )
```


```{r}
#| fig-asp: 0.5
posterior_jags |> 
    mcmc_areas(
        pars = "delta"
    ) +
    labs(
        title = expression(paste("Posterior distribution of ", delta))
    )
```

```{r}
p_lower <- posterior_jags |> 
    subset_draws(variable = "delta") |> 
    summarise_draws("P(delta < 0)" = function(x) mean(x < 0))

p_lower |> 
    gt() |> 
    fmt_percent(
        columns = 2
    )
```


```{r}
data = list(
    Y1 = d$Placebo,
    Y2 = d$Treatment,
    n = nrow(d),
    m = nrow(d)
)
model_string <- textConnection("model{
    # Likelihood
    for (i in 1:n) {
        Y1[i] ~ dnorm(mu, tau[1])
    }
    
    for (i in 1:m) {
        Y2[i] ~ dnorm(mu + delta, tau[2])
    }
    # Priors
    mu ~ dnorm(0, 0.01^2)
    delta ~ dnorm(0, 0.01^2)
    for (i in 1:2) {
        tau[i] ~ dgamma(0.01, 0.01)
        sigma2[i] <- 1 / tau[i]
    }
    
}")

inits <- list(
    mu = mean(d$Placebo),
    delta = mean(d$Treatment) - mean(d$Placebo),
    tau = rep(1 / var(d$Placebo), 2)
)

model <- jags.model(
    model_string,
    data = data,
    inits = inits,
    n.chains = 4
)

update(model, 10000)

params <- names(inits)
samples <- coda.samples(
    model,
    variable.names = params,
    n.iter = 10000
)

posterior_jags <- samples |> 
    as_draws_df()
```


```{r}
posterior_jags |> 
    summarise_draws() |> 
    gt() |> 
    fmt_number(
        columns = mean:rhat,
        decimals = 3
    ) |> 
    fmt_number(
        columns = starts_with("ess"),
        decimals = 0
    )
```


```{r}
#| fig-asp: 0.5
posterior_jags |> 
    mcmc_areas(
        pars = "delta"
    ) +
    labs(
        title = expression(paste("Posterior distribution of ", delta))
    )
```

```{r}
p_lower <- posterior_jags |> 
    subset_draws(variable = "delta") |> 
    summarise_draws("P(delta < 0)" = function(x) mean(x < 0))

p_lower |> 
    gt() |> 
    fmt_percent(
        columns = 2
    )
```


---

# 2

Load the classic Boston Housing Data in R:

```{r}
data(Boston, package = "MASS")
```

The response variable is `medv`, the median value of owner-occupied homes *(in $1,000s)*, and the other 13 variables are covariates that describe the neighborhood.


a. Fit a Bayesian linear regression model with uninformative Gaussian priors for the regression coefficients. Verify the MCMC sampler has converged, and summarize the posterior distribution of all regression coefficients.

---

**Solution:** We use the Jeffreys prior

$$
 gamma
$$

$$
p(\beta, \sigma^2) \propto \sigma^{-2},
$$

which gives us the posterior distributions

$$
\begin{aligned}
\beta \vert \sigma^2, \mathbf Y &\sim \mathrm{Normal}(\hat \beta, V_\beta \sigma^2) \\
\sigma^2 \vert \mathbf Y &\sim \mathrm{}
\end{aligned}
$$

$$
\hat \beta = \left(X^T X\right)^{-1}X^T y \quad \text{and} \quad V_\beta = \left(X^TX\right)^{-1}.
$$

This will give us a proper posterior distribution, since we have many more data points than we have parameters in our model.

```{r}
Y <- Boston$medv
X <- Boston |> select(-medv) |> as.matrix()

beta_mean <- solve(t(X) %*% X) %*% t(X) %*% Y


```

---

b. Perform a classic least squares *(e.g. using the `lm` function in `R`)*. Compare the results numerically and conceptually with the Bayesian results.



---

**Solution:**


---


c. Refit the Bayesian model with double exponential priors for the regression coefficients, and discuss how the results differ from the analysis with uninformative priors.

---

**Solution:**

---

d. Fit a Bayesian linear regression model in *a.* using only the first 500 observations and compute the posterior predictive distribution for the final 6 observations. Plot the posterior predictive distribution versus the actual value for these 6 observations and comment on whether the predictions are reasonable.

---

**Solution:**

---
