---
title: "Homework #8 (Solution)"
subtitle: "Exercise from Chapter 6.5: 4"
description: |
    **Assigned:** Friday, November 3rd, 2023 <br>
    **Due:** Friday, November 10th, 2023
author: 
    -   name: Brynjólfur Gauti Guðrúnar Jónsson
        email: brynjolfur@hi.is
        url: https://www.hi.is/staff/brynjolfur
        affiliation: 
        -   name: School of Engineering and Natural Sciences, University of Iceland
            city: Reykjavík
            url: https://english.hi.is/school_of_engineering_and_natural_sciences
# Change this to pdf if you'd rather work in that format
format: html 
editor: source
title-block-banner: true
#### The setting below makes our HTML document self-contained so that we can turn it in ####
#### UNCOMMENT THE LONE BELOW SO THAT YOU CAN TURN IN YOUR HTML DOCUMENT ###################
# embed-resources: true 
---

{{< downloadthis index.qmd dname="hw8_solution" label="Download the Quarto document used to render this file" type=primary class=downloadbutton >}}

```{r}
#| echo: fenced
#| output: hide
library(tidyverse)
library(ggplot2)
library(rjags)
library(posterior)
library(bayesplot)
library(gt)
theme_set(theme_classic())
```


# 4

Download the marathon data of Section 6.4 from the course webpage. Let $Y_{ij}$ be the speed of runner $i$ in mile $j$. Fit the hierarchical model

$$
\begin{aligned}
Y_{i1} &\sim \mathrm{Normal}(\mu_i, \sigma_0^2) \\
Y_{ij} \vert Y_{ij-1} &\sim \mathrm{Normal}\left(\mu_i + \rho_i(Y_{ij-1} - \mu_i), \sigma_i^2\right) \\
\mu_i &\sim \mathrm{Normal}(\theta_1, \theta_2) \\
\rho_i &\sim \mathrm{Normal}(\theta_3, \theta_4) \\
\sigma_i^2 &\sim \mathrm{InvGamma}(\theta_5, \theta_6)
\end{aligned}
$$

```{r}
load(url("https://www4.stat.ncsu.edu/~bjreich/BSMdata/BostonMarathon2016.RData"))
```



a. Draw a DAG for this model and give an interpretation for each parameter in the model.

---

**Solution:**

The dag is plotted below. The variables can be interpreted as follows

* $\mu_i$: This is the mean running speed of runner $i$.
* $\sigma^2_i$: This is how varied the speed of runner $i$ is around their mean, $\mu_i$.
* $\rho_i$: This parameter governs how a runner's speed in mile $j$ depends on their speed during mile $j - 1$.
  - $\rho_i > 1$: The difference between the runner's current speed and average speed increases with each mile in the same direction, i.e. if a runner is running above their average, then we expect them to keep accelerating until they reach goal.
  - $0 < \rho_i < 1$: The difference $Y_{ij} - \mu_i$ is then expected to be smaller than the previous mile, $Y_{ij-1} - \mu_i$, but in the same direction. I.e. if the runnar ran faster than $\mu_i$ during mile $j - 1$ we still expect them to run faster during mile $j$ but less so.
  - $-1 < \rho_i < 0$: The difference $Y_{ij} - \mu_i$ is then expected to be smaller than the previous mile, $Y_{ij-1} - \mu_i$, in absolute value, but also in the opposite direction. So, if a runner ran slower than $\mu_i$ during mile $j - 1$, we expect them to run faster than $\mu_i$ during mile $j$ and vice versa.
  - $\rho_i < -1$: The runner is always running at a speed that is further and further away from $\mu_i$, but it is swinging between slower and faster speeds.

```{dot}
digraph boxes_and_circles {

# a 'graph' statement
graph [overlap = true, fontsize = 10]

theta1[label =<θ<sub>1</sub>>];
theta2[label =<θ<sub>2</sub>>]; 
theta3[label =<θ<sub>3</sub>>]; 
theta4[label =<θ<sub>4</sub>>]; 
theta5[label =<θ<sub>5</sub>>]; 
theta6[label =<θ<sub>6</sub>>]; 
mu[label = <μ<sub>i</sub>>];
rho[label = <ρ<sub>i</sub>>];
sigma1[label = <σ<sub>0</sub>>];
sigmai[label = <σ<sub>i</sub>>];

Yi1[label = <Y<sub>i1</sub>>]
Yij[label = <Y<sub>ij</sub>>]

# edge statements
theta1->mu theta2->mu
theta3->rho theta4-> rho
theta5->sigmai theta6->sigmai

mu->Yi1 sigma1->Yi1

mu->Yij sigmai->Yij rho->Yij Yij->Yij Yi1->Yij
}
```

The parameters above are split into a data layer, a process layer and a prior layer as follows:

* Data layer: $Y_{ij}$, $i = 1, \dots, n$, $j = 1, \dots 26$.
* Process layer: $\mu_i$, $\sigma^2_i$, $\rho_i$, $i = 1, \dots, n$
* Prior layer: $\sigma^2_0$, $\theta_1, \dots, \theta_6$

---

b. Select uninformative prior distributions for $\theta_1, \dots, \theta_6$.

---

**Solution:**

Since $\theta_1$ and $\theta_3$ are both parameters for the mean of normal distributions, it makes sense to give them normal priors. To make them uninformative we can give them large variances, such as for example

$$
\theta_1, \theta_3 \sim \mathrm{Normal}(0, 10^4).
$$

$\theta_2$ and $\theta_4$ are both variances in normal distributions, so we could give them Inverse Gamma priors. To make them uninformative we make the prior variance high, for example

$$
\theta_2, \theta_4 \sim \mathrm{InvGamma}(0.001, 0.001)
$$

$\theta_5$ and $\theta_6$ are the shape and scale parameters in an Inverse Gamma distribution. We know that they are both positive and unbounded, so we could for example give them both Gamma priors with high variance, for example

$$
\theta_5, \theta_6 \sim \mathrm{Gamma}(0.001, 0.001)
$$

---

c. Fit the model in `JAGS` using three chains each with 25,000 iterations and thoroughly assess MCMC convergence for the $\theta_j$.

---

**Solution:**

First we prepare the data. 

```{r}
Y <- rbind(Marathon_male$SPEED_mile, Marathon_female$SPEED_mile)

jags_data <- list(
    N_obs = ncol(Y),
    N_runners = nrow(Y),
    Y = Y
)
```

Then we code the model and sample from its posterior.

```{r}
#| cache: true

model_string <- textConnection("model{
    
    for (i in 1:N_runners) {
        mu[i] ~ dnorm(theta[1], inv_theta2)
        rho[i] ~ dnorm(theta[3], inv_theta4)
        tau[i] ~ dgamma(theta[5], theta[6])
        sigma[i] <- pow(tau[i], -1)
        Y[i, 1] ~ dnorm(mu[i], tau0)
    
        for (j in 2:N_obs) {
            Y[i, j] ~ dnorm(mu[i] + rho[i] * (Y[i, j - 1] - mu[i]), tau[i])
        }
    }
    
    tau0 ~ dgamma(0.001, 0.001)
    sigma0 <- pow(tau0, -1)
    
    theta[1] ~ dnorm(0, 0.0001)
    theta[3] ~ dnorm(0, 0.0001)
    
    inv_theta2 ~ dgamma(0.001, 0.001)
    theta[2] <- pow(inv_theta2, -1)
    inv_theta4 ~ dgamma(0.001, 0.001)
    theta[4] <- pow(inv_theta4, -1)
    
    theta[5] ~ dgamma(0.001, 0.001)
    theta[6] ~ dgamma(0.001, 0.001)
    
    
}")

model <- jags.model(
    model_string,
    jags_data,
    n.chains = 3
)

update(model, 10000)

samples <- coda.samples(
  model,
  variable.names = c("mu", "sigma", "tau", "rho", "theta", "sigma0", "Y[5,11]", "Y[5,12]", "Y[5,13]"),
  n.iter = 25000
)

posterior_jags <- samples |> 
  as_draws_df()
```

The table and trace plots below show us that the chains seem to have converged. 

```{r}
posterior_jags |> 
    subset_draws(variable = "theta") |> 
    summarise_draws(rhat, ess_bulk, ess_tail) |> 
    gt() |> 
    fmt_number()
```

```{r}
posterior_jags |> 
    mcmc_trace(
        regex_pars = "theta"
    )
```





---

d. Are the data informative about the $\theta_j$? That is, are the posterior distributions more concentrated than the prior distributions?

---

**Solution:**

To check whether the data are informative, we compare the prior variance of each $\theta_j$ to its posterior variance. If the posterior variance is much smaller than the prior variance we can conclude that the data are informative for the parameter.

As we see in the table below, this is the case for each of $\theta_1, \dots, \theta_6$

```{r}
sds <- posterior_jags |> 
    subset_draws(variable = "theta") |> 
    summarise_draws() |> 
    pull(sd) |> 
    as.numeric()
    
tribble(
    ~theta, ~prior_variance,
    "theta[1]", 100,
    "theta[2]", Inf,
    "theta[3]", 100,
    "theta[4]", Inf,
    "theta[5]", 1000,
    "theta[6]", 1000
) |> 
    mutate(
        posterior_variance = sds^2
    ) |> 
  gt()
```


---

e. In light of *c.* and *d.*, are there any simplifications you might consider and if so, how would you compare the full and simplified models?

In addition to the results from *c.* and *d.*, we can also look at the marginal densities of each $\theta_j$. In the figure below, we see that $\theta_4$, the variance of the normal distribution from which we draw $\rho_i$ is very small in comparison to $\theta_3$ the mean of the normal distribution.

Our results from *c.* tell us that this parameter is well estimated.

The results in *d.* tell us that our priors aren't artificially making this posterior density narrow.

```{r}
posterior_jags |> 
    mcmc_dens(
        regex_pars = "theta"
    )
```

We could thus consider simplifying our model by using one $\rho$ parameter that is shared by all the participants. Looking at the figure of the posterior density of each $\rho_i$, it seems like this might work for most participants, but there are some $\rho_i$ that seem to be different from the rest.

```{r}
#| cache: true

plot_dat <- posterior_jags |> 
  subset_draws("rho") |> 
  summarise_draws()
```

```{r}
#| fig-asp: 1.5

plot_dat |> 
  mutate(
    id = parse_number(variable)
  ) |> 
  ggplot(aes(id, mean, ymin = q5, ymax = q95)) +
  geom_pointrange(
    linewidth = 0.5,
    size = 0.5
  ) +
  coord_flip() +
  labs(
    x = "Individual",
    y = expression(rho[i]),
    title = expression(paste("Individual-specific ", rho[i], " estimates")),
    subtitle = "Posterior mean and 95% credible intervals"
  )
```

