{
  "hash": "41914a053221450b844e1c63f40520da",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Homework #1 (Solution)\"\nsubtitle: Chapters 1.1, 1.2 and 1.3\nabstract: |\n    **Assigned:** Friday, August 25th, 2023 <br>\n    **Due:** Friday, September 1st, 2023\nauthor: \n    -   name: Brynjólfur Gauti Guðrúnar Jónsson\n        email: brynjolfur@hi.is\n        url: https://www.hi.is/staff/brynjolfur\n        affiliation: \n        -   name: School of Engineering and Natural Sciences, University of Iceland\n            city: Reykjavík\n            url: https://english.hi.is/school_of_engineering_and_natural_sciences\n# Change this to pdf if you'd rather work in that format\nformat: html \neditor: source\ntitle-block-banner: true\n#### The setting below makes our HTML document self-contained so that we can turn it in ####\n#### UNCOMMENT THE LONE BELOW SO THAT YOU CAN TURN IN YOUR HTML DOCUMENT ###################\n# embed-resources: true \n---\n\n{{< downloadthis index.qmd dname=\"hw1_solution\" label=\"Download the Quarto document used to render this file\" type=primary class=downloadbutton >}}\n\n\n\n# 1. Sample survey\n\nSuppose we are going to sample 100 individuals from a county (with population size much larger than 100) and ask each sampled person whether they support policy Z or not. Let $Y_i = 1$ if person $i$ in the sample supports the policy, and $Y_i = 0$ otherwise.\n\na.  Assume $Y_1, \\dots, Y_{100}$ are, conditoinal on $\\theta$, i.i.d. binary random variables with expectation $\\theta$. Write down the joint distribution of $\\mathrm{Pr}(Y_1 = y_1, \\dots, Y_{100} = y_{100})$ in a compact form. Also write down the form of $\\mathrm{Pr}(\\sum_{i_1}^{100}Y_i=y)$.\n\n-----\n\n**Solution:** First, we write down the distribution of a single observation, $Y_i$:\n\n$$\nP(Y_i = 1\\vert \\theta) = \\theta \\quad \\mathrm{and} \\quad P(Y_i = 0|\\theta) = (1 - \\theta).\n$$\n\nWe can write these two cases together as follows\n\n$$\nPr(Y_i = y_i) = \\theta^{y_i}(1 - \\theta)^{1 - y_i}.\n$$\n\nWe recognize this as the Bernoulli distribution. The joint distribution is obtained by multiplying together all the observations\n\n$$\nPr(Y_1 = y_1, \\dots, Y_{100} = y_{100}) = \\prod_{i=1}^{100}\\theta^{y_i}(1 - \\theta)^{1 - y_i} = \\theta^{\\sum_i y_i}(1 - \\theta)^{100 - \\sum_i y_i}.\n$$\n\nThis is the distribution of any specific sequence of $y_i$'s. Let $X = \\sum_{i=1}^{100}y_i$. There are $\\binom{100}{X} = \\frac{100!}{X!(100-X)!}$ ways to choose a group of size $X$ from a group of size $100$. Then we can write the distribution of $X$ as\n\n$$\nPr(X = x) = \\binom{100}{x} \\theta^x(1 - \\theta)^{100 - x}\n$$\n\n------------------------------------------------------------------------\n\nb.  For the moment, suppose you believed that $\\theta \\in \\{0, 0.1, \\dots, 0.9, 1.0\\}$. Given that the results of the survey were $\\sum_{i=1}^{100}Y_i=73$, compute $\\mathrm{Pr}(\\sum_{i=1}^{100}Y_i=73\\vert\\theta)$ for each of these $11$ values of $\\theta$ and plot these probabilities as a function of $\\theta$ (point mass at each value of $\\theta$).\n\n-----\n\n**Solution:** The equation above now becomes\n\n$$\nPr(X = 73|\\theta) = \\binom{100}{73} \\theta^{73}(1 - \\theta)^{27}\n$$\n\nWhen evaluating this expression, we do our calculations on the log scale to avoid numerical underflow. This gives us\n\n$$\nPr(X = 73|\\theta) = \\binom{100}{73} e^{73 \\ln(\\theta) + 27 \\ln(1 - \\theta)}.\n$$\n\nWe could easily calculate the binomial coefficient on the log scale, but I trust that our programming language of choice has an efficient and safe implementation.\n\n\n::: {.cell layout=\"[[2,4]]\"}\n\n````{.cell-code}\n```{{r}}\n#| layout: [[2, 4]]\n# Include your code here\n\nmy_likelihood <- function(theta) {\n    choose(100, 73) * exp(73 * log(theta) + 27 * log(1 - theta))\n}\n\nthetas <- seq(0, 1, by = 0.1)\nresult <- my_likelihood(thetas)\n\ndata.frame(theta = thetas, likelihood = result)\n\nplot(\n    thetas, \n    result, \n    xlab = expression(theta), \n    ylab = expression(Pr(X==73*\"|\"*theta))\n)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   theta   likelihood\n1    0.0 0.000000e+00\n2    0.1 1.114936e-50\n3    0.2 4.378461e-30\n4    0.3 8.515317e-19\n5    0.4 1.750513e-11\n6    0.5 1.512525e-06\n7    0.6 2.204769e-03\n8    0.7 7.196692e-02\n9    0.8 2.168109e-02\n10   0.9 8.758007e-07\n11   1.0 0.000000e+00\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nc.  Now suppose you originally had no prior information to believe one of these $\\theta$-values over another, and thus $\\mathrm{Pr}(\\theta=0.0) = \\mathrm{Pr}(\\theta=0.1) = \\dots = \\mathrm{Pr}(\\theta=0.9) = \\mathrm{Pr}(\\theta = 1.0) = \\frac{1}{11}$. Use Bayes' rule to compute $p(\\theta\\vert \\sum_{i=1}^{100}Y_i=73)$ for each $\\theta$-value. Make a plot of this posterior distribution as a function of $\\theta$ (point mass at each value of $\\theta$).\n\n-----\n\n**Solution:** First we write out all the equations that we know, before piecing them together:\n\n$$\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{Pr(X=73|\\theta)\\pi(\\theta)}{m(X=73)} \\\\\nPr(X=73|\\theta) &= \\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\\\\n\\pi(\\theta) &= \\frac{1}{11} \\\\\nm(X=73) &= \\sum_\\theta\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}.\n\\end{aligned}\n$$\n\nThis gives us\n\n$$\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}\n}{\\sum_\\theta\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot \\frac{1}{11}} \\\\\n&= \\frac{\\theta^{73}(1-\\theta)^{27}}{\\sum_\\theta \\theta^{73}(1-\\theta)^{27}}\n\\end{aligned}\n$$\n\nOnce again doing our calculations on the log scale, we get\n\n$$ \np(\\theta \\vert X = 73) = \\frac{e^{73\\ln(\\theta) + 27\\ln(1 - \\theta)}}{\\sum_\\theta e^{73\\ln(\\theta) + 27\\ln(1 - \\theta)}}\n$$\n\n\n::: {.cell layout=\"[[2,4]]\"}\n\n````{.cell-code}\n```{{r}}\n#| layout: [[2, 4]]\n# Include your code here\n\nmy_posterior <- function(theta) {\n    theta_seq <- seq(0, 1, by = 0.1)\n    C <- sum(exp(73 * log(theta_seq) + 27 * log(1 - theta_seq)))\n    exp(73 * log(theta) + 27 * log(1 - theta)) / C\n}\n\nthetas <- seq(0, 1, by = 0.1)\nposterior <- my_posterior(thetas)\n\ndata.frame(theta = thetas, posterior = posterior)\n\nplot(\n    thetas, \n    posterior, \n    xlab = expression(theta), \n    ylab = expression(p(theta*\"|\"*Y))\n)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   theta    posterior\n1    0.0 0.000000e+00\n2    0.1 1.163146e-49\n3    0.2 4.567788e-29\n4    0.3 8.883524e-18\n5    0.4 1.826206e-10\n6    0.5 1.577927e-05\n7    0.6 2.300105e-02\n8    0.7 7.507881e-01\n9    0.8 2.261859e-01\n10   0.9 9.136709e-06\n11   1.0 0.000000e+00\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nd.  Now suppose you allow $\\theta$ to be any value in the interval $[0, 1]$. Using the uniform prior density for $\\theta$, namely, $p(\\theta) = 1$, derive and plot the posterior density of $\\theta$ as a function of $\\theta$. According to the posterior density, what is the probability of $\\theta > 0.8$?\n\n-----\n\n**Solution:** This time, our posterior is seen to be\n\n$$\n\\begin{aligned}\np(\\theta \\vert X = 73) &= \\frac{\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot 1\n}{\\int_0^1\\binom{100}{73} \\theta^{73}(1 - \\theta)^{27} \\cdot 1 d\\theta} \\\\\n&= \\frac{\\theta^{73}(1-\\theta)^{27}}{\\int_0^1 \\theta^{73}(1-\\theta)^{27}d\\theta} \\\\\n&= C \\cdot \\theta^{73}(1-\\theta)^{27}\n\\end{aligned}\n$$\n\nWe can use the fact that the equation above should integrate to one to find out what $C$ is.\n\n$$\n\\int_0^1 C\\cdot \\theta^{73}(1 - \\theta)^{27} = 1 \\\\\n\\int_0^1 \\theta^{73}(1 - \\theta)^{27} = \\frac1C.\n$$\n\nFrom working with the Beta distribution, we know that\n\n$$\n\\int_0^1\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1 - x)^{\\beta-1} = 1.\n$$\n\nUsing this, we see that\n\n$$\nC = \\frac{\\Gamma(73 + 27 + 2)}{\\Gamma(73 + 1)\\Gamma(27 + 1)},\n$$\n\ngiving us\n\n$$\np(\\theta \\vert X = 73) = \\frac{\\Gamma(73 + 27 + 2)}{\\Gamma(73 + 1)\\Gamma(27 + 1)} \\theta^{73}(1 - \\theta)^{27}.\n$$\n\nThis means that after observing the data, our uncertainty about $\\theta$ is described by the Beta distribution with $\\alpha = 73+1$ and $\\beta = 27+1$.\n\n$$\n\\theta\\vert Y \\sim \\mathrm{Beta}(73 + 1, 27 + 1)\n$$\n\nWe'll use R's built in `dbeta` *(d for density)* function to evaluate this.\n\n\n::: {.cell layout=\"[[2,4]]\"}\n\n````{.cell-code}\n```{{r}}\n#| layout: [[2, 4]]\n# Include your code here\nthetas <- seq(0, 1, by = 0.1)\nposterior <- dbeta(thetas, 73 + 1, 27 + 1)\n\ndata.frame(theta = thetas, posterior = posterior)\n\nplot(\n    thetas, \n    posterior, \n    xlab = expression(theta), \n    ylab = expression(p(theta*\"|\"*Y))\n)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   theta    posterior\n1    0.0 0.000000e+00\n2    0.1 1.126085e-48\n3    0.2 4.422245e-28\n4    0.3 8.600470e-17\n5    0.4 1.768018e-09\n6    0.5 1.527650e-04\n7    0.6 2.226817e-01\n8    0.7 7.268659e+00\n9    0.8 2.189790e+00\n10   0.9 8.845588e-05\n11   1.0 0.000000e+00\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pbeta(0.8, 73 + 1, 27 + 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03848785\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\ne.  Why are the heights of posterior densities in *c.* and *d.* not the same?\n\n-----\n\n**Solution:** The density in *c.* belongs to a discrete variable and the density in *d.* belongs to a continuous variable. This means the two functions have different meanings. Discrete random variables have densities *(often called mass functions)* that can be interpreted as probabilities, but that is not the case for continuous random variables. Thus the density of a continuous random variable can be greater than one as long as it integrates to one over it's domain.\n\n------------------------------------------------------------------------\n\n# 2. Random numbers, probability density functions (pdf) and cumulative density functions (cdf)\n\nThe goal of this exercise is to generate random numbers, plot the histogram, the empirical pdf and cdf for these numbers, and see how they compare to the theoretical pdf and cdf. The goal is also to compare the sample mean and standard deviation to the theoretical mean and standard deviation.\n\na.  Generate $B = 3000$ numbers from the gamma distribution with parameters $\\alpha = 2$ and $\\beta = 0.1$. Compute the sample mean and the sample standard deviation and compare to the theoretical mean and standard deviation.\n\n-----\n\n**Solution:**\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n# Include your code here\nB <- 3000\nalpha <- 2\nbeta <- 0.1\nX <- rgamma(n = B, shape = alpha, rate = beta)\n\ntheoretical_mean <- alpha / beta\ntheoretical_sd <- sqrt(alpha / beta^2)\n\nobs_mean <- mean(X)\nobs_sd <- sd(X)\n\ndata.frame(\n    theoretical_mean,\n    obs_mean,\n    theoretical_sd,\n    obs_sd\n)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  theoretical_mean obs_mean theoretical_sd   obs_sd\n1               20 19.53301       14.14214 13.66634\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nb.  Plot the theoretical density (pdf) of the gamma distribution. Plot the empirical density based on the data on the same graph. Plot the histogram of the data on another graph.\n\n-----\n\n**Solution:**\n\nUsing base R:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n# Include your code here\n\nx_dens <- density(X, from = 0)\n\nplot(\n    x_dens, \n    main = \"Comparing the empirical and theoretical densities\", \n    col = \"blue\"\n)\ncurve(\n    dgamma(x, shape = alpha, rate = beta), \n    from = 0, to = 120, \n    add = TRUE, \n    col = \"red\"\n)\nlegend(\n    \"top\", \n    c(\"Empirical\", \"Theoretical\"), \n    col = c(\"blue\", \"red\"), \n    lty = 1\n)\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nUsing the tidyverse:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| output: false\nlibrary(dplyr)\nlibrary(ggplot2)\ntheme_set(theme_classic())\n```\n````\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ntibble(\n    X = X\n) |> \n    ggplot(aes(X)) +\n    geom_density(\n        bounds = c(0, Inf),\n        aes(color = \"Empirical\", lty = \"Empirical\")\n        ) +\n    stat_function(\n        fun = function(x) dgamma(x, shape =alpha, rate = beta),\n        geom = \"line\",\n        aes(color = \"Theoretical\", lty = \"Theoretical\")\n    ) +\n    labs(\n        color = \"Type\",\n        linetype = \"Type\",\n        x = \"X\",\n        y = \"Density\",\n        title = \"Comparing the empirical and theoretical densities\"\n    )\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\nc.  Plot the theoretical cumulative density function (cdf) of the gamma distribution. Plot the empirical cumulative density based on the data on the same graph.\n\n-----\n\n**Solution:**\n\nUsing base R:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n# Include your code here\nX_sorted <- sort(X)\nquantile <- seq_along(X_sorted) / (length(X_sorted) + 1)\n\nplot(\n    X_sorted, quantile, \n    type = \"l\", \n    col = \"blue\", \n    main = \"Comparing the empirical and theoretical CDF\",\n    xlab = \"x\",\n    ylab = expression(P(X<x))\n)\ncurve(\n    pgamma(x, shape = alpha, rate = beta), \n    from = 0, to = 120, \n    add = TRUE, \n    col = \"red\"\n)\nlegend(\n    \"right\", \n    c(\"Empirical\", \"Theoretical\"),\n    col = c(\"blue\", \"red\"), \n    lty = 1\n)\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nUsing the tidyverse:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(dplyr)\nlibrary(ggplot2)\ntibble(\n    X = X\n) |> \n    arrange(X) |> \n    mutate(\n        empirical = row_number() / (n() + 1)\n    ) |> \n    ggplot(aes(X)) +\n    geom_line(\n        aes(y = empirical, color = \"Empirical\", lty = \"Empirical\")\n        ) +\n    stat_function(\n        fun = function(x) pgamma(x, shape =alpha, rate = beta),\n        geom = \"line\",\n        aes(color = \"Theoretical\", lty = \"Theoretical\")\n    ) +\n    labs(\n        color = \"Type\",\n        linetype = \"Type\",\n        x = \"X\",\n        y = \"Density\",\n        title = \"Comparing the empirical and theoretical CDF\"\n    )\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}